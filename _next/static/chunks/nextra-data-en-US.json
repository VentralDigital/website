{"/components":{"title":"Nextra Components","data":{"nextjs#NextJS":"","links#Links":"All relative Markdown links are automatically converted to Next.js links. This\nmeans that the target page will be prefetched. And when you click on a link, the\npage will be loaded on the client-side like a SPA, without making a full page\nload. For example:\nWill be equivalent to:\nThis feature makes navigation between Nextra pages fast and seamless.","static-images#Static Images":"This feature is enabled via staticImage: true in the Nextra config by\ndefault.\nNextra supports automatically optimizing your static image imports with the\nMarkdown syntax. You do not need to specify the width and height of the image,\njust use the ![]() Markdown syntax:\nThis loads the og.png file inside the public folder, and automatically\nwraps it with Next.js <Image>.\nYou can also use ![](../public/og.png) to load the image from a relative\npath, if you don't want to host it via public.\nThe standard way to use\nNext.js Image\ninside MDX is to directly import the component:","custom-css#Custom CSS":"See ../styles.css, which will be imported globally.","cards#Cards":"","with-images#With images":"","with-icons#With Icons":"","footer-cards#Footer Cards":"","callouts#Callouts":"A callout is a short piece of text intended to attract attention.\nA callout is a short piece of text intended to attract attention.\nA callout is a short piece of text intended to attract attention.\nA callout is a short piece of text intended to attract attention.\nSpace Invaders is a 1978 shoot 'em up arcade game developed by Tomohiro\nNishikado.","tabs#Tabs":"pnpm: Fast, disk space efficient package manager.\nnpm is a package manager for the JavaScript programming language.\nYarn is a software packaging system.","steps#Steps":"","step-1#Step 1":"Contents for step 1.","step-2#Step 2":"Contents for step 2.","images#Images":"","figure#Figure":"Some text below","screenshot#Screenshot":"","tables#Tables":"","standard#Standard":"Left\tCenter\tRight\tHeader\tTitle\tHere's this\tParagraph\tText\tAnd more\tStrikethrough\t\tText","option-table#Option Table":"","code#Code":"Supported languages: https://github.com/shikijs/shiki/blob/main/docs/languages.md","inlined#Inlined":"Inlined syntax highlighting is also supported let x = 1{:jsx} via:","blocks#Blocks":"Supports\nFilenames / Titles\nLine highlights\nSubstring highlights\nOn-hover copy button\nOptional line numbers","ansi-highlighting#ANSI Highlighting":"You can highlight ANSI escape codes:\nRenders:","task-list#Task List":"Write the press release\nUpdate the website\nContact the media","cuuuuuustom#Custom Heading Id":"","faq#FAQ":"Answer\nA\nB\nC\nAnother answer","file-tree#File Tree":"","diagrams#Diagrams":"","latex#LaTeX":"Using LaTeX within MDX is as simple as wrapping your expression in $ or $$.\nwill be rendered as:\nThe Pythagorean equation: .\nYou can still use Markdown and MDX syntax in the same line as your LaTeX expression.\nIf you want to display $ in your content instead of rendering it as an\nequation, you can escape it with a backslash (\\). For example \\$e = mc^2\\$\nwill be rendered as $e = mc^2$.\nTo learn more about KaTeX and its supported functions and conventions, visit KaTeX‚Äôs documentation.","asciimath#AsciiMath":"With asciimath: .","layout-bleeding#Layout Bleeding":"When wrapping your content with <Bleed>, it will be slightly wider than the\ncontainer and will overflow on both sides.\nThere is nothing to writing. All you do is sit down at a typewriter and bleed.‚Äî Ernest Hemingway\nIt provides a better reading experience when you want to present some graphical\ninformation, which normally looks nicer in a larger size.For example, you can put text, image, video or any component inside.You can even make it full-bleed using <Bleed full> to let it break out of the container completely."}},"/donate":{"title":"Donate","data":{"":"If you've been enjoying the content, consider donating any cryptocurrency of your choice.","bitcoin-btc#Bitcoin (BTC)":"Address:","ether-eth--any-tokens#Ether (ETH) & Any Tokens":"Address:","bitcoin-cash-bch#Bitcoin Cash (BCH)":"Address:","litecoin-ltc#Litecoin (LTC)":"Address:","zcash-zec#Zcash (ZEC)":"Address:","monero-xmr#Monero (XMR)":"Address:","firo-firo#Firo (FIRO)":"Address:\nThe currency you wanted to donate is not listed?"}},"/about":{"title":"About","data":{"":"Ventral Digital LLC is a research and consultancy firm specializing in Information Security and Privacy.The term ventral (ven-truhl; of or relating to the venter or belly; abdominal.) is conventionally used in medical context. To us, it represents the excitement and seemingly endless flow of energy one experiences when both the intellectual and primal interests are in alignment.","patrickd#patrickd":"Patrick Drotleff\nùïè\nPrevious to being a Security Researcher, Patrick worked in the Web2/SaaS space for over 10 years, writing and reviewing code, always feeling drawn towards the security aspects of technology. During these years he developed a keen sense for identifying edge cases and became-well practiced at technical writing that breaks down complex topics into intuitive explanations. He finally broke into the Web3/Blockchain space through the very first Secureum Bootcamp and was selected as a mentor due to his continued activity as alumni. Patrick has since worked as an independent consultant in BlockSec, reviewed various projects together with other independent researchers at Spearbit, and endeavored into many rabbit holes on the Ventral Digital blog. Drawing inspiration from The Sovereign Individual, Patrick advocates for the development and securing of key technologies that enhance privacy and decentralize trust and power.\nIndependent Security Researcher in Web3/Blockchain\nSecureum Bootcamp Mentor and Epoch 0 alumni\nSecurity Research Contractor at Spearbit\nAudited projects such as Cozy Finance, Paladin, Art Gobblers, and SushiSwap\nConsultant/vCISO for Optimism's Bedrock upgrade\nFreelance Senior Backend/DevOps Engineer\nTechnologies: AWS, Azure, Kubernetes\nSpecialized in refactoring, migration, and design of SaaS APIs with cloud services\nOrganized company internal CTF challenges for developers\nSaaS/JS Senior Backend Developer, DevOps, full-time @ maloon GmbH\nTechnologies: ELK Stack\nSpecialized in research, prototyping, and quality/security reviews\nResponsible for evaluation of applicant developers\nSaaS/JS Backend Developer, DevOps, part-time student @ maloon GmbH\nTechnologies: CoffeeScript, JavaScript, TypeScript, MongoDB, NodeJS, MongoDB, ElasticSearch, RabbitMQ, Redis\nImplemented data synchronization services between various databases\nImplemented web crawlers for sales lead generation processes\nDrupal/PHP Developer, DevOps, part-time student @ maloon GmbH\nTechnologies: Drupal, PHP, nginx, Apache2 (SUExec), haproxy, LXC\nResearch & Development of Social Media synchronization MVPs\nMentor in the Drupal Community and responsible for security/quality reviews of contributed plugins\nCreated various online tools, such as simplytest.me, easing the testing and review of Drupal patches and modules\nStarted Bachelor of Science in Information Technology @ TH Ingolstadt\nTechnologies: LaTeX, JavaScript, PHP, Perl, Java, C, Assembly, WebGL, SQL, GWT, LUA\nGame design with L√∂ve engine, mobile game optimization\nThesis on SaaS API design","guest-writers#Guest Writers":"","0xnorman#0xNorman":"Wu Enbang\nùïè\nNorman, a recent graduate from the University of Wisconsin with a degree in Mathematics, has distinguished himself as an independent auditor. He has won 3 Code4rena bounties in a roll during his college years. Currently, Norman is interning at Movebit, a leading audit firm in Sui and Aptos ecosystems. With a great passion for blockchain, Norman is honing his skills as an auditor to secure the web3 ecosystem.","parsely#Parsely":"Darren Chapman\nùïè\nDarren has almost 20 years of experience in IT within the Banking Industry and holds certifications from IBM, SAP, and Sun Micro Systems. Darren currently specializes in the IBM Middleware stack, developing integration solutions in numerous technologies. After deciding to follow a life-long interest in security, he gained the PNPT certification from TCM Security. With his found passion for blockchain, Darren is honing his skills as an auditor to contribute to the vibrant world of Web3."}},"/posts/2020/10/11/s3-api-compatibility-on-microsoft-azure":{"title":"S3 API Compatibility On Microsoft Azure","data":{"":"October 11, 2020 by patrickd\nNote that this article is rather old and the proposed solution may no longer work with the steps described.\nMicrosoft's Azure Cloud Service Blob Storage is an alternative to AWS S3's Object Storage, but that doesn't mean it is intended as a drop-in replacement. While the S3 API is now widely considered a quasi-standard for cloud data storage, this was not the case when Azure Blob Storage was first created. While it too has a relatively simple HTTP API, the interfacing is still very different to S3.To establish compatibility between S3 API clients and the Azure Blob Storage we'll be using the S3Proxy project. It's a Java application that acts as a proxy between a S3 API client and the Azure Blob Storage Service by translating requests accordingly.","prerequisites#Prerequisites":"In this example setup the S3Proxy program will be deployed as a container to an AKS Cluster. Azure Kubernetes Service provides managed Kubernetes clusters that are easily set up, managed and monitored via the Azure Portal web interface.If you do not have an AKS yet, follow Microsoft's instructions on how to setup a Cluster with AKS. Furthermore you might want to set up an HTTPS ingress that will allow you to securely expose the S3Proxy service on the Internet with SSL encryption.","setup#Setup":"Once your AKS cluster is ready, create an Azure Blob Storage Account that S3 Proxy will connect to in order to store files at. After it has been created successfully, navigate to Access Keys and copy the name of your storage account and one of the keys displayed.\nWhen dealing with Access Keys from Azure Storage Accounts, make sure that they do NOT contain any special characters such as slashes. Otherwise you might run into issues when using them with S3Proxy. Within the Azure Portal you can keep regenerating new keys until you get one with only digits and letters (the == characters at the end are part of the base64 encoding and are fine).\nThe following is an example manifest file that uses an image from dockerhub (andrewgaul/s3proxy) which includes the S3Proxy in a way that allows us to simply configure it using environment variables:\nHere you will use the Storage Account name and key that you previously copied from the Access Keys page on your Storage Account's settings.\nNote that it's generally recommended to use Secrets when handling credentials in Kubernetes.\nAfter making your adjustments you may now deploy S3Proxy by executing kubectl apply -f s3proxy.yaml{:bash}.","test#Test":"You can test your S3Proxy setup using the AWS CLI on your local computer. To do so first edit the ~/.aws/credentials configuration file and add the following lines:\nThen add the following lines to your ~/.aws/config file:\nWith that you should be able to manage your Azure Blob Storage Account using the AWS CLI while specifying --endpoint-url and --profile:"}},"/":{"title":"Index","data":{"cryptocurrency-privacy-technologies#Cryptocurrency Privacy Technologies":"","smart-contract-auditors-rewind#Smart Contract Auditor's Rewind":"","2024#2024":"","2023#2023":"","2022#2022":"","2021#2021":"","2020#2020":""}},"/posts/2020/8/9/cracking-sessions-mongodb-nosql-operator-injection":{"title":"MongoDB NoSQL Operator Injection: Cracking Sessions","data":{"":"August 9, 2020 by patrickd\nThis article discusses how Operator Injection in MongoDB may be used for efficiently cracking a secret session token and therefore breaking authentication.","exploit#Exploit":"Let's assume we have a NodeJS application with an authentication system that stores its API access tokens within a sessions collection together with the ID of the user that the session belongs to:\nThe following is a GET /me{:http} route that makes use of that system and returns information for the currently authenticated user:\nAs in the previous article we can again use query parameters to specify an object instead of the expected access_token string (?key[field]=value{:js}) in order to inject an operator into the database query.While it's simple to use the not-equal ($ne) operator to authenticate with the first session found in database (by natural order, not unlikely to be an admin user), we won't be able obtain the actual access_token that was used as part of the response.\nBut as the target value is a string we can now make use of the Regular Expression Matching Operator ($regex), allowing us to iterate the token's alphabet ‚Äì character by character:\nCompared to attempting to determine the token by brute force (up to  requests necessary in this example), this is a much more practical way to \"crack\" a session (only up to  requests necessary).","caveats#Caveats":"For user facing applications, sessions are more commonly stored in cookies rather than being passed as query parameters. It appears that the most common cookie-parsing libraries for NodeJS do not parse objects specified in the cookie's query-string.Note however that the behavior is different for other cookie parsers: For example, sending Cookie: session[$ne]=_nothing_will_match_this_ as HTTP header to a PHP7 Server will be decoded as an object for the $_COOKIE global making it a potential attack vector for the described exploit."}},"/posts/2020/7/25/iterating-collections-mongodb-nosql-operator-injection":{"title":"MongoDB NoSQL Operator Injection: Iterating Collections","data":{"":"July 25, 2020 by patrickd\nDuring Penetration-Test reconnaissance, enumerating through rows of a SQL database is usually simple due to the use of sequential identifiers:\nThe NoSQL database MongoDB uses \"ObjectIds\" which are 12 byte (represented as a 24 characters long hexdecimal string) identifiers made up of:\nLength\tDescription\t4 bytes\tunix timestamp\t5 bytes\t\"random\" value, typically a 3 byte machine specific identifier (eg. from mac address) and the 2 byte process id\t3 bytes\tsequential counter (does not reliably count in increments of 1)\t\nAlthough this means that ObjectIds can be called \"incremental\" (as each new id is greater than the previous one), they are still more difficult to predict than a simple integer.","exploit#Exploit":"Let's assume we have a NodeJS application with a route controller executing a read operation (using the mongoose ODM library) in order to fetch a single document from a collection with a specified identifier:\nThe used expressJS HTTP application server framework (and many others) will parse the query parameters while allowing arrays (?key[]=value1&key[]=value2{:js}) and even objects (?key[field]=value{:js}) to be specified.This can be used to inject query operators such as not-equal ($ne):\nResulting query: Find one document with an ID that is not equal to the specified ID.\nThis yields us the very first document of the users collection (the first document that does not match the specified identifier in the natural order of the collection), which is typically an administrative user.After having determined the identifier of the first document in the collection, we can use the greater-than ($gt) operator to iterate over the whole collection:","caveats#Caveats":"Note that vectors involving MongoDB ObjectIds are only available when ODMs such as mongoose are used that will automatically convert strings into them. When native clients are used, query values will usually be passed through a conversion function causing errors to be thrown when attempting to exploit this flaw: Provided identifier is not a valid MongoDB ObjectId string: '[object Object]'.Another problem is that the ID must be part of the query parameters. If REST-like patterns are used and the identifier is part of the path (/users/5ef3bfd5bec2121c8525f3fe) we won't be able to inject operators so easily."}},"/posts/2020/9/6/mongodb-nosql-operator-injection-mitigation":{"title":"MongoDB NoSQL Operator Injection: Mitigation","data":{"":"September 6, 2020 by patrickd\nIn previous articles we explored different ways of exploiting Operator Injections. Here we want to look at how they can be mitigated.","validating-user-input#Validating User Input":"Due to the age of this article, the exact methods shown below may be wildly outdated.\nThis won't be a surprise for experienced programmers, but as with most security issues, it is always about validating all values that have been provided by the user. Whether it is the URL (path, query parameters, ...) or other HTTP Headers (Cookies, Referer, Host, ...) or if one was sent, the request body itself. Before making use of User Input within an application we should validate whether it looks anything like we expect it to.In terms of the NodeJS applications from previous examples this could mean using a Schema Validator like AJV to create a reusable middleware that can be easily plugged in before the controllers' business logic:\nWith this it is no longer possible to pass Objects (?id[key]=value{:js}) or Arrays (?id[]=value{:js}) as id parameter values to this route's controller, effectively mitigating any chance for NoSQL Injections.To reduce the chance of human error even further we could prepare similar middlewares for all other possible User Input vectors (validateBody, validateCookies, validateHeaders, ...) and we might even want to make the raw values completely inaccessible unless the appropriate validator middleware has been called before.","disabling-advanced-query-parsing#Disabling Advanced Query parsing":"Most No SQL Injection vectors appear in the URL Query Parameters and that, even though the feature enabling them is usually not used by the vulnerable application itself. Therefore, even though it only covers a small part of the overall possible attack surface, it should be considered to simply disable the functionality that allows specifying Objects and Arrays as Query Parameter values ‚Äì if that is possible."}},"/posts/2021/10/17/secureum-bootcamp-ethereum-101-quiz":{"title":"Secureum Bootcamp Ethereum 101 Quiz","data":{"":"This is a writeup of the Secureum Bootcamp Ethereum 101 Quiz containing solutions and references to the provided study material.\nFor fairness, it was published after submissions to it were closed. The quiz consisted of 32 questions with a strict timelimit of 16 minutes. The ordering of the questions was randomized, so the numbering here won‚Äôt match with the numbering elsewhere.\nAugust 23, 2021 by patrickd","question-1-of-32#Question 1 of 32":"Which of the following EVM components is/are non-volatile across transactions?\n A. Stack \n B. Memory \n C. Storage \n D. Calldata \nCorrect is C.\nA stack machine is a computer processor or a virtual machine in which the primary interaction is moving short-lived temporary values to and from a push down stack.\nfrom Stack machine - Wikipedia\nThe EVM is a simple stack-based architecture consisting of the stack, volatile memory, non-volatile storage with a word size of 256-bit (chosen to facilitate the Keccak256 hash scheme and elliptic-curve computations) and Calldata.\nfrom point 59 of Ethereum 101 - by Secureum\nCalldata is a read-only byte-addressable space where the data parameter of a transaction or call is held.\nfrom point 63 of Ethereum 101 - by Secureum","question-2-of-32#Question 2 of 32":"The number of transactions in a Ethereum block depend on\n A. Nothing. It is a constant \n B. Gas used by transactions \n C. Block gas limit \n D. Block difficulty \nCorrect is B & C.\nBlock gas limit is set by miners and refers to the cap on the total amount of gas expended by all transactions in the block, which ensures that blocks can‚Äôt be arbitrarily large. Blocks therefore are not a fixed size in terms of the number of transactions because different transactions consume different amounts of gas.\nfrom point 48 of Ethereum 101 - by Secureum","question-3-of-32#Question 3 of 32":"EVM Stores\n A. Most significant byte in the smallest memory address \n B. Most significant byte in the largest memory address \n C. In Big-endian order \n D. In Little-endian order \nCorrect is A & C.\nEVM uses big-endian ordering where the most significant byte of a word is stored at the smallest memory address and the least significant byte at the largest\nfrom point 65 of Ethereum 101 - by Secureum","question-4-of-32#Question 4 of 32":"Ethereum's thread model is characterised by\n A. Trusted miners and users \n B. Trusted users, untrusted miners \n C. Trusted miners, untrusted users \n D. Everyone is untrusted \nCorrect is D.\nGiven the aspirational absence of trusted intermediaries, everyone and everything is meant to be untrusted by default. Participants in this model include developers, miners/validators, infrastructure providers and users, all of whom could potentially be adversaries.\nfrom point 95 of Ethereum 101 - by Secureum","question-5-of-32#Question 5 of 32":"Ethereum smart contracts do not run into halting problems because\n A. EVM is not Turing Complete \n B. EVM is Turing Complete \n C. EVM is Turing Complete but is bounded by gas sent in transaction \n D. EVM is Turing Complete but is bounded by the stack depth \nCorrect is C.\nTuring-complete systems face the challenge of the halting problem i.e. given an arbitrary program and its input, it is not solvable to determine whether the program will eventually stop running. So Ethereum cannot predict if a smart contract will terminate, or how long it will run. Therefore, to constrain the resources used by a smart contract, Ethereum introduces a metering mechanism called gas.\nfrom point 10 of Ethereum 101 - by Secureum","question-6-of-32#Question 6 of 32":"Which of the following operation(s) touch(es) storage?\n A. SWAP \n B. SLOAD \n C. DUP \n D. PUSH \nCorrect is B.\nMost EVM instructions operate with the stack (stack-based architecture) and there are also stack-specific operations e.g. PUSH, POP, SWAP, DUP etc.\nfrom point 60 of Ethereum 101 - by Secureum\nStorage is a 256-bit to 256-bit key-value store. [...] This is accessed with SLOAD/SSTORE instructions.\nfrom point 62 of Ethereum 101 - by Secureum","question-7-of-32#Question 7 of 32":"The most gas-expensive operation is\n A. SLOAD \n B. SSTORE \n C. CREATE \n D. SELFDESTRUCT \nCorrect is C.\nSLOAD is 2100 gas and SSTORE is 20,000 gas to set a storage slot from 0 to non-0 and 5,000 gas otherwise. CREATE is 32000 gas and SELFDESTRUCT is 5000 gas.\nfrom point 78 of Ethereum 101 - by Secureum","question-8-of-32#Question 8 of 32":"Transaction T1 attempts to write to storage values S1 and S2 of contract C. Transaction T2 attempts to read the same storage values S1 and S2. However, T1 reverts due to an exception after writing S2.Which or the following is/are TRUE?\n A. T2 reads the value of S1 updated by T1 \n B. T2 reads the value of S1 prior to T1's attempted update \n C. T2 also reverts because of the dependency on T1 \n D. This scenario is not possible \nCorrect is B.\nTransaction properties: Atomic: it is all or nothing i.e. cannot be divided or interrupted by other transactions\nfrom point 32 of Ethereum 101 - by Secureum\nA transaction reverts for different exceptional conditions such as running out of gas, invalid instructions etc. in which case all state changes made so far are discarded and the original state of the account is restored as it was before this transaction executed.\nfrom point 79 of Ethereum 101 - by Secureum","question-9-of-32#Question 9 of 32":"The gas tracking website https://etherscan.io/gastracker says that Low gas cost is 40 gwei. This affects\n A. The transaction gasPrice \n B. The transaction gasLimit \n C. The transaction value \n D. Both B & C \nCorrect is A.\nGas price: The price a transaction originator is willing to pay in exchange for gas. The price is measured in wei per gas unit. The higher the gas price, the faster the transaction is likely to be confirmed on the blockchain. The suggested gas price depends on the demand for block space at the time of the transaction.\nfrom point 35 of Ethereum 101 - by Secureum\ngasLimit: The maximum amount of gas the originator is willing to pay for this transaction value: The amount of ether (in wei) to send to the destination\nfrom point 33 of Ethereum 101 - by Secureum","question-10-of-32#Question 10 of 32":"Security of Ethereum DApps depend on\n A. Security of their smart contracts \n B. Security of their off-chain components \n C. Security of Ethereum \n D. None of the above \nCorrect is A, B, C.\nOn-chain vs Off-chain: Smart contracts are ‚Äúon-chain‚Äù Web3 components and they interact with ‚Äúoff-chain‚Äù components that are very similar to Web2 software. So the major differences in security perspectives between Web3 and Web2 mostly narrow down to security considerations of smart contracts vis-a-vis Web2 software.\nfrom point 90 of Ethereum 101 - by Secureum","question-11-of-32#Question 11 of 32":"A nonce is present in\n A. Ethereum transaction \n B. Ethereum account \n C. Both A & B \n D. Neither A nor B \nCorrect is C.\nEthereum account contains four fields: The nonce, a counter used to make sure each transaction can only be processed once...\nfrom point 23 of Ethereum 101 - by Secureum\nA transaction is a serialized binary message that contains the following components: nonce: A sequence number, issued by the originating EOA, used to prevent message replay...\nfrom point 33 of Ethereum 101 - by Secureum","question-12-of-32#Question 12 of 32":"Miners are responsible for setting\n A. Transaction gas price \n B. Block gas limit \n C. Both A & B \n D. Neither A nor B \nCorrect is B.\nGas price: The price a transaction originator is willing to pay in exchange for gas. The price is measured in wei per gas unit. The higher the gas price, the faster the transaction is likely to be confirmed on the blockchain. The suggested gas price depends on the demand for block space at the time of the transaction.\nfrom point 35 of Ethereum 101 - by Secureum\nBlock gas limit is set by miners and refers to the cap on the total amount of gas expended by all transactions in the block, which ensures that blocks can‚Äôt be arbitrarily large.\nfrom point 48 of Ethereum 101 - by Secureum","question-13-of-32#Question 13 of 32":"Which of the following information CANNOT be obtained in the EVM?\n A. Block difficulty \n B. Transaction logs \n C. Balance of an account \n D. Block hash of any block \nCorrect is B, D.\n0x31 BALANCE 1 1 Get balance of the given account\n0x40 BLOCKHASH 1 1 Get the hash of one of the 256 most recent complete blocks\n0x44 DIFFICULTY 0 1 Get the block‚Äôs difficulty\n(there's no operation to access transaction logs)from points 70, 71 of Ethereum 101 - by Secureum","question-14-of-32#Question 14 of 32":"Miners are incentivized to validate and create new blocks by\n A. Block rewards \n B. Altruism \n C. Transaction fees \n D. Their belief in decentralization \nCorrect is A, C.\nMiners are rewarded for blocks accepted into the blockchain with a block reward in ether (currently 2 ETH). A miner also gets fees which is the ether spent on gas by all the transactions included in the block.\nfrom point 47 of Ethereum 101 - by Secureum","question-15-of-32#Question 15 of 32":"Smart contracts on Ethereum\n A. May be deployed by anyone \n B. May be deployed only through the DApp store \n C. May have some form of access control \n D. Are guaranteed to be secure \nCorrect is A, C.\nWeb3: is a permissionless, trust-minimized and censorship-resistant network for transfer of value and information.\nfrom point 88 of Ethereum 101 - by Secureum","question-16-of-32#Question 16 of 32":"Hardfork on Ethereum\n A. Has never happened \n B. Happened only once after the DAO attack \n C. Happens with backwards-incompatible protocol changes \n D. Happens when developers and miners disagree on changes \nCorrect is C.\nA hard fork to introduce an exponential difficulty increase, to motivate a transition to PoS when ready....\nfrom \"Ethereum‚Äôs Four Stages of Development\" of Mastering Ethereum","question-17-of-32#Question 17 of 32":"Which call instruction could be used to allow modifying the caller account's state?\n A. CALL \n B. CALLCODE \n C. DELEGATECALL \n D. STATICALL \nCorrect is B, C.\n0xf1 CALL 7 1 Message-call into an account\n0xf2 CALLCODE 7 1 Message-call into this account with an alternative account‚Äôs code\n0xf4 DELEGATECALL 6 1 Message-call into this account with an alternative account‚Äôs code, but persisting the current values for sender and value\n0xfa STATICCALL 6 1 Static message-call into an account\nfrom point 77 of Ethereum 101 - by Secureum\nAnother variant of call is delegatecall, which replaced the more dangerous callcode. [...] Essentially, delegatecall runs the code of another contract inside the context of the execution of the current contract.\nfrom \"Calling Other Contracts (send, call, callcode, delegatecall)\" of Mastering Ethereum\nPermits non-state-changing calls to itself or other contracts while disallowing any modifications to state during the call (and its subcalls, if present) to increase smart contract security and assure developers that re-entrancy bugs cannot arise from the call.\nfrom \"Appendix A: Ethereum Standards\" of Mastering Ethereum","question-18-of-32#Question 18 of 32":"The length of addresses on Ethereum is\n A. 256 bits \n B. 20 bytes \n C. Depends on the Externally-Owned-Account or Contract address \n D. Configurable \nCorrect is B.\nEthereum state is made up of objects called \"accounts\", with each account having a 20-byte address and state transitions being direct transfers of value and information between accounts.\nfrom point 22 of Ethereum 101 - by Secureum","question-19-of-32#Question 19 of 32":"Which of the following statements is/are TRUE about gas?\n A. Unused gas is returned to the transaction destination account \n B. Gas used by the transaction is credited to the beneficiary address in block header \n C. Unused gas is credited to the beneficiary address in block header \n D. Both A & B \nCorrect is B.\nGas refund and beneficiary: Any unused gas in a transaction (gasLimit minus gas used by the transaction) is refunded to the sender‚Äôs account at the same gasPrice. Ether used to purchase gas used for the transaction is credited to the beneficiary address (specified in the block header), the address of an account typically under the control of the miner. This is the transaction ‚Äúfees‚Äù paid to the miner.\nfrom point 56 of Ethereum 101 - by Secureum","question-20-of-32#Question 20 of 32":"The high-level languages typically used for writing Ethereum smart contracts are\n A. Go \n B. C++ \n C. Vyper \n D. Solidity \nCorrect is C, D.\nSolidity language continues to dominate smart contracts without much real competition (except Vyper perhaps).\nfrom point 94 of Ethereum 101 - by Secureum","question-21-of-32#Question 21 of 32":"Ethereum nodes talk to each other via\n A. Peer-to-Peer network \n B. Client-Server network \n C. Satellite network \n D. None of the above \nCorrect is A.\nEthereum node/client: A node is a software application that implements the Ethereum specification and communicates over the peer-to-peer network with other Ethereum nodes.\nfrom point 46 of Ethereum 101 - by Secureum","question-22-of-32#Question 22 of 32":"EVM is not a von Neumann architecture because\n A. It was co-founded by Vitalik Buterin, not John von Neumann \n B. Program instructions are stored separately from data \n C. Program instructions are stored in a ROM not RAM \n D. It is quasi Turing complete \nCorrect is B.\nEVM does not follow the standard von Neumann architecture. Rather than storing program code in generally accessible memory or storage, it is stored separately in a virtual ROM accessible only through a specialized instruction.\nfrom point 64 of Ethereum 101 - by Secureum\nIn computer science, a universal Turing machine (UTM) is a Turing machine that simulates an arbitrary Turing machine on arbitrary input. [...] This principle is considered to be the origin of the idea of a stored-program computer used by John von Neumann in 1946 for the \"Electronic Computing Instrument\" that now bears von Neumann's name: the von Neumann architecture.[1]\nfrom Universal Turing machine - Wikipedia","question-23-of-32#Question 23 of 32":"User A sends transaction T1 from address A1 with gasPrice G1 and later transaction T2 from address A2 with gasPrice G2\n A. T1 will be always included in an earlier block than T2 \n B. Inclusion/Ordering of these transactions depends only on gas prices G1 and G2 \n C. Inclusion/Ordering of these transactions depends only on network congestion \n D. Inclusion/Ordering of these transactions depends on miners \nCorrect is D.\nInclusion: Transaction inclusion is not guaranteed and depends on network congestion and gasPrice among other things. Miners determine inclusion.\nOrder: Transaction order is not guaranteed and depends on network congestion and gasPrice among other things. Miners determine order.\nfrom point 32 of Ethereum 101 - by Secureum","question-24-of-32#Question 24 of 32":"The types of accounts on Ethereum are\n A. All Accounts are the same \n B. Permissioned Accounts and Permissionless Accounts \n C. Externally-Owned-Accounts and Contract Accounts \n D. User Accounts and Admin Accounts \nCorrect is C.\nEthereum has two different types of accounts:\nExternally Owned Accounts (EOAs) controlled by private keys\nContract Accounts controlled by their contract code\nfrom point 24 of Ethereum 101 - by Secureum","question-25-of-32#Question 25 of 32":"The number of decimals in Ether is\n A. 0 \n B. 1 \n C. 18 \n D. Configurable \nCorrect is C.\nEthereum‚Äôs currency unit is called ether or ‚ÄúETH.‚Äù Ether is subdivided into smaller units and the smallest unit is named wei. [...] and 10**18 wei is 1 Ether.\nfrom point 17 of Ethereum 101 - by Secureum","question-26-of-32#Question 26 of 32":"The difference(s) between Bitcoin and Ethereum is/are\n A. The underlying tokens: Bitcoin vs Ether \n B. Smart contract support \n C. UTXO vs Accounts \n D. Nakamoto Consensus \nCorrect is A, B, C.See the Ethereum Whitepaper\nConsensus algorithm: Ethereum uses Bitcoin‚Äôs consensus model, Nakamoto Consensus\nfrom point 9 of Ethereum 101 - by Secureum","question-27-of-32#Question 27 of 32":"Security Audits for smart contracts are performed because\n A. They are required for listing DApp on the DApp store \n B. They are required for deployment on Ethereum \n C. They help remove vulnerabilities and reduce risk \n D. They are required by exchanges to list tokens \nCorrect is C.\nAudit-as-a-Silver-Bullet: Secure Software Development Lifecycle (SSDLC) processes for Web2 products have evolved over several decades to a point where they are expected to meet some minimum requirements of a combination of internal validation, external assessments (e.g. product/process audits, penetration testing) and certifications depending on the value of managed assets, anticipated risk, threat model and the market domain of products (e.g. financial sector has stricter regulatory compliance requirements).\nfrom point 100 of Ethereum 101 - by Secureum","question-28-of-32#Question 28 of 32":"The number of modified Merkle-Patricia trees in an Ethereum block is\n A. One \n B. Three \n C. Three plus number of contract accounts \n D. Three plus number of transactions included in the block \nCorrect is C.\nBlocks contain block header, transactions and ommers‚Äô block headers. Block header contains [...] stateRoot, transactionsRoot and receiptsRoot are 256-bit hashes of the root nodes of modified Merkle-Patricia trees.\nfrom points 53, 54 of Ethereum 101 - by Secureum","question-29-of-32#Question 29 of 32":"EVM opcodes\n A. Are multi-byte instructions \n B. Are single byte instructions \n C. Take operands in registers \n D. Take operands on stack \nCorrect is B, D.\nThe code in Ethereum contracts is written in a low-level, stack-based bytecode language, referred to as \"Ethereum virtual machine code\" or \"EVM code\". The code consists of a series of bytes (hence called bytecode), where each byte represents an operation.\nfrom points 58 of Ethereum 101 - by Secureum\nMost EVM instructions operate with the stack (stack-based architecture) and there are also stack-specific operations e.g. PUSH, POP, SWAP, DUP etc.\nfrom points 60 of Ethereum 101 - by Secureum","question-30-of-32#Question 30 of 32":"Gas for EVM opcodes\n A. Is constant and the same for all opcodes \n B. May be changed over time to prevent DoS attacks \n C. Depend on the gas price \n D. Depend on the miners \nCorrect is B.\nGas costs for different instructions are different depending on their computational/storage load on the client\nfrom points 78 of Ethereum 101 - by Secureum\nTangerine Whistle ‚Äî A hard fork to change the gas calculation for certain I/O-heavy operations and to clear the accumulated state from a denial-of-service (DoS) attack that exploited the low gas cost of those operations.\nSpurious Dragon ‚Äî A hard fork to address more DoS attack vectors, and another state clearing. Also, a replay attack protection mechanism.\nfrom \"Ethereum‚Äôs Four Stages of Development\" of Mastering Ethereum","question-31-of-32#Question 31 of 32":"Ethereum Virtual Machine is a\n A. Register-based virtual machine \n B. Stack-based virtual machine \n C. Heap-based virtual machine \n D. Stackless virtual machine \nCorrect is B.\nThe EVM is a simple stack-based architecture consisting of the stack, volatile memory, non-volatile storage with a word size of 256-bit (chosen to facilitate the Keccak256 hash scheme and elliptic-curve computations) and Calldata.\nfrom points 59 of Ethereum 101 - by Secureum","question-32-of-32#Question 32 of 32":"Which of the following statement(s) is/are FALSE?\n A. EVM can get the block number only of the current block \n B. EVM can get the block hash only of the current block \n C. EVM can get the account balance only of the current account \n D. EVM can get the code hash of only the current account \nCorrect is B, C, D.\n0x31 BALANCE 1 1 Get balance of the given account\n0x3f EXTCODEHASH 1 1 Get hash of an account‚Äôs code\n0x40 BLOCKHASH 1 1 Get the hash of one of the 256 most recent complete blocks\n0x43 NUMBER 0 1 Get the block‚Äôs number\nfrom points 70, 71 of Ethereum 101 - by Secureum"}},"/posts/2021/1/16/httpjs-double-cookies-against-csrf-and-session-theft":{"title":"HTTP/JS-Double-Cookies Against CSRF And Session Theft","data":{"":"January 16, 2021 by patrickd\nModern single page applications and mobile apps often use RESTlike API backends. These APIs are usually only intended to be used by their respective frontends and tend to use very simple authentication mechanisms: After having sent the user's credentials the response either sets Cookies, which will be automatically attached with every further request made, or the response body contains an access-token which the frontend will now manually attach to every further request made, eg. as query parameter.Both of these quite common methods have their strengths and weaknesses: Using cookies with the httpOnly flag set will prevent the session being stolen as it will not be readable from JavaScript if there is an XSS issue. But it will open up vectors for CSRF attacks unless additional measures are put in place to prevent them. Cross Site Request Forgery means that an attacker could make the victims browser execute a malicious HTTP request, which will automatically have the users own cookies attached to it, making it look intentional and legitimate.On the other hand, not using cookies and manually attaching the access-token with JavaScript on each request has the exact opposite of issues: While it's no longer possible to do CSRF it's now quite easy to steal a session using a XSS vulnerability allowing the attacker to copy the access-token and eg. use it in their own browser. While this is often prevented by locking a user's session to their IP address, it will likely cause UX problems since the users will often find themselves logged out after changing networks or when their dynamic IP changed on the next day.In this article I'd like to suggest a quite elegant way to combine these methods, gaining both of their advantages without any of their disadvantages.","implementation#Implementation":"Let's say we have a POST /login{:http} route where the frontend application will send a request body containing the username and password to after the user submitted the login form.\nThe response will now cause the browser to set two cookies:\nhttp_accesstoken containing token A is set to be httpOnly, meaning JavaScript - that being the application or code of a XSS vulnerability - will not have access to its value.\njs_accesstoken containing token B will be readable by the browser's scripts - purposefully so.\nFurther requests made by the frontend app will now automatically have both cookies attached to them. But the API will only make use of the http_accesstoken cookie while it expects the frontend to have read the js_accesstoken cookie value and add it as a (for example) separate HTTP header on each future API request made.\nThis way both the backend and the frontend can manage their sessions solely via cookies. The frontend does not have to worry about separately storing the other token somewhere. If the user logs out the response will clear both tokens and there's no worry about clean up. If the frontend loads and finds a js_accesstoken cookie already set, it should do an authenticated test-request (eg. GET /{:http}) to see if the session belonging to these tokens is still active and valid.","caveats#Caveats":"Naturally this is not a bullet proof solution since the attacker could still prepare their XSS payload to not simply steal the session but, since stealing both tokens is impossible, immediately execute the malicious requests in the same manner that the real frontend application would. Such is only preventable by hardening the application against XSS as much as possible.It should also be mentioned that there are some other cookie flags that should be used for a real world implementation of this method: SameSite: Strict adds further protection against CSRF attacks and Secure will make sure that the cookies are only send on encrypted HTTPS requests helping against Man-In-The-Middle attack vectors."}},"/posts/2021/11/14/secureum-bootcamp-security-pitfalls-amp-best-practices-201-quiz":{"title":"Secureum Bootcamp Security Pitfalls & Best Practices 201 Quiz","data":{"":"This is a writeup of the Secureum Bootcamp Security Pitfalls & Best Practices 201 Quiz containing solutions and references to the provided study material.\nFor fairness, it was published after submissions to it were closed. All questions are concerning the same snippet of code. No syntax highlighting or indentation was used in the original quiz, so it was skipped here as well. Make sure to read code comments carefully. The quiz consisted of 8 questions with a strict timelimit of 16 minutes. The ordering of the questions was randomized, so the numbering here won‚Äôt match with the numbering elsewhere.\nThis Quiz from Epoch 0 was declared to be RACE-1 of Epoch Infinity.\nNovember 14, 2021 by patrickd","code#Code":"All 8 questions in this quiz are based on the InSecureumToken contract shown below. This is the same contract you see for all the 8 questions in this quiz.The InSecureumToken contract implements a token contract which allows users to buy tokens by depositing Ether (at a certain conversion ratio), and transfer tokens.","question-1-of-8#Question 1 of 8":"To avoid lock of funds, the following feature(s) MUST be implemented before contract deployment\n A. A transferFrom() function \n B. A burn() function \n C. A way to withdraw/exchange/use Ether from the contract \n D. None of the above \nCorrect is C.Locked Ether: Contracts that accept Ether via payable functions but without withdrawal mechanisms will lock up that Ether. Remove payable attribute or add withdraw function.from point 29 of Security Pitfalls & Best Practices 101 - by Secureum","question-2-of-8#Question 2 of 8":"Which of the following assertion(s) is/are true (without affecting the security posture of the contract)?\n A. buy() does not need payable keyword \n B. balances must be private \n C. transfer() can be external \n D. safeAdd() can be public \nCorrect is C, D.buy() cannot function without being payable. There's no reason the visibility of balances needs to be private. transfer() can be external since it's not called internally. safeAdd() can be public since it is a pure function.","question-3-of-8#Question 3 of 8":"The total supply is limited by\n A.  \n B.  \n C.  \n D. None of the above \nCorrect is D.It would be B, but MAX_SUPPLY isn't actually used anywhere in the code.","question-4-of-8#Question 4 of 8":"The following issue(s) is/are present in the codebase\n A. An integer underflow allows one to drain Ether \n B. Unsafe rounding allows one to receive new tokens for free \n C. A division by zero allows one to trap/freeze the system \n D. None of the above \nCorrect is B.It's impossible to get any Ether out of this contract, so no draining either. It divides desired_tokens first and only then multiplies by the decimals, this causes any amount of tokens below 10 to result in 0 required_wei_sent. There are no divisions here that could allow a division by 0.","question-5-of-8#Question 5 of 8":"The following issue(s) is/are present in the codebase\n A. A front-running allows one to pay less than expected for tokens \n B. A lack of access control allows one to receive tokens for free \n C. Incorrect balance update allows one to receive new tokens for free \n D. None of the above \nCorrect is C.No requests made before/after a function call would be able to change the token price. All of the functions are intended to be used by users, so no \"access control\" would be possible without excluding users. A user can send all of their tokens to themselve, which will double their balance due to the pre-loaded variable reuse.","question-6-of-8#Question 6 of 8":"The following issue(s) is/are present in the codebase\n A. A reentrancy allows one to drain Ether \n B. A reentrancy allows one to drain the tokens \n C. A reentrancy allows one to receive new tokens for free \n D. None of the above \nCorrect is D.No reentrancies are possible since no external calls are made.","question-7-of-8#Question 7 of 8":"The following issue(s) is/are present in the codebase\n A. An integer overflow allows one to drain Ether \n B. An integer overflow allows one to receive new tokens for free \n C. An integer overflow allows one to trap/freeze the system \n D. None of the above \nCorrect is D.While it is indeed possible to exploit an overflow at the multiplication ((desired_tokens / 10) * decimals){:solidity}, it doesn't allow you to receive FREE tokens (although it makes them a bargain).","question-8-of-8#Question 8 of 8":"The InSecureumToken contract strictly follows the specification of\n A. ERC20 \n B. ERC777 \n C. ERC721 \n D. None of the above \nCorrect is D.Since decimals was defined as uint (same as uint256) and not as uint8 as ERC20 or ERC777 standardized, it can't strictly be either of them. And since this is clearly a fungible token contract, it can't be ERC721."}},"/posts/2021/11/21/secureum-bootcamp-audit-techniques-tools-101-quiz":{"title":"Secureum Bootcamp Audit Techniques & Tools 101 Quiz","data":{"":"This is a writeup of the Secureum Bootcamp Audit Techniques & Tools 101 Quiz containing solutions and references to the provided study material.\nFor fairness, it was published after submissions to it were closed. The quiz consisted of 16 questions with a strict timelimit of 16 minutes. The ordering of the questions was randomized, so the numbering here won‚Äôt match with the numbering elsewhere.\nNovember 8, 2021 by patrickd","question-1-of-16#Question 1 of 16":"Which of the below is/are accurate?\n A. Audits identify all security vulnerabilities and guarantee bug-free code \n B. Audits cover only smart contracts but never the offchain code \n C. Audits suggest fixes for issues identified and aim to reduce risk \n D. None of the above \nCorrect is C.\nAudit Non-goal: Audit is not a security guarantee of ‚Äúbug-free‚Äù code by any stretch of imagination but a best-effort endeavour by trained security experts operating within reasonable constraints of time, understanding, expertise and of course, decidability.\nfrom point 4 of Security Audit Techniques & Tools 101 - by Secureum\nAudit Scope: For Ethereum-based smart-contract projects, the scope is typically the on-chain smart contract code and sometimes includes the off-chain components that interact with the smart contracts.\nfrom point 2 of Security Audit Techniques & Tools 101 - by Secureum\nAudit Goal: The goal of audits is to assess project code (with any associated specification, documentation) and alert project team, typically before launch, of potential security-related issues that need to be addressed to improve security posture, decrease attack surface and mitigate risk.\nfrom point 3 of Security Audit Techniques & Tools 101 - by Secureum\nAudit: It detects and describes (in a report) security issues with underlying vulnerabilities, severity/difficulty, potential exploit scenarios and recommended fixes.\nfrom point 1.1 of Security Audit Techniques & Tools 101 - by Secureum","question-2-of-16#Question 2 of 16":"Audit reports from audit firms typically include\n A. Finding likelyhod/difficulty, impact and severity \n B. Exploit scenarios and recommended fixes \n C. Formal verification of all findings with proofs and counterexamples \n D. All of the above \nCorrect is A, B.\nAudit Reports: include details of the scope, goals, effort, timeline, approach, tools/techniques used, findings summary, vulnerability details, vulnerability classification, vulnerability severity/difficulty/likelihood, vulnerability exploit scenarios, vulnerability fixes and informational recommendations/suggestions on programming best-practices.\nfrom point 13 of Security Audit Techniques & Tools 101 - by Secureum\nAudit Findings Classification: The vulnerabilities found during the audit are typically classified into different categories which helps to understand the nature of the vulnerability, potential impact/severity, impacted project components/functionality and exploit scenarios.\nfrom point 14 of Security Audit Techniques & Tools 101 - by Secureum","question-3-of-16#Question 3 of 16":"These audit techniques are especially well-suited for smart contracts (compared to Web2 programs)\n A. Formal verification because contracts are relatively smaller with specific properties \n B. Fuzzing because anyone can send random inputs to contracts on blockchain \n C. Static source-code analysis because contracts are expected to be open-source \n D. High-coverage testing because contract states and transitions are relatively fewer \nCorrect is A, B, C, D.\nTesting: Smart contract testing has a similar motivation but is arguably more complicated despite their relatively smaller sizes (in lines of code) compared to Web2 software\nfrom point 22.1 of Security Audit Techniques & Tools 101 - by Secureum\nFuzzing is especially relevant to smart contracts because anyone can interact with them on the blockchain with random inputs without necessarily having a valid reason or expectation (arbitrary byzantine behaviour)\nfrom point 24.1 of Security Audit Techniques & Tools 101 - by Secureum\nIt is natural for this Q to be open to interpretation because it is a 100K ft level question comparing techniques \"especially well-suited\" to web3 smart contracts vis-a-vis web2 software, which covers a lot of ground. It is not about what is possible or what is expected but about suitability. All these techniques can be and are performed on web2 applications but generalized aspects of size, scope, nature of user-interactions, source-code availability/expectations and reduced states/transitions of smart contracts make the techniques more suitable for them compared to web2 software. In fact, @dguidotalks specifically about these aspects in what web2 can learn from web3 about security in his SafeCast interview here: https://twitter.com/0xRajeev/status/1454273518154051586\nfrom Rajeev on Secureum Discord","question-4-of-16#Question 4 of 16":"The following kinds of findings may be expected during audits\n A. True positives after confirmation from the project team \n B. False positives due to assumptions from missing specification and threat model \n C. False negatives due to limitations of time and expertise \n D. None of the above \nCorrect is A, B, C.\nFindings may be contested as not being relevant, outside the project‚Äôs threat model or simply acknowledged as being within the project‚Äôs acceptable risk model\nfrom point 89.2 of Security Audit Techniques & Tools 101 - by Secureum\nFalse Positives: are findings which indicate the presence of vulnerabilities but which in fact are not vulnerabilities. Such false positives could be due to incorrect assumptions or simplifications in analysis which do not correctly consider all the factors required for the actual presence of vulnerabilities.\nfrom point 28 of Security Audit Techniques & Tools 101 - by Secureum\nFalse Negatives: are missed findings that should have indicated the presence of vulnerabilities but which are in fact are not reported at all. Such false negatives could be due to incorrect assumptions or inaccuracies in analysis which do not correctly consider the minimum factors required for the actual presence of vulnerabilities.\nfrom point 29 of Security Audit Techniques & Tools 101 - by Secureum\nAuditors generally collate all findings from their review into a report which is handed to the project team. At this point, the assumption from the auditors is that all the findings in their report are true positives. However, depending on the differing threat/trust models or different assumptions made between the audit & project teams, some of the findings may be treated as false positives by the project team which thereafter may choose to ignore such findings, recognize but not act (via fixes) on them, etc. Auditors internally may bring up their individual findings with other team members to discuss if they are indeed true/false. They may also bring up doubtful findings with the project team during an audit (interim discussions/clarifications). Findings which are deemed by everyone as false positives, i.e. irrelevant, will not be included in the report. There may also be disagreements between the auditors & project teams about the threat/trust models, assumptions or difficulty/severity levels, which may lead to opposing viewpoints which are sometimes documented in the reports. But, in general, many of the reported findings are \"confirmed\" by the projects, after which we can think of them as true positives.\nfrom Rajeev on Secureum Discord","question-5-of-16#Question 5 of 16":"Which of the following is/are true?\n A. Audited projects always have clear/complete specification and documentation of all contract properties \n B. Manual analysis is typically required for detecting application logic vulnerabilities \n C. Automated tools like Slither and MythX have no false negatives \n D. The project team always fixes all the findings identified in audits \nCorrect is B.\nVery few smart contract projects have detailed specifications at their first audit stage. At best, they have some documentation about what is implemented. Auditors spend a lot of time inferring specification from documentation/implementation which leaves them with less time for vulnerability assessment.\nfrom point 20.3 of Security Audit Techniques & Tools 101 - by Secureum\nManual analysis is however the only way today to infer and evaluate business logic and application-level constraints which is where a majority of the serious vulnerabilities are being found\nfrom point 27.3 of Security Audit Techniques & Tools 101 - by Secureum\nAutomated analysis using tools is cheap (typically open-source free software), fast, deterministic and scalable (varies depending on the tool being semi-/fully-automated) but however is only as good as the properties it is made aware of, which is typically limited to Solidity and EVM related constraints\nfrom point 27.1 of Security Audit Techniques & Tools 101 - by Secureum\nFindings may be contested as not being relevant, outside the project‚Äôs threat model or simply acknowledged as being within the project‚Äôs acceptable risk model\nfrom point 89.2 of Security Audit Techniques & Tools 101 - by Secureum","question-6-of-16#Question 6 of 16":"Automated tools for smart contract analysis\n A. Are sufficient therefore making a manual analysis unnecessary \n B. Have no false positives whatsoever \n C. Are best-suited for application-level vulnerablities \n D. None of the above \nCorrect is D.\nManual analysis is however the only way today to infer and evaluate business logic and application-level constraints which is where a majority of the serious vulnerabilities are being found\nfrom point 27.3 of Security Audit Techniques & Tools 101 - by Secureum\nAutomated analyzers do not understand application-level logic and their constraints. They are limited to constraints/properties of Solidity language, EVM or Ethereum blockchain.\nfrom point 83.1 of Security Audit Techniques & Tools 101 - by Secureum\nSmart contract security tools are useful in assisting auditors while reviewing smart contracts. They automate many of the tasks that can be codified into rules with different levels of coverage, correctness and precision. They are fast, cheap, scalable and deterministic compared to manual analysis. But they are also susceptible to false positives. They are especially well-suited currently to detect common security pitfalls and best-practices at the Solidity and EVM level. With varying degrees of manual assistance, they can also be programmed to check for application-level, business-logic constraints.\nfrom point 79 of Security Audit Techniques & Tools 101 - by Secureum","question-7-of-16#Question 7 of 16":"Which of the following is/are true?\n A. Slither supports detectors, printers, tools and custom analyses \n B. Echidna is a symbolic analyzer tool \n C. MythX is a combination of static analysis, symbolic checking and fuzzing tools \n D. None of the above \nCorrect is A, C.\nSlither is a Solidity static analysis framework written in Python 3. It runs a suite of vulnerability detectors, prints visual information about contract details, and provides an API to easily write custom analyses. Slither enables developers to find vulnerabilities, enhance their code comprehension, and quickly prototype custom analyses.\nfrom point 33 of Security Audit Techniques & Tools 101 - by Secureum\nEchidna is a Haskell program designed for fuzzing/property-based testing of Ethereum smart contracts.\nfrom point 45 of Security Audit Techniques & Tools 101 - by Secureum\nMythX is a powerful security analysis service that finds Solidity vulnerabilities in your Ethereum smart contract code during your development life cycle. It is a paid API-based service which uses several tools on the backend including a static analyzer (Maru), symbolic analyzer (Mythril) and a greybox fuzzer (Harvey) to implement a total of 46 detectors.\nfrom point 56 of Security Audit Techniques & Tools 101 - by Secureum","question-8-of-16#Question 8 of 16":"Which of the following is/are correct about false positives?\n A. They are findings that are not real concerns/vulnerabilities after further review \n B. They are real vulnerabilities but are falsely claimed by auditors as benign \n C. They are possible with automated tools \n D. None of the above \nCorrect is A, C.\nFalse positives require further manual analysis on findings to investigate if they are indeed false or true positives\nfrom point 28.1 of Security Audit Techniques & Tools 101 - by Secureum\nSmart contract security tools are useful in assisting auditors while reviewing smart contracts. They automate many of the tasks that can be codified into rules with different levels of coverage, correctness and precision. They are fast, cheap, scalable and deterministic compared to manual analysis. But they are also susceptible to false positives.\nfrom point 79 of Security Audit Techniques & Tools 101 - by Secureum","question-9-of-16#Question 9 of 16":"Audit findings\n A. May include both specific vulnerabilities and generic recommendations \n B. May not all be fixed by the project team for reasons of relevancy and acceptable trust/threat model \n C. Always have demonstratable proof-of-concept exploit code on mainnet \n D. None of the above \nCorrect is A, B.\nIt detects and describes (in a report) security issues with underlying vulnerabilities, severity/difficulty, potential exploit scenarios and recommended fixes.\nfrom point 1.1 of Security Audit Techniques & Tools 101 - by Secureum\nIt also provides subjective insights into code quality, documentation and testing.\nfrom point 1.2 of Security Audit Techniques & Tools 101 - by Secureum\nFindings may be contested as not being relevant, outside the project‚Äôs threat model or simply acknowledged as being within the project‚Äôs acceptable risk model\nfrom point 89.2 of Security Audit Techniques & Tools 101 - by Secureum\nCodified exploits should always be on a testnet, kept private and responsibly disclosed to project teams without any risk of being actually executed on live systems resulting in real loss of funds or access\nfrom point 99.2 of Security Audit Techniques & Tools 101 - by Secureum","question-10-of-16#Question 10 of 16":"Which of the following is/are typical manual review approach(es)?\n A. Asset flow \n B. Symbolic checking \n C. Inferring constraints \n D. Evaluating assumptions \nCorrect is A, C, D.\nAudit Techniques: Symbolic checking (automated)\nfrom point 19.6 of Security Audit Techniques & Tools 101 - by Secureum\nManual review approaches: Auditors have different approaches to manual reviewing smart contract code for vulnerabilities. [...] Starting with access control [...] Starting with asset flow [...] Inferring constraints [...] Evaluating assumptions\nfrom point 90 of Security Audit Techniques & Tools 101 - by Secureum","question-11-of-16#Question 11 of 16":"Access control analysis is a critical part of manual review for the reason(s) that\n A. It is the easiest to perform because smart contracts never have access control \n B. It is the fastest to perform because there are always only two roles: users and admins \n C. It is fundamental to security because privileged roles (of which there may be many) may be misused/compromised \n D. None of the above \nCorrect is C.\nWhile the overall philosophy might be that smart contracts are permissionless, in reality, they do indeed have different permissions/roles for different actors who interact/use them.\nfrom point 91.1 of Security Audit Techniques & Tools 101 - by Secureum\nStarting with access control: Access control is the most fundamental security primitive which addresses ‚Äòwho‚Äô has authorised access to ‚Äòwhat.‚Äô (In a formal access control model, the ‚Äòwho‚Äô refers to subjects, ‚Äôwhat‚Äô refers to objects and an access control matrix indicates the permissions between subjects and objects.)\nfrom point 91 of Security Audit Techniques & Tools 101 - by Secureum\nThe general classification is that of users and admin(s). For purposes of guarded launch or otherwise, many smart contracts have an admin role that is typically the address that deployed the contract. Admins typically have control over critical configuration and application parameters including (emergency) transfers/withdrawals of contract funds.\nfrom point 91.2 of Security Audit Techniques & Tools 101 - by Secureum","question-12-of-16#Question 12 of 16":"Which of the following is/are true about vulnerability difficulty and impact?\n A. Difficulty indicates how hard it was for auditors to detect the issue \n B. Difficulty is an objective measure that can always be quantified \n C. Impact is typically classified as High if there is loss/lock of funds \n D. None of the above \nCorrect is C.\nAudit Findings Likelihood/Difficulty: Per OWASP, likelihood or difficulty is a rough measure of how likely or difficult this particular vulnerability is to be uncovered and exploited by an attacker.\nfrom point 15 of Security Audit Techniques & Tools 101 - by Secureum\nMany likelihood and impact evaluations are contentious and debatable between the audit and project teams, typically with security-conscious audit teams pressing for higher likelihood and impact and project teams downplaying the risks.\nfrom point 100.3 of Security Audit Techniques & Tools 101 - by Secureum\nIf there is any loss or locking up of funds then the impact is evaluated as High. Exploits that do not affect funds but disrupt the normal functioning of the system are typically evaluated as Medium. Anything else is of Low impact.\nfrom point 100.2 of Security Audit Techniques & Tools 101 - by Secureum","question-13-of-16#Question 13 of 16":"Application-level security constraints\n A. Are always clearly/completely specified and documented \n B. Have to be typically inferred from the code or discussions with project team \n C. Typically require manual analysis \n D. None of the above \nCorrect is B, C.\nAuditors may need to infer business logic and their implied constraints directly from the code or from discussions with the project team and thereafter evaluate if those constraints/properties hold in all parts of the codebase.\nfrom point 83.3 of Security Audit Techniques & Tools 101 - by Secureum\n[...] However, application-level constraints are rules that are implicit to the business logic implemented and may not be explicitly described in the specification e.g. mint an ERC-721 token to the address when it makes a certain deposit of ERC-20 tokens to the smart contract and burn it when it withdraws the earlier deposit. Such constraints may have to be inferred by the auditors while manually analyzing the smart contract code.\nfrom point 95 of Security Audit Techniques & Tools 101 - by Secureum","question-14-of-16#Question 14 of 16":"Which of the following is/are typically true?\n A. Static analysis analyzes program properties by actually executing the program \n B. Fuzzing uses valid, expected and deterministic inputs \n C. Symbolic checking enumerates individual states/transitions for efficient state space traversal \n D. None of the above \nCorrect is D.\nStatic analysis: is a technique of analyzing program properties without actually executing the program.\nfrom point 23 of Security Audit Techniques & Tools 101 - by Secureum\nFuzzing: or fuzz testing is an automated software testing technique that involves providing invalid, unexpected, or random data as inputs to a computer program.\nfrom point 24 of Security Audit Techniques & Tools 101 - by Secureum\nInstead of enumerating reachable states one at a time, the state space can sometimes be traversed more efficiently by considering large numbers of states at a single step.\nfrom point 25.4 of Security Audit Techniques & Tools 101 - by Secureum","question-15-of-16#Question 15 of 16":"Which of the following is/are generally true about asset flow analysis?\n A. Analyzes the flow of Ether or tokens managed by smart contracts \n B. Assets should be withdrawn only by authorized addresses \n C. The timing aspects of asset withdrawals/deposits is irrelevant \n D. The type and quantity of asset withdrawals/deposits is irrelevant \nCorrect is A, B.\nStarting with asset flow: Assets are Ether or ERC20/ERC721/other tokens managed by smart contracts. Given that exploits target assets of value, it makes sense to start evaluating the flow of assets into/outside/within/across smart contracts and their dependencies.\nfrom point 92 of Security Audit Techniques & Tools 101 - by Secureum\nWho: Assets should be withdrawn/deposited only by authorised/specified addresses as per application logic\nfrom point 92.1 of Security Audit Techniques & Tools 101 - by Secureum\nWhen: Assets should be withdrawn/deposited only in authorised/specified time windows or under authorised/specified conditions as per application logic (when)\nfrom point 92.2 of Security Audit Techniques & Tools 101 - by Secureum\nWhat type: Assets, only of authorised/specified types, should be withdrawn/deposited as per application logic\nfrom point 92.6 of Security Audit Techniques & Tools 101 - by Secureum\nHow much: Assets, only in authorised/specified amounts, should be withdrawn/deposited as per application logic\nfrom point 92.7 of Security Audit Techniques & Tools 101 - by Secureum","question-16-of-16#Question 16 of 16":"Which of the following is/are generally true about control and data flow analyses?\n A. Interprocedural control flow is typically indicated by a call graph \n B. Intraprocedural control flow is dictated by conditionals (if/else), loops (for/while/do/continue/break) and return statements \n C. Interprocedural data flow is evaluated by analyzing the data used as argument values for function parameters at call sites \n D. Interprocedural data flow is evaluated by analyzing the assignment and use of variables/constants along control flow paths within function \nCorrect is A, B, C, D.\nEvaluating control flow: Interprocedural (procedure is just another name for a function) control flow is typically indicated by a call graph which shows which functions (callers) call which other functions (callees), across or within smart contracts\nfrom point 93.1 of Security Audit Techniques & Tools 101 - by Secureum\nEvaluating control flow: Intraprocedural (i.e. within a function) control flow is dictated by conditionals (if/else), loops (for/while/do/continue/break) and return statements.\nfrom point 93.2 of Security Audit Techniques & Tools 101 - by Secureum\nEvaluating data flow: Interprocedural data flow is evaluated by analyzing the data (variables/constants) used as argument values for function parameters at call sites\nfrom point 94.1 of Security Audit Techniques & Tools 101 - by Secureum\nEvaluating data flow: Intraprocedural data flow is evaluated by analyzing the assignment and use of (state/memory/calldata) variables/constants along the control flow paths within functions.\nfrom point 94.2 of Security Audit Techniques & Tools 101 - by Secureum"}},"/posts/2021/11/28/secureum-bootcamp-audit-findings-101-quiz":{"title":"Secureum Bootcamp Audit Findings 101 Quiz","data":{"":"This is a writeup of the Secureum Bootcamp Audit Findings 101 Quiz with solutions.\nFor fairness, it was published after submissions to it were closed. All questions are concerning the same snippet of code. No syntax highlighting or indentation was used in the original quiz, so it was skipped here as well. Make sure to read code comments carefully. The quiz consisted of 8 questions with a strict timelimit of 16 minutes. The ordering of the questions was randomized, so the numbering here won‚Äôt match with the numbering elsewhere.\nThis Quiz from Epoch 0 was declared to be RACE-2 of Epoch Infinity.\nNovember 28, 2021 by patrickd","code#Code":"All 8 questions in this quiz are based on the InSecureumDAO contract snippet. This is the same contract snippet you will see for all the 8 questions in this quiz.The InSecureumDAO contract snippet illustrates some basic functionality of a Decentralized Autonomous Organization (DAO) which includes the opening of the DAO for memberships, allowing users to join as members by depositing a membership fee, creating proposals for voting, casting votes, etc. Assume that all the other functionality (that is not shown or represented by ...) is implemented correctly","question-1-of-8#Question 1 of 8":"Based on the comments and code shown in the InSecureumDAO snippet\n A. DAO is meant to be opened only by the admin by making an Ether deposit to the contract \n B. DAO can be opened by anyone by making an Ether deposit to the contract \n C. DAO requires an exact payment of membershipFee to join the DAO \n D. None of the above \nCorrect is A, B, C.While the payable openDAO() function is protected by the correctly implemented onlyAdmin modifier, it is always possible to force send Ether into a contract via selfdestruct(). The onlyWhenOpen() modifier only checks for the contracts own balance which can be bypassed by doing that. The payable join() function indeed checks for the msg.value to exactly match membershipFee.","question-2-of-8#Question 2 of 8":"Based on the comments and code shown in the InSecureumDAO snippet\n A. Guarded launch via circuit breakers has been implemented correctly for all state modifying functions \n B. Zero-address check(s) has/have been implemented correctly \n C. All critical privileged-role functions have events emitted \n D. None of the above \nCorrect is D.All state modifying functions, that can be accessed by users other than admin, are indeed correctly \"protected\" by the onlyWhenOpen modifier, but that modifier is, as explained in the previous answer, not correctly implemented itself. The only zero-address check is made during construction, but it's currently ensuring that the admin will always be the zero-address - it's therefore doing the opposite of a correctly implemented zero-address-check. There are several functions that sound more than critical enough to have events (eg. removeAllMembers), but this contract isn't using events at all.","question-3-of-8#Question 3 of 8":"Reentrancy protection only on join() (assume it‚Äôs correctly specified) indicates that\n A. Only payable functions require this protection because of handling msg.value \n B. join() likely makes untrusted external call(s) but not the other contract functions \n C. Both A and B \n D. Neither A nor B \nCorrect is B.A simply sounds like nonesense. Since it says that we should assume that reentrancy protection has been used correctly, and a reentrancy vulnerability requires making untrusted external calls, we can assume that it is at least likely, although not certain, that other functions do not.","question-4-of-8#Question 4 of 8":"Access control on msg.sender for DAO membership is required in\n A. createVote() to prevent non-members from creating votes \n B. castVote() to prevent non-members from casting votes \n C. getWinningOutcome() to prevent non-members from viewing winning outcomes \n D. None of the above \nCorrect is A, B.It wouldn't make much sense to pay a membership fee if you are allowed to create and cast votes without it. There's no clear reason to prevent non-members from accessing winning outcomes though, since they'd be publicly readable on the blockchain anyway.","question-5-of-8#Question 5 of 8":"A commit/reveal scheme (a cryptographic primitive that allows one to commit to a chosen value while keeping it hidden from others, with the ability to reveal the committed value later) is relevant for\n A. join() to not disclose msg.sender while joining the DAO \n B. createVote() to not disclose the possible outcomes during creation \n C. castVote() to not disclose the vote being cast \n D. All the above \nCorrect is C.It's not possible to hide the msg.sender using a simple commit/reveal scheme and it wouldn't make much sense to try, since there's no clear advantage from temporarily hiding your membership. It also wouldn't make much sense to not disclose possible outcomes of a new vote, unless you want to make your members vote blindly on the options. It does make sense to hide what you are voting for until voting closes, since this makes it impossible to calculate how many members voted for a specific option and how many fake/sibyl members you'd exactly need to create in order to manipulate the vote.","question-6-of-8#Question 6 of 8":"Security concern(s) from missing input validation(s) is/are present in\n A. createVote() for duplicate _voteId \n B. castVote() for existing _voteId \n C. getWinningOutcome() for existing _voteId \n D. setMembershipFee() for sanity/threshold checks on _fee \nCorrect is A, D.The createVote() function currently allows overwriting existing votes by specifying a previously used _voteId. It would probably be better to use an array instead of a mapping here and simply push new votes into it. castVote() has a modifier in place ensuring that a vote can only be cast on existing votes. Since function body should be assumed as correctly implemented, we should also assume there are no security concerns in regards to validation either. Without sanity/threshold checks when setting fees in setMembershipFee(), admins could practically close the DAO off, preventing new members from joining, which could certainly be considered a security concern for the protocol.","question-7-of-8#Question 7 of 8":"removeAllMembers() function\n A. Will not work as expected to remove all the members from the DAO \n B. Will work as expected to remove all the members from the DAO \n C. Is a critical function missing an event emission \n D. None of the above \nCorrect is A, C.What the function actually does is only removing the admin as member from the DAO, he'd still stay the admin though. Properly implementing this function would actually be rather difficult, since a simple delete on the mapping variable without specifying a key would not actually delete any of its values. Assuming it would work as advertised by its name you can certainly say it would be a critical function that should emit an event, which it currently does not.","question-8-of-8#Question 8 of 8":"InSecureumDAO will NOT be susceptible to something like the 2016 ‚ÄúDAO exploit‚Äù\n A. Because it derives from ReentrancyGuard.sol which protects all contract functions by default \n B. Only if it does not have a withdraw Ether function vulnerable to reentrancy and makes no external calls \n C. Because Ethereum protocol was fixed after the DAO exploit to prevent such exploits \n D. Because Solidity language was fixed after the DAO exploit to prevent such exploits \nCorrect is B.The 2016 \"DAO exploit\" was indeed a reentrancy issue caused by an external call within an Ether withdrawal function. But simply inheriting from ReentrancyGuard.sol will not prevent them since you actually have to apply the nonReentrant modifier to relevant functions. There have indeed been some efforts to prevent reentrancy issues with changes made in Ethereum and Solidity, but none of them can be considered a \"fix\". A recurrence of the 2016 \"DAO exploit\" is indeed still possible, although more unlikely since, thanks to all the attention it got, this anti-pattern is now widely known and rarely found anymore during audits."}},"/posts/2021/11/13/damn-vulnerable-defi-v2-part-1-setup-and-challenges-1-to-4":{"title":"Damn Vulnerable DeFi V2 - Part #1: Setup And Challenges 1 To 4","data":{"":"November 13, 2021 by patrickd\n@tinchoabbate has recently released an updated version of the Damn Vulnerable DeFi, modernized with current Solidity version, tooling and more levels. After having recently completed OpenZeppelin's Ethernaut and smarx's Capture the Ether it's finally time to tackle, what I expect to be the most challenging CTF of these.\nSpoilers! A This is a writeup and spoiling all of the fun lies in its nature.","setup#Setup":"While the other CTFs so far could basically be played using the Browser only (eg. by mostly making use of Remix) this one appears to need some local setup, so we'll start from a fresh Ubuntu box.","clone-the-repository#Clone the repository":"git clone https://github.com/tinchoabbate/damn-vulnerable-defi.git","enter-the-repository#Enter the repository":"cd damn-vulnerable-defi/","check-out-the-latest-version#Check out the latest version":"git checkout v2.0.0","install-nodejs-version-manager#Install NodeJS version manager":"curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\nRestart your terminal so the next commands are available","install-nodejs-v14#Install NodeJS v14":"nvm install 14","install-yarn-globally-for-current-node-version#Install yarn globally for current node version":"npm install -g yarn","install-dependencies#Install dependencies":"yarn\nMake sure to run this within the repository to install dependencies\nRight off the bat it's clear that, since this is all running locally, there's no highscore lists or any other sort of tracking. This is less of a test that you take and more of a practice book that you are given. You're free to cheat yourself (eg. by adjusting the success conditions), but its not like that'll do you any good. But most importantly, that also means there are no unexpected surprises here: No inconsistencies between what's actually deployed and the code that you're given - what you see is all there is, and that is great.","challenge-1---unstoppable#Challenge #1 - Unstoppable":"There's a lending pool with a million DVT tokens in balance, offering flash loans for free.If only there was a way to attack and stop the pool from offering flash loans ...You start with 100 DVT tokens in balance.\nIn code review we often say, if you want to understand the code, first read the tests. Since all of these challenges seem to be setup, exploited and checked for success by tests, I think this is a good place to start.What we find in unstoppable.challenge.js is a very simple setup. A token \"DamnValuableToken\" (DVT) is created, of which 1000000 \"ether\" (==  token) are deposited into UnstoppableLender (pool) contract, and 100 tokens are send to our attacker EOA account. Finally, a quick test is run using the ReceiverUnstoppable contract which takes a flash loan from UnstoppableLender and pays it back immediately.Looking at the UnstoppableLender contract, the first thing that immediately pops into my eyes is the fact that it's keeping track of the poolBalance instead of relying on checking its actual balance from the DVT token contract. This could lead to \"accounting errors\", when tokens are sent to the contract directly without using the depositTokens() function.\nAnd indeed, as we can see here (line 40): Before allowing to take a flash loan it always checks whether the actual token balance exactly matches the balance tracked within the poolBalance variable. So we should be able to break this contract simply by sending it one \"unsolicited\" DVT token.Now how to write the exploit? It seems that the setup step already gives us all of the javascript we'll be needing to write it, we basically just have to do some copy and pasting.\nThe first time we run yarn run unstoppable to check our solution, there'll be some more first-time setup going on (downloading various solc versions and compiling the contracts).\nIf you, like me at first, installed the newest nodejs version you might run into Error: error:0308010C:digital envelope routines::unsupported, ERR_OSSL_EVP_UNSUPPORTED errors. In that case you simply have to switch to nodejs v14 with nvm install 14, remove the node_modules folder and rerun yarn to re-install dependencies.\nLearning: Don't keep track of balances if you don't have to. And if you do, always assume that your balance might end up being inconsistent with the real one. In case of UnstoppableLender it would have been better to check the balance with assert(poolBalance <= balanceBefore);{:solidity}, allowing the actual balance to be larger than the accounted one.","challenge-2---naive-receiver#Challenge #2 - Naive receiver":"There's a lending pool offering quite expensive flash loans of Ether, which has 1000 ETH in balance.You also see that a user has deployed a contract with 10 ETH in balance, capable of interacting with the lending pool and receiving flash loans of ETH.Drain all ETH funds from the user's contract. Doing it in a single transaction is a big plus ;)\nFrom the description alone, I somewhat assume that the user's contract isn't properly checking that it's actually interacting with the flash loan contract, which would allow us to get back a loan that we've never given? And doing it within a single transaction likely requires writing an exploit contract instead of doing it from the javascript test suite.The test setup is quite similar to before, just this time it's not about tokens but ether and it's not us and the pool getting the initial balances but the pool and the user's flash loan receiving contract. It seems a bit strange that the user's EOA account isn't actually used to deploy the FlashLoanReceiver contract, maybe an oversight.The most interesting part here is the success condition: I initially assumed we had to drain the user's contract and obtain the ether into our attacker EOA account, but instead it expects that the pool ends up with the user's ether. So most likely we need to force the user's contract to take unsolicited flash loans and force them to pay high fees draining the contract's funds.And indeed, NaiveReceiverLenderPool's flashLoan() function allows specifying a borrower instead of assuming the message sender is the borrower. And furthermore, FlashLoanReceiver does not appear to have any checks on whether it actually \"asked\" for a flash loan in the first place.With the receiver contract having initial funds of 10 ether and each flash loan costing 1 ether, that means we need to force it to accept 10 flash loans in order to drain all of its funds. This also nicely explains why the challenge description makes it sound like there's extra points for doing it within a single transaction (from our own contract).But first, let's do it the easy way, let's trigger 10 flash loans in 10 separate transactions via javascript alone:\nThat works, nice and easy. But let's get those extra points now, by putting the flashLoan() calls within a contract, allowing us to drain the funds within a single transaction:\nThen by adjusting the test suite to deploy the Exploit contract, triggering its constructor:\nLearning: It's not enough to make sure that your callbacks can only be called by certain whitelisted addresses, you also need to make sure you expected them to call in the first place.","challenge-3---truster#Challenge #3 - Truster":"More and more lending pools are offering flash loans. In this case, a new pool has launched that is offering flash loans of DVT tokens for free.Currently the pool has 1 million DVT tokens in balance. And you have nothing.But don't worry, you might be able to take them all from the pool. In a single transaction.\nThe name seems to imply there's too much trusting going on here, so I expect some kind of permission/authentication issue.Starting with truster.challenge.js we can see that the same DVT tokens are back and all of the tokens are immediately and directly transferred into the pool, no deposit/add liquidity function this time. The success condition is, somehow transferring all of them to the attacker EOA account.There's only TrusterLenderPool.sol to look at this time. The first thing that seems out of place is the use of a low-level function call with userinput as calldata to any target address.So the question now is, what could that target be that could cause something bad to happen? A hint is basically directly above the functionCall(): damnValuableToken.transfer(borrower, borrowAmount);{:solidity} ‚Äì it transfers the specified amount of tokens to the borrower ‚Äì from whom? From the contract calling, because the ERC20 transfer function basically authenticates using msg.sender.We're allowed to make a functionCall just like that while being able to use the TrusterLenderPool as msg.sender and we could use it to give ourselves an ERC20 allowance in TrustLenderPools' name. But hold on, the borrower also needs to transfer back the tokens it received and ERC20 doesn't have callbacks allowing the receiver to react to a token transfer ‚Äì this is the original intention of the functionCall after all, to tell the borrower contract that it should now make use of the loan and pay it back once finished. So can we somehow send the tokens back on time or prevent receiving them in the first place?Well, how about we borrow an amount of 0 tokens? There appears to be no check whether we're borrowing anything at all, and as long as neither sender nor receiver are zero-addresses OpenZeppelin's ERC20 implementation doesn't seem to care about a transaction of 0 tokens either. In this case we can basically call the flashLoan() function without actually taking any loans but being able to call any function on any address in its name!We're again challenged to do it all within a single transaction, so let's build our borrowing exploit contract:\nAnd like before we adjust the test suite to deploy our Exploit contract:\nWhoops! At first, I was planning to call all of my contracts \"Exploit.sol\", assuming it's sufficient for each of them to be within separate directories but it turned out that they are all within the same namespace which is why I got HardhatError: HH701: There are multiple artifacts for contract \"Exploit\", please use a fully qualified name.. So in the end I had to rename Exploit to TrusterExploit to make it work.\nLearning: A reentrancy-guard is no silver bullet to prevent bad things that can happen from making calls to other contracts! In this specific case, the biggest issue is allowing to specify a call-target that is different from the borrower contract. It would've been a lot saver to require the msg.sender to be both borrower and target, expecting the borrower to implement a specific interface and not allow arbitrary data to be passed through.Most importantly though, don't forget that when you make a call to another contract (eg. a token), that contract assumes that the message you are sending has been authenticated and is purposeful. You wouldn't let other people use your browser while you're still logged in everywhere, would you?","challenge-4---side-entrance#Challenge #4 - Side entrance":"A surprisingly simple lending pool allows anyone to deposit ETH, and withdraw it at any point in time.This very simple lending pool has 1000 ETH in balance already, and is offering free flash loans using the deposited ETH to promote their system.You must take all ETH from the lending pool.\nI'm calling it now: Entrance, sounds like re-entrancy and side means re-entrancy not through the same function but through another!But first, let's look at the tests again. The setup is even more simple this time: There's one pool contract, 1000 ether instead of tokens, which are put into the pool via a deposit() function. The success conditions are that the pool has been drained of all ether and that the attacker EOA's balance is greater than at the start.And the first thing that stands out to me after my initial call is, that SideEntranceLenderPool does indeed not make use of OpenZeppelin's ReentrancyGuard at all, although all previous contracts did.The withdraw() function makes an external call to the msg.sender but does so while making use of the checks-effects-interactions pattern (by first setting the balance to 0 and only then making the call) and is therefore not susceptible to reentrancy on its own.The interesting part lies in flashLoan() and how it checks that the loan has been paid back: It does so by ensuring the contract's overall balance stays the same ‚Äì but what it does not, is checking the balances mapping. The sum of all balances within the mapping should always be equal (or less, since unsolicited ether can be forced into the contract) to the actual contract's balance. But the sum of balances should, beside of a flash loan being in progress, never be higher than the actual amount of ether available in the contract.This allows us to take a flash-loan and pay it into the pool contract via the deposit() function, where the deposit will be accredited to us. That's fine because the pool now thinks we have returned it since its balance is the same as before. After having done that we can simply withdraw all of the pool's funds, while all the other contributors to the pool are rekt.Again, we'll write an Exploit contract, but this time it needs to have a callback so we can't do it all within the constructor:\nAnd the test now needs to start the exploit by calling pwn() after deployment:\nLearning: Even if you implement the checks-effects-interactions pattern properly within each individual function, you might still want to consider making use of a ReentrancyGuard if you're not sure that there might be a reentrancy possible by combining these functions in some manner. That would've been an easy fix here, although not cheap in regards to gas costs. Another solution would be to keep track of the sum of deposits and make sure that it's never higher than the actual contract balance."}},"/posts/2021/11/28/secureum-bootcamp-audit-findings-201-quiz":{"title":"Secureum Bootcamp Audit Findings 201 Quiz","data":{"":"This is a writeup of the Secureum Bootcamp Audit Findings 201 Quiz with solutions.\nFor fairness, it was published after submissions to it were closed. All questions are concerning the same snippet of code. No syntax highlighting or indentation was used in the original quiz, so it was skipped here as well. Make sure to read code comments carefully. The quiz consisted of 8 questions with a strict timelimit of 16 minutes. The ordering of the questions was randomized, so the numbering here won‚Äôt match with the numbering elsewhere.\nThis Quiz from Epoch 0 was declared to be RACE-3 of Epoch Infinity.\nNovember 28, 2021 by patrickd","code#Code":"All 8 questions in this quiz are based on the InSecureumNFT contract snippet. This is the same contract snippet you will see for all the 8 questions in this quiz.InSecureumNFT is a NFT project that aims to distribute CryptoSAFU NFTs to its community where most of them are fairdropped based on past contributions and a few are sold. CryptoSAFUs with lower IDs have more unique traits, may be valued higher and therefore require a random distribution for fairness. Assume that all strictly required ERC721 functionality (not shown) and any other required functionality (not shown) are implemented correctly. Only functionality specific to the sale and minting of NFTs is shown in this contract snippet.","question-1-of-8#Question 1 of 8":"Missing zero-address check(s) in the contract\n A. May allow anyone to start the sale \n B. May put the NFT sale proceeds at risk \n C. May burn the newly minted NFTs \n D. None of the above \nCorrect is B.While the require statement in startSale() states that only the deployer may call the function AND the price needs to be not zero, the actual code uses OR which allows anyone to start the sale as long as they specify a valid price - but that can't be fixed by adding a zero-address check. All proceeds appear to be intended to go to the benificiary and since there's no validation of the _benificiary address when it is set during construction, a zero-address could indeed put the sale proceeds to risk. In the given code, the internal _mint(_to) function is always called with msg.sender as _to value which can't be a zero-address.","question-2-of-8#Question 2 of 8":"Given that lower indexed/numbered CryptoSAFU NFTs have rarer traits (and are considered more valuable as commented in _mint), the implementation of InSecureumNFT is susceptible to the following exploits\n A. Buyers can repeatedly mint and revert until they receive desired NFT \n B. Buyers can generate addresses to mint until they receive desired NFT \n C. Miners can manipulate block.timestamp to facilitate minting of desired NFT \n D. None of the above \nCorrect is A, B, C.The index of a CryptoSAFU NFT depends on a nonce that increases after every mint and has an internal visibility preventing contracts to read its current value easily, which would allow them to predict an index for the current block. But a prediction is not necessary since a contract can simply call _mint() repeatedly every block and revert if the result is not desired, ensuring a refund. The msg.sender is indeed also a variable for the \"random\" index generation, although it's very effective exploiting it, since you'd still have to pay the full price for each of those attempts because the nonce will change after each buy. There's also no need to generate a new address, you can just keep buying using the same address until you receive the desired NFT. A miner would indeed be able to pre-calculate a desirable index off-chain by picking a specific block.timestamp and adding their mint-transaction to the beginning of their block.","question-3-of-8#Question 3 of 8":"The getPrice() function\n A. Is expected to reduce the mint price over time after sale starts \n B. Allows free mints after ~13337 blocks from when startSale() is called \n C. Visibility should be changed to external \n D. None of the above \nCorrect is A, B.The price is multiplied by (saleDuration - elapsed):{:solidity} and while saleDuration stays constant, elapsed will increase over time, making the multiplicator value and therefore the price lower over time. There's indeed a possibility for a free mint when saleDuration and elapsed have exactly the same value, which is not a very likely scenario though. Once elapsed is larger than saleDuration the subtraction will won't underflow since that is handled by if (elapsed > saleDuration){:solidity}. Since this function is called internally, it wouldn't make much sense to change its visibility to external.","question-4-of-8#Question 4 of 8":"InSecureumNFT contract is\n A. Not susceptible to reentrancy given the absence of external contract calls \n B. Not susceptible to integer overflow/wrapping given the compiler version used and the absence of unchecked blocks \n C. Susceptible to reentrancy during minting \n D. Perfectly safe for production \nCorrect is B, C.There are multiple external contract calls. The compiler version and absence of unchecked blocks should indeed prevent integer overflows/wrapping, but instead will cause reverts which could lead to Denial of Service. The fact that mint() keeps checking the current balance instead of the actual msg.value and that the onERC721Received hook is called before the balance is transferred to the beneficiary, can indeed be exploited using a reentrancy attack.","question-5-of-8#Question 5 of 8":"Assuming InSecureumNFT contract is deployed in production (i.e. live for users) on mainnet without any changes to shown code\n A. Use of evident test configuration will cause fewer NFTs to be minted than expected in production \n B. Illustrates the lack of best-practice for test parameterization to be removed or kept separate from production code \n C. It will behave as documented in code to mint the expected number of NFTs in production \n D. None of the above \nCorrect is A, B.Multiple comments throughout the code show a discrepancy between the configuration expected in production and the actual configuration that is currently implemented. A much better way to do it, would be parameterization by setting these values during construction, which allows using the same code without changes for both mainnet and testnets.","question-6-of-8#Question 6 of 8":"The function startSale()\n A. May be successfully called/executed by anyone \n B. May be successfully called/executed with _price of 0 \n C. Must be called for minting to happen successfully \n D. None of the above \nCorrect is A, B, C.While the require statement in startSale() states that only the deployer may call the function AND the price needs to be not zero, the actual code uses OR which allows anyone to start the sale as long as they specify a valid price. This also means that a price of 0 can be successful if the caller is the deployer. The mint() function requires publicSale to be true, which can only happen by calling startSale().","question-7-of-8#Question 7 of 8":"The minting of NFTs\n A. Requires an exact amount of ETH to be paid by the buyer \n B. Refunds excess ETH paid by buyer back to the buyer \n C. Transfers the NFT salePrice to the beneficiary address \n D. May be optimized to prevent any zero ETH transfers in its refund mechanism \nCorrect is B, C, D.Thanks to the refund mechanism after the actual price has been determined, the buyer does not need to send an exact amount of ETH. After refunding the buyer, what is left in the contracts balance and sent to the benificiary should indeed be the salePrice. The refund mechanism can be optimized by skipping transfers when the current balance equals the price exactly.","question-8-of-8#Question 8 of 8":"The NFT sale\n A. May be restarted by anyone any number of times \n B. Can be started exactly once by deployer \n C. Is missing an additional check on publicSale \n D. Is missing an event emit in startSale \nCorrect is A, C, D.startSale() is not checking whether publicSale is already true, allowing saleStartTime to be reset and also overwriting the price and can indeed be called by anyone since the authentication can be bypassed by simply specifying a _price unequal to 0. The start of the sale would certainly be a good point to log an event, but events are currently completely missing from the contract."}},"/posts/2021/12/1/damn-vulnerable-defi-v2-5-the-rewarder":{"title":"Damn Vulnerable DeFi V2 - #5 The Rewarder","data":{"":"December 1, 2021 by patrickd\nThis is Part 2 of the Damn Vulnerable DeFi V2 writeup. You can find the Setup and Challenges 1 to 4 in the previous article.\nRemember, don't read this unless you really want to be spoiled!","code-review#Code Review":"There's a pool offering rewards in tokens every 5 days for those who deposit their DVT tokens into it.Alice, Bob, Charlie and David have already deposited some DVT tokens, and have won their rewards!You don't have any DVT tokens. But in the upcoming round, you must claim most rewards for yourself.Oh, by the way, rumours say a new pool has just landed on mainnet. Isn't it offering DVT tokens in flash loans?\nThis is a big one! Four contracts and even the test suite has a lot of code. And it sounds like we have to make use of flash loans in order to get rewards despite not being able to lock the loans up for 5 days?Let's not get overwhelmed and start with the test setup again. The flash loan pool gets a million token right from the start, and each of the 4 people mentioned get 100 DVT, which are immediately deposited by them into the rewarder pool. After this initial setup, time is advanced by 5 days and a round of rewards is paid out: 25 reward tokens for each person.The success conditions require that the participants so far, only get 0.01 or less reward tokens, while the attacker earns nearly 100 tokens. All of this must happen in the next round and the attacker may not have any DVT tokens at the end (cause he is supposed to borrow them).Starting with the contract I expect to have the least code, RewardToken.sol is a relatively simple ERC20 token, with the only difference to DVT that its creator may mint unlimited amounts of it. I didn't expect anything to stand out here but actually that \"@dev A mintable ERC20 with 2 decimals to issue rewards\" comment is rather strange. That is because the default decimals() function of OpenZeppelin's ERC20 implementation returns 18 and there's nothing in RewardToken overriding that. Did we already stumble upon the issue? Inconsistent handling of decimals? Or just an oversight from the refactoring to v2?Let's next look at FlashLoanerPool.sol since we should be fairly familiar with it at this point and it's probably the tool we're supposed to use to exploit the rewarder contract with. And yes, it's the typical pattern, just this time the borrowing contract needs to implement a receiveFlashLoan(uint256) function to be notified about the loan.Next AccountingToken.sol, which I expect are given in return for DVT deposits made into the rewarder contract? It looks more complicated than it is: Basically its creator may freely mint and burn tokens, to and from any address. Transfers and allowances appear to have been disabled. And it's possible to create snapshots of the current balances (accessible via balanceOfAt()), that are likely made for each round of rewards. See OpenZeppelin's ERC20 Snapshot extension for more information.Finally we look at where the magic is supposed to happen, TheRewarderPool.sol's constructor looks as expected: It deploys the accounting and reward token contracts, and then creates the first snapshot starting the first round. The inline comment \"// Assuming all three tokens have 18 decimals\" stands a bit out though, while it's correct it's in conflict with the pevious comment we found in the RewardToken contract.Continuing on, it has 3 public/external functions that can be called by anyone: deposit(uint), withdraw(uint) and distributeRewards(). Deposit and withdraw do just about what you would expect, they exchange your DVT with the accounting tokens, with the only difference that depositing also triggers the reward distribution function. distributeRewards() first checks whether enough time has passed to start a new round, if so it creates a snapshot. Either way, it calculates the callers rewards based on how much he has deposited compared to all other users. It then checks whether the caller has already retrieved their reward for the current round, if not, the calculated amount of reward tokens is minted.After understanding how reward distribution works, I finally noticed the issue: If you time it right and start the new round with your deposit, you can claim the reward for that round immediately and withdraw. Doing all of that from another contract, meaning in a single transaction, you can make use of a flashloan and claim the majority of the rewards. I'm not quite sure about this yet though, since it requires you to manipulate time and seems a bit easy to be honest - but let's try it:","exploit#Exploit":"That's a lot of interfaces now, but I left them in place instead of importing the contracts thinking it probably makes the code more easily understood when read here on the web.Just like in the previous challenge we again have to deploy this contract and call the pwn() function. But this time we have to make sure to trigger the exploit only when the timing is right: We need to be the ones starting the new round.\nAnd indeed, it appears we've solved it the intended way, I think?","conclusion#Conclusion":"Learning? It's not so simple this time, since it wasn't a typical \"vulnerability\" but rather a naive implementation. Even if you'd force the round-start (snapshot) to be within a different transaction/block than the deposit of liquidity in order to prevent attacks from flash loans, a user with lots of tokens could still simply deposit them at the end of a round and withdraw immediately after claiming their rewards after the new round was started. Your protocol probably wants to incentivize long-term staking and not, short-term arbitrage trades. So it would probably be best to get rid of rounds and instead reward users for every second they are providing liquidity like many modern DeFi projects are doing today."}},"/posts/2021/2/20/input-validation-json-schemas-best-practices":{"title":"Input Validation With JSON Schemas: Best Practices","data":{"":"February 20, 2021 by patrickd\nIn a previous article we discussed how AJV can be used to build API middlewares for validating user input with JSON schemas. This article builds on it by providing an example set of rules that can be implemented as a best practice when writing schemas.","validate-before-usage#Validate before usage":"It should generally be avoided to make use of any user input without validating it first.\nThis is one of the things that should be checked during code review but you might even want to consider having automated processes in place to detect these kinds of problems. This could mean making it impossible to use input variables without having applied a validator function on them, therefore causing an error. Or by implementing things like a custom ESLint rule that will check whether a variable seems to have been validated before usage ‚Äì these are likely to be prone to false positives though.The following is a (incomplete) list of possible User Input sources in a typical web application:\nHTTP Headers\nURI\nPath\nPath parameters (eg. id in /user/:id)\nQuery parameters\nHost Name (maybe derived from other headers, eg. X-Forwarded-Host)\nContent Type\nContent Length\nCookies (both their names and values)\nReferer\nIP Address (specifically when it's derived from other headers, eg. X-Forwarded-For)\nProtocol / Port (eg. X-Forwarded-Proto)\nUser Agent\n...\nHTTP Body\nRaw/Binary (consider checking \"magic bytes\", running anti-virus, etc.)\nSerialized information (json, xml, etc.: decode and validate)\nMulti-Part-Uploads (part-separators, part-length, ...)\n...\nWeb-Socket Messages\nDatabase Entries created with User Input (these should already be validated, but usually not sanitized)\nCached User input\nForwarded User input (Inter-Process-Communication)\nFetched User input from 3rd-party APIs (even if it is not \"User input\", you may not want to fully trust the 3rd-parties security or consistency)\n...\nRemember that any validation made by your frontend clients can usually easily be bypassed and can't be relied upon.\nBut we still want to do as much validation as early as we can. Even if we already have some validation in place at deeper parts of our program (eg. Database Schemas), from a standpoint of application security these measures usually take effect \"too late\". We want to reject any unreasonable values from ever reaching our business logic in the first place.","validation-should-be-as-strict-as-reasonably-possible#Validation should be as strict as reasonably possible":"Structure and contents of User Input should be within the expectations of our business logic.\nTo ensure this, we want to define our schemas as explicitly as possible, making sure that anything outside of expectations, is rejected by the validator function. This again, should be double-checked during manual code review and, if possible, through automated means.","object-validation#Object validation":"As shown in the previously, the JSON Schema validator AJV supports the automatic removal of any properties that are not explicitly defined within an Object's schema with removeAdditional: true.\nadditionalProperties should always be present\nPreferably we want to be able to always specify additionalProperties: false to ensure the removal of properties we do not expect to be present. Any properties that we do expect should be explicitly defined within the properties. It should not be forgotten to also set this on all Sub-Object's defined within the parent Object's properties.\nBusiness logic that requires allowing additional properties or generally, objects with unknown properties should be avoided. If this is not possible (for example for key-value maps) other restrictions should be enforced (such as propertyNames, patternProperties, minProperties, maxProperties).\nrequired should always be present\nIf possible, any fields used by our business logic should be required to be present. For more complicated cases, like where the presence of a field is only required if another field is present as well, the dependencies option can be used.\nEven if no properties can be specified as required, we should still explicitly state this fact within the schema by specifying an empty array.","string-validation#String validation":"One of the following options should always be present, preferably used in the following order:\nenum\nformat (build-in or custom)\npattern\nThe regular expression should match the full string (start with ^ and end with $).\nIt should have an easy-to-understand explanation of what it is supposed to match in the code-comments (reading and understanding regular expressions is not easy).\nIt should have a variation of unit tests ensuring it works as intended.\nPatterns that are re-used across various schemas should be defined as custom formats (and therefore used via the format option).\nminLength & maxLength\nIf there's no minimum length that can be derived from the implemented use-case, a minimum of 1 should be used to ensure truthyness of the value. If possible, maxima should follow already existing restrictions set by database schemas.\nName, Address, Phone, Title and Identifier-like fields can generally be restricted to a lenient maximum of 512 characters.\nFree-Text and Description-like fields should be restricted to a reasonable maximum within the database's capabilities. For example, a lenient maximum of 1048576 (1 MiB) could be possible for MongoDB (which supports an overall 16MiB within a single document) assuming there aren't too many other fields allowing for such sizeable inputs.\nUse-cases that are re-used across various schemas should be defined as custom formats.","numeric-validation#Numeric validation":"There are two numeric types, one for whole numbers (type: 'integer') and one for fractional numbers (type: 'number'). Prefer integer over number whenever possible.One of the following options should always be present, preferably used in the following order:\nenum\nminimum/exclusiveMinimum and/or maximum/exclusiveMaximum\nWhenever there are no clear minima or maxima based on the use-case, it should at least be ensured that numeric value has the correct sign (negative allowed? zero allowed? positive allowed?).\nDepending on how you continue to process the value, you might want to restrict the number to a range that prevents over or underflows.","array-validation#Array validation":"The items option should always be present and define what type of values may be contained within the list or tuple.\nThe uniqueItems option should be present for scalar item values and explicitly state whether duplicate values are allowed or not.\nThe minItems and maxItems options should be present. If no maximum or minimum of allowed items can be derived from the use-case, a range of 1-1000 should suffice for most cases. Remember though that some technologies have hard limits (eg. MonoDB with 16MiB) ‚Äì if it's possible for items to be very large, it might make sense to choose smaller limits. (In case of an array being restricted by both enum and uniqueItems it would not hurt to omit min/max restrictions since they'd be redundant).","automatic-defaults#Automatic defaults":"The default option should be specified whenever possible. Especially in cases when it should possible to omit values within the User Input that are still required by the business logic, the default option together with the useDefaults: true setting (during initialization of AJV) will ensure they are present as expected automatically.","caveats#Caveats":"It will often be difficult to come up with a reasonable minimum or maximum value ‚Äì when in doubt, pick a value based on the most extreme use-case you can come up with but is unlikely to cause any problems within your system.Also, be especially careful when restricting validation of values belonging to existing data in your system. It might be that customers are currently using a value that lies outside of your defined minimum or maximum and that might cause them to no longer able to make updates or execute related actions. Consider checking real-world data (how do customers actually use your product?) before deciding on restrictions.As stated at the beginning, this is merely an example set of rules that can be used to build your own best practices upon. Depending on the technologies used in your project and its specifications, it might require many adjustments for you to make use of it. But it could, at least, offer a good basis to start with."}},"/posts/2021/12/21/fuzzing-complex-projects-with-echidna-sushi-bentobox":{"title":"Fuzzing Complex Projects With Echidna: Sushi's BentoBox","data":{"":"December 21, 2021 by patrickd\nDuring Secureum Bootcamp's second phase \"CARE\" (Comprehensive Audit Readiness Evaluation), participants were asked to review their assigned Project for typical Pitfalls and Best Practices to help prepare the project for an actual Audit. Since we were free to attempt going further than that, I wanted to try my hand at fuzzing with Echidna and quickly realized that it wouldn't be easy to do that in this case.The problem is that Echidna expects a rather simple environment: To run it, you need to specify a Solidity source file, a single Contract to fuzz, and the constructor must not require any parameters. The Project we were assigned to, on the other hand, Sushi's BentoBox-Strategies contracts (Solidity 0.8.7), requires the BentoBox contract to be deployed (Solidity 0.6.12), and both have 3rd party dependencies such as WETH9 and Aave.","the-bentobox#The BentoBox":"BentoBox is a Platform you can transfer your Tokens into, to make use of various Applications (currently only Kashi Lending, but the AMM Trident and other things have been announced). Additionally, Sushi's Operations Team is able to decide on Investment Strategies for a percentage of the Tokens that are currently unused within the BentoBox.To make things easier to grasp, we'll concentrate on a simplified use-case: A User deposits Ether into the BentoBox, without using any particular Application for now. BentoBox wraps them into WETH Tokens and at some point, they are rebalanced into the investment Strategy that the Sushi Ops team has set for WETH. This particular Strategy uses Aave to earn interest on the provided liquidity. For investing the WETH Tokens, the Strategy contract gets aWETH Tokens that increase over time and can be exchanged for the same amount of WETH when withdrawn from Aave. Sometime later the Ops Team decides to switch to another, more promising Strategy and the current one is exited, expecting the full amount of WETH (+ earned interest) to be returned into the BentoBox.Using fuzzing we now want to find out whether there's any case where the Strategy might fail to correctly send back all funds when this happens.","initialization-via-remix#Initialization via Remix":"For contracts with complex initialization, the official Tutorial recommends using Etheno and the Project's Testsuite to basically record the transactions made when running a single test and replay them when initializing Echidna. The tests of the AaveStrategy however, make use of a chain-fork from an archive node - that might be a bit much for a simple initialization recording.But why use tests if we can make use of Etheno's recording functionality and take care of the initialization by hand just once?\nIf you are running this on the same host as your Browser, you can simply open your Metamask extension and switch to the Network \"Localhost 8545\". Then look for the \"Import Account\" option and paste in the private key of the Account (0) from the Ganache logs. Within Remix, open the \"Deploy & Run Transactions\" sidebar and select the Environment \"Injected Web3\". All of the contract deployments and interactions you now make within Remix will be written into the init.json file, which can be imported into Echidna.\nIf you are running Etheno within a container or VM, the problem is that it's not possible to adjust the Interface/IP address that port 8545 is listening on. One way to work around this is creating a port forwarding within the guest system with socat TCP4-LISTEN:8544,fork TCP4:127.0.0.1:8545{:bash} and then another one on your host with socat TCP4-LISTEN:8545,fork TCP4:CONTAINER_IP:8544{:bash}. With that, you should be able to connect to it as if you were running Etheno locally.\nSo, what should be recorded? Let's trace the dependencies the AaveStrategy has:\nWhile we could deploy the real Aave contracts since they're open source, that would likely make things a lot more complicated than necessary. Instead, it would be easier to assume Aave works as expected and only mock out the functions that the Strategy is actually interacting with. Aside from that, we also need an instance of BentoBox and a \"Strategy Token\" which for this use-case will be the wrapped ether contract \"WETH9\", which BentoBox has a dependency on anyway.The fact that AaveStrategy's constructor needs some parameters, can be taken care of via the constructor of a \"proxy\" contract that does not, by using constants. That proxy can also take care of initializing the Aave mock contracts. BentoBox and WETH9 on the other hand use incompatible, older Solidity versions and it makes sense to record their deployment.So first we'll copy WETH9's source code into Remix, compile it with 0.4.18 and deploy it. After that, we'll copy a flattened version of BentoBox that has already been conveniently adjusted to allow switching between Strategies without the usual 2 week waiting period, compile it with 0.6.12 and deploy it too. Finally, we want to directly transfer ownership of the BentoBox to where our \"proxy\" contract will be deployed to by Echidna, which we can find out by checking the default configuration, and calling transferOwnership(0x00a329c0648769a73afac7f9381e08fb43dbea72, true, false){:solidity}.\nIf you attempt this multiple times, note that every time Ganache is restarted all transactions made so far are gone. The problem is that Metamask doesn't know about this, unless you go hit \"Reset Accounts\" in its Advanced Options, deleting the history so far and ensuring that the next transaction will work.\nAfter stopping the Etheno command, you'll find all of the above transactions recorded in the init.json file. To avoid Echidna complaining about parsing errors when reading it, adjust the values of the gas_price and value fields from numbers to strings.","initialization-via-proxy-constructor#Initialization via \"Proxy\" constructor":"Having taken care of dependencies with incompatible Solidity versions, the leftover initializations can be done by making use of the \"Proxy Pattern\" (not to be confused with the delegate-call-proxy patterns):\nThis Proxy contract will be our fuzzing target for Echidna. The AaveStrategy is (not quite yet) initialized to invest the WETH token and then activated within BentoBox (which the Proxy is able to do since ownership has been transferred to it when init.json is replayed).If we had chosen to also deploy the real Aave contracts when creating that recording, we could simply specify those addresses here for ILendingPool and IAaveIncentivesController. But instead, we'll write some mock contracts simulating them in the most basic way:\nWith this kind of mocking, the AaveStrategy should be able to exchange between the strategy token (WETH) and the aToken (aWETH) without issues - but also without interest. Later, the mock contracts can be extended with functions that give profits, rewards and even simulate losses. We can make Echidna explore these scenarios by adding fuzzable util functions like aave_giveReward(uint256 amount){:solidity} that will call them.The fuzzing campaign can also be made more powerful by adding the option of using multiple strategy and aTokens instead of just using WETH. But for now let's keep things simple and just get the fuzzer running with some guardrails on, after it turns out that it's unable to find anything this way, we can always allow for more complexity.","fuzz-targets#Fuzz-targets":"As things stand, there's still nothing Echidna can actually fuzz yet, so let's add some functions to the Proxy that should be its targets:\nHere again, complexity was reduced by hardcoding function parameters, which can be exposed to Echidna later if necessary. In general, you'll need to find a balance here between too much exposure, where Echidna will find itself looking for a needle in the haystack, and too many assumptions, where an issue can't be discovered anymore since the guardrails prevent Echidna from doing so.","the-testcase#The Testcase":"Finally, we have to give Echidna a way to check whether something has gone wrong while it poked at the BentoBox:\nFor the fuzzer to pick these functions up as testcases, they must be part of the abi (external/public), return a boolean value, and be prefixed with echidna_. Aim to write as many of these properties as you can come up with early on, since Echidna can check them \"all at once\" it would be a waste to have to rerun it for new testcases later.Now before starting up the fuzzer I recommend doing a dry run by playing through this as the fuzzer would. Switch Remix back to its \"Javascript VM\" and deploy all of the contracts and their dependencies, just like we did when creating the init.json. Call each of the fuzzing target functions and check whether they had the expected effects. Between each call, check the testcase functions and make sure they all return true when you expect them to. Once confident that the environment you've built for Echidna works properly, it's time to start fuzzing.","finally-fuzzing#Finally, Fuzzing!":"To tell Echidna to initialize using the transaction log in init.json, we need a simple configuration file:\nAfter that, we only have to tell Echidna where to find the Proxy contract, its name, the config file, and make sure it knows that the OpenZeppelin contract dependencies can be found within the node_modules folder:\nAnd now it's time to iterate: Tweak the fuzzing exposure, open up for more search space or narrow down to make it focus on a specific part of the code. For example, at the moment all of the function calls are made via the EchidnaProxy contract, which has Owner and Executor access roles, it would make sense to allow the fuzzer to make unauthenticated calls too. Things like that should be considered when adjusting the environment while waiting for the previous campaign to finish.Tweak, fuzz, repeat"}},"/posts/2021/5/15/cracking-a-very-old-monero-wallet":{"title":"Cracking A Very Old Monero Wallet","data":{"":"May 15, 2021 by patrickd\nYou've probably heard stories like these before: Some years ago, someone played around with mining a Crypto Currency when it wasn't worth much, forgot about it and now that its price has gone up, they don't remember the Wallet's password anymore. It's the story we were recently approached with by a client who'd really like to have that money now.Unfortunately he did not only forget the Wallet's password, he also did not write down the recovery seed anywhere, so this doesn't leave a lot of other options aside from trying to open the Wallet by brute force.","is-this-gonna-be-worth-it#Is this gonna be worth it?":"Furthermore he also is not quite sure about the amount of Monero that he might have mined back then: It could be anything between a few hundred to a few thousand Monero, which is, with current market prices anywhere between USD 50,000 and a few millions. But maybe it's less and not even worth the effort?Without having him send us the encrypted Wallet file itself for now, since he's probably worried about us running off with his potential riches, we asked for anything that could be a hint and luckily he had preserved the original wallet software and all of its files, which included a log that has some interesting information:\nThe first hexdecimal string in the log file is most likely the Wallet's Address and if this were a Crypto Currency such as Bitcoin we could very easily find out the amount of Coins it holds. However Monero is a privacy focused Crypto Currency, this means that you cannot simply take the public key (from which the address is derived) of a Wallet and use a Blockchain explorer tool to look up its contents.According to the Moneropedia you require the Private View Key in order to be able to check a Wallet's Transactions and therefore be able to determine how many Moneros a Wallet has. There's also a public view key that doesn't allow you to identify a Wallet's Transactions but rather whether a Block contains any of them or not ‚Äì this is useful to avoid downloading Monero's complete Blockchain just to check your balance while still protecting your privacy.The log file appears to contain a View Key too, but it does not specify whether this the public or private one. Surely you'd not write a private key to a log file?In order to find out, we'll attempt to generate a View Only Wallet which requires the only two things we might have: The Address and the Private View Key. Using Monero's CLI Wallet we follow the user guide's instructions:\nSeems like that worked! Apparently the log file really did contain the Private View Key and we now have a View-Only Wallet that allows checking the possible balance of the locked up Wallet. We sent the generated Wallet back to our client and waited for him to tell us whether we should continue.\nNote that a View-Only Wallet can only show a balance based on incoming transactions. Unless you are certain no outgoing transactions were sent from the Wallet, the displayed balance will likely be incorrect.\nAfter several hours - of him waiting for the Blockchain to synchronize - he asked us to continue and attempt actual recovery of the funds and promised to make it worth our while should we succeed: We'd get 10% \"finder's reward\" under the condition that in any case we'd cover 10% of the costs. Sounds fair!","trustless-password-cracking#Trustless password cracking?":"When you generate a Monero Wallet, it creates two files: One file without any extension (eg. wallet1) and another file with the keys extension (wallet1.keys). Neither files contain any human readable information, likely due to being encrypted.We were wondering whether we had to crack the actual wallet or whether it might be possible the extract some kind of password hash from these files. This would allow us to attempt recovering the password without the client having to trust us with the original wallet files.The Monero Wallet software is fortunately open source and it's quite easy to find out how it checks whether the entered password is correct. We assumed that if there was something like a password hash, surely this check would make use of it.Within wallet2.cpp the bool wallet2::verify_password(const epee::wipeable_string& password){:cpp} method seemed like what were looking for, but it appears that there's likely no such thing as a hash since what it actually does is decrypting the .keys file with the specified password and then checking whether the decrypted contents make any sense. If they don't, the password must have been incorrect.While searching the Internet we actually found various Websites that claimed to be able to extract a Monero Wallet's \"hash\" which could then be used by cracking tools such as John the Ripper or Hashcat. Since this required \"uploading\" your .keys file, we initially thought that this might be some kind of scam attempting to steal peoples wallet files and cracking them before they do.It turns out that these websites are simply using the monero2john.py script that was added as a Tool as part of the pull request that added Monero Wallet cracking support to John the Ripper. And actually reading that python script's code quickly makes you realize that the \"hash\" generated here is actually just the .keys file's content encoded as hexdecimal string with wallet-name:$monero$0* prepended.In short, we'll have to ask our client to trust us and send us the full .keys file. And we also highly discourage uploading your Wallet files to any websites.","is-the-wallet-too-old#Is the Wallet too old?":"Now that we know John the Ripper (\"jumbo version\") supports cracking Monero Wallets it seems quite straight forward: First generate the password \"hash\" using the monero2john script and store its output in a so called password file, then run the john command specifying the password file as a target.Having read through the pull request though, there appears to be a caveat: Legacy Wallets (pre 2016) that did not use JSON as a format yet, are still supported by the official Monero Wallet software but this support has not been implemented in John. Thanks to the logfile we know that the Wallet in question is from 2014, so it's likely that John wont be able to crack it.In order to validate this, we generated 2 new wallets with test as a password: One wallet was created using the newest Wallet software available, the other one was generated using the original Wallet software send to us by our client (since this is a Windows executable we use wine to run it).Creating a legacy wallet:\nNext, we create the password file for John containing \"hashes\" for both the legacy and the JSON format Wallet. We also create a wordlist containing only the one password we know will work: test.\nWe can now have John work on these:\nAs can be seen by the output, it is indeed true that using John as is will not work for the original Wallet file, since it was only able to \"crack\" the JSON format wallet.","patching-john#Patching John":"Looking at the monero_fmt_plug.c that was added with the previously mentioned pull request, we can see that after attempting to decrypt the keys file, it will check whether the first 32 bytes contain the string key_data ‚Äì if it does, the Wallet is considered cracked.\nThe reason behind this can be found out by adding a printf call that logs the 32 decrypted characters to console. For a supported JSON wallet the output will be something like this: {\"key_data\":\"\\u0001\\u0011\\u0001\\{:json}. We can see that the string key_data is actually the first field it expects to be defined in the JSON.Doing the same for our legacy test Wallet, the output is m_creation_timestamp ‚Äì which is neither JSON nor contains the currently expected string. The solution for adding legacy wallet support is therefore surprisingly simple: We check whether the decrypted characters contain key_data OR m_creation_timestamp.With this patched we can now rerun our compatibility test from before and we can see that it now indeed works for all Monero Wallet formats:\nWe created a pull request for adding these changes to the official John the Ripper jumbo repository.","chachacha#ChaChaCha!":"After having looked at John's Monero plugin for a while now, we noticed that, aside from the JSON and non-JSON format Wallets, there's another backwards compatibility measure where we're curious whether it actually has a significant performance impact: For every cracking attempt, it first decrypts the keys file using the standard ChaCha20 algorithm and when that fails, tries it again with ChaCha8 ‚Äì which is a modification that only does 8 rounds of mixing instead of the standard 20.We can very easily determine that Wallets generated by our client's original Wallet software appear to always use the modified ChaCha8. This effectively means that we can remove the first, and likely more expensive, attempt of decryption using ChaCha20.\nAfter actually testing the performance difference of the original and ChaCha8-only John ‚Äì it turned out that the impact on cracking performance doesn't appear to be very significant.","lets-get-crackin#Let's get crackin'":"We now have everything in place to actually start cracking the real wallet. Using wordlists of commonly used \"lazy\" passwords, strings mentioned in and around the wallet software and various hints from the client himself. Together with John's word mangling functionality this should already prove quite powerful. After that we'll likely not have much of a choice but to use brute force ‚Äì up to the point where we think that continuing to keep trying will actually end up costing more than we could potentially unlock here."}},"/posts/2022/07/secureum-bootcamp-epoch-june-race-7":{"title":"Secureum Bootcamp Epoch‚àû - June RACE #7","data":{"":"This is a write-up of the Secureum Bootcamp Race 7 Quiz of Epoch Infinity with solutions.\nThis quiz had a strict time limit of 16 minutes for 8 questions, no pause. Choose all and only correct answers.Syntax highlighting was omitted since the original quiz did not have any either.\nJuly 5, 2022 by patrickd","code#Code":"All 8 questions in this RACE are based on the InSecureumApe contract. This is the same contract you will see for all the 8 questions in this RACE.InSecureumApe is adapted from a well-known contract. The question is below the shown contract.","question-1-of-8#Question 1 of 8":"The mint price of an InSecureumApe is:\n A.  ETH  \n B.  ETH  \n C.  ETH  \n D.  ETH  \nCorrect is D.We can see the price is determined by the apePrice constant in wei, by which the number of tokens to mint are multiplied by.The inline comment claims it to be //0.08 ETH but, knowing that ethereum has 18 decimals, we can check and realize the price actually 0.8 eth.The https://eth-toolbox.com/ website offers a quick way to convert between these denominations.It would've been a lot better if the code made use of denominations, this would've made the code much more readable and likely prevented the issue: 0.08 ether.","question-2-of-8#Question 2 of 8":"The security concern(s) with InSecureumApe access control is/are\n A. Owner can arbitrarily pause public minting of InSecureumApes  \n B. Owner can arbitrarily mint InSecureumApes  \n C. Single-step ownership change  \n D. Missing event emits in and time-delayed effects of owner functions  \nCorrect is A, B, C, D.The saleIsActive state variable is checked within mintApe() can be toggled via the flipSaleState() by the owner at any time, without delay or warning.The reserveApes() function allows the owner to mint arbitrary amounts of tokens at any time even bypassing the MAX_APES maximum supply config set during construction.The transferOwnership() function inherited from OpenZeppelin's Ownable contract only ensures that ownership is not transferred to the zero-address, but it can be transferred in a single step to any other potentially invalid address.None of the functions using the onlyOwner modifier emit events or have any sort of time-delay for their action, due to this users can suffer from unwanted surprises that are difficult to monitor for.","question-3-of-8#Question 3 of 8":"The security concern(s) with InSecureumApe constructor is/are\n A. Missing sanity/threshold check on maxNftSupply  \n B. Missing sanity/threshold check on saleStart  \n C. Potential integer overflow  \n D. None of the above  \nCorrect is A, B, C.None of the mentioned parameters are sanity/threshold checked which would allow accidental deployment with incorrect parameters that could be noticed too late, after money has already gone into the contract.Unlike in Solidity 0.8.x, integer overflows aren't automatically checked for in this version, so an extremely high saleStart value could indeed cause an integer overflow, although unlikely for sane values.This is true despite the fact that the code appears to be using SafeMath for uint256;. Remember that this merely adds the methods of the SafeMath library to the type. For this to actually have any effect the code actually needs to make use of the methods:\nLater in Solidity 0.8.19 User-Defined Operators were added. A library making use of this, would indeed be able to override the addition operator (+) to make it equivalent to calling SafeMath.add().","question-4-of-8#Question 4 of 8":"The total number of InSecureumApes that can ever be minted is\n A. maxApePurchase  \n B. MAX_APES  \n C. MAX_APES + 30  \n D. type(uint256).max  \nCorrect is D.Since the reserveApes() function allows the owner to arbitrarily mint tokens without checking the MAX_APES variable, it's possible to mint as many tokens as the totalSupply variable can hold, which is the maximum value an uint256 can have.","question-5-of-8#Question 5 of 8":"The public minting of InSecureumApes\n A. Must be paid the exact amount in Ether  \n B. May be performed 19 NFTs at a time  \n C. Uses _safeMint to prevent locked/stuck NFTs  \n D. None of the above  \nCorrect is B, C.The amount doesn't need to be paid exactly, more can be sent but shouldn't since any above this amount is kept by the protocol and not sent back.The contract doesn't correctly check how many tokens can be minted at a time, it should be numberOfTokens <= maxApePurchase to allow 20 as described.The contract indeed uses the _safeMint() function that'll ensure that if the receiver is a contract, it must correctly implement the onERC721Received() function, proving that the receiver is capable of handling NFTs and that they won't be stuck after receiving them.","question-6-of-8#Question 6 of 8":"The security concerns with InSecureumApe is/are\n A. Use of a floating pragma and an older compiler version  \n B. Oracle price manipulation  \n C. Reentrancy allowing bypass of maxApePurchase check  \n D. None of the above  \nCorrect is A, C.The best practice is to avoid floating pragmas for contracts to ensure that they're always tested with the same Solidity version throughout the entire development cycle until deployment.The contract does not make use of any oracles.Since _safeMint() is used and calls onERC721Received() on receiving contracts, a NFT receiver can indeed call back into the mintApe() function and bypass how many tokens can be minted within a single transaction. But this check can be bypassed by simply repeatedly calling mintApe() from a custom contract since the function doesn't ensure that only EOAs can call it.","question-7-of-8#Question 7 of 8":"The starting index determination\n A. Is meant to randomize NFT reveal post-mint  \n B. Can be triggered by the owner at any time  \n C. May be triggered only 9 days after sale start  \n D. Accounts for the fact that EVM only stores previous 256 block hashes  \nCorrect is A, B, D.You can read about how this is used for post-mint reveal randomization in this article.The 9-day delay of the REVEAL_TIMESTAMP variable can be overriden at any point in time, it can also be triggered earlier if the totalSupply matches MAX_APES exactly, or be triggered at any time by the owner via emergencySetStartingIndexBlock().It accounts for the block hash access limitation by falling back to using the hash of the previous block instead.","question-8-of-8#Question 8 of 8":"Potential gas optimization(s) in InSecureumApe is/are\n A. Caching of storage variables  \n B. Avoiding initializations of variables to default values of their types  \n C. Use of immutables  \n D. None of the above  \nCorrect is A, B, C.Whenever storage variables are read from multiple times, they should be cached in memory to safe gas. This is missing for MAX_APES in mintApe() and startingIndexBlock in setStartingIndex().All state variables are zero-initialized by default, therefore there's no need to manually set saleIsActive to false, for example.The state variable MAX_APES is only set once during construction and should be immutable to save gas"}},"/posts/2021/8/23/cryptohack-ctf-review-key-takeaways":{"title":"CryptoHack CTF: Key Takeaways","data":{"":"August 23, 2021 by patrickd\nCryptoHack is a collection of Capture-The-Flag-like Challenges that intend to teach you modern cryptography, the math behind it and how to exploit it when implemented incorrectly. Since the Authors of the Platform ask participants not to share any instructions on how to solve the Challenges, this won't be a write-up but rather a spoiler-free list of realizations that one might have while solving the challenges.","use-standards-use-libraries-and-use-them-correctly#Use standards, use libraries and use them correctly":"If you've been doing any programming you should have heard by now that \"reinventing the wheel\" is usually considered a bad practice. One should always look for established and widely used libraries instead of re-implementing complex logic or algorithms from scratch.Especially learning about the mathematical intricacies in cryptography will bring that point home. Small errors or misunderstandings can introduce fatal flaws into your application's security. You'll be tempted to introduce optimizations, simplifications and shortcuts which will likely end up doing much more harm than good. Sometimes things that intuitively feel like genuine improvements, such as using even larger numbers than recommended, may actually have the opposite effect.CryptoHack will introduce you to many of these issues and their outcome, and it assured me of something I was already suspecting: I should definitely not try to use my own cryptographic inventions in production. Playing around with all the newly learned things and attempting to give them my own twist was certainly a fun experience, but one has to remember that standards like RSA have been published as scientific papers and have been analyzed, criticized and attacked by other researchers for years. Those standards have stood the test of time and had a lot of brainpower looking at how secure they really are - it's not very likely I'm smart enough to pull all that off alone.But even if you use libraries implementing established standards, there's still a big chance that it'll all be for naught because you are simply not using them in a proper manner. Since they are likely to provide flexibility for implementing a wide variety of different use cases, that flexibility will give you lots of opportunities to mess things up again. So even when utilizing trusted and widely used libraries, thinking you can blindly rely on them to do everything correctly out of the box, with default options or by simply copying and pasting the example code snippets, might end up shooting you in the foot.Therefore, you shouldn't dismiss the usefulness of understanding the principles behind the ciphers that you're planning to make use of even if you do not intend to implement them yourself.","what-you-lack-in-knowledge-you-can-make-up-with-osint-experience#What you lack in knowledge you can make up with OSINT experience":"While most challenges on CryptoHack start nice and easy, the difficulty ramps up pretty quickly and you'll soon find yourself blindly poking around at your code until it finally produces the solution - and that might even work, for a while. Then you'll quickly run into walls where the provided resources won't be enough to understand and solve the challenge. And this is where you can make use of and practice your information gathering skills.Remember that the challenges aren't entirely original. They are often based on bugs, issues and attacks that have been reported and probably explained somewhere on the Internet. Don't be intimidated if your search leads you to PDFs of scientific papers with complex mathematical formulas, skim over them to look what places could contain the information that you need and take them apart piece by piece.If you don't know where to start looking, note that these challenges are in the typical style of CTFs: Check for hints in the title or description of the challenge. If a word or a whole sentence doesn't fit into the context of cryptography, at least as far as you know now, you should probably search the Internet about it and find out why it was mentioned at all, since it's likely a clue.And lastly, if you're getting really frustrated there's always the chance that a similar challenge has been part of another CTF before. You can always look for writeups of those and even if you don't find the actual solution you'll still likely find helpful resources and tools. A good example is the RsaCTFtool, which is able to perform a variety of attacks on RSA that are typical for CTF challenges. Look at all of the attacks it supports, maybe there's one that matches what you're working on.","python-is-excellent-for-experimenting-with-cryptography#Python is excellent for experimenting with cryptography":"Python is well known for being the most popular language in the InfoSec scene, and that probably for good reasons. To me, it offers all the simplicity of being a high level scripting language while at the same time allowing you to work with low-level structures and interfaces.More than that, it really shines in cryptography. While most other languages I know of struggle with big numbers, Python not only supports them natively but has so many crypto-libraries making it easy to use while working on the challenges. It lets you easily generate primes, make use of modular arithmetic while using large exponents, and convert values between most of the encodings that you'll ever encounter in the real world.\nNice and short RSA implementation right? Did you spot a problem?","cryptography-is-about-more-than-encryption#Cryptography is about more than encryption":"You have probably already heard about \"digital-signatures\" which provide proof that an encrypted message you received is authentic. But have you heard about how cryptography allows people to exchange secrets on a public medium without having ever met before? You are using standards like PEM, DER and x509 every day, but what do they actually do? Do you know how to calculate with probabilities to find out how random the numbers that your computer comes up with really are?From the historically grown mess that many of the standards are that we're using today, to the mind blowing and elegant mathematics beneath all of it. The CryptoHack challenges are a great exercise for anyone looking to brush up on their cryptography knowledge and, while sometimes a bit frustrating, I had a lot of fun trying to solve them while struggling with the complex mathematical concepts that I was completely ignorant of. Many thanks to the authors that provide us this incredible website free of charge.You can take a look at how far I managed to get at https://cryptohack.org/user/patrickd/"}},"/posts/2022/10/3/secureum-bootcamp-epoch-october-race-10":{"title":"Secureum Bootcamp Epoch‚àû - October RACE #10","data":{"":"This is a write-up of the Secureum Bootcamp Race 10 Quiz of Epoch Infinity with solutions.\nThis quiz had a strict time limit of 16 minutes for 8 questions, no pause. Choose all and only correct answers.Syntax highlighting was omitted since the original quiz did not have any either.\nOctober 3, 2022 by patrickd","code#Code":"All 8 questions in this RACE are based on the following contracts. You will see them for all the 8 questions in this RACE. The questions are below the shown contracts.","question-1-of-8#Question 1 of 8":"Which statements are true in Test1()?\n A. The function does not use all supplied extra data \n B. The function can revert due to an underflow \n C. The function can revert due to an overflow \n D. The function accesses memory which it should not \nCorrect is A, B, C.Answer A seems a bit confusing when looking at Test1() alone, but seeing the xtr variable of Test2() brings some clarity: The Test1() function signature expects one uint to be passed, but then within the function body it loads 64 bytes directly from calldata. Test2() then shows how the function is intended to be called by concatenating extra data to the ABI encoded calldata. It adds two more uint types which together are 64 bytes of extra data. But then in the abi.decode only the first uint from extra data is actually decoded and used.Both B and C are true since a Solidity version (^0.8.0) is used, that automatically checks for integer over/underflows and reverts when these happen. In this specific case, an overflow could happen when parameter n or the number supplied from extra data are large enough to wrap. The underflow can happen when the overall supplied calldata is smaller than 64 bytes, making the subtraction within the slicing parameters fail.You could argue that accessing msg.data directly should be avoided when possible. But this doesn't access memory but read-only calldata. Therefore no memory is accessed that should not be.","question-2-of-8#Question 2 of 8":"Which statements are true in Test2()?\n A. Result of encodePacked is deterministic \n B. abi.decode always succeeds \n C. It calls function Test1() without any problem \n D. It should use uint256 instead of uint \nCorrect is A, C.Deterministic means that you should always get the same predictable output for a given input. As such, encodePacked always encodes passed data the same way.Test2's abi.decode will only succeed if no error happens in Test1(). If Test1() reverts the returned data would not contain a decodable uint but error data. One way to cause this to happen would be supplying a number n that causes an overflow. The best practice is to check the success boolean before attempting to decode the returned data.Answer D leaves some room for interpretation. uint is an alias of uint256 and there should not be an issue using it here. But it's a common best practice to avoid the shorter alias and instead use the longer-named version of the type. While this is generally considered to improve readability, I'd argue that consistency (always using the same type) is more important.","question-3-of-8#Question 3 of 8":"Which statements are true in NextBookNonce()?\n A. wrap and unwrap cost additional gas \n B. It is safe to use unchecked \n C. There is something suspicious about the increment value \n D. It could have used x.nonce++ \nCorrect is B, C.The calls to wrap and unwrap are basically telling Solidity whether it should treat a certain variable as being of a custom type (Nonce) or of its native type (uint256). This switch is basically just syntactic sugar for handling types within Solidity, the EVM will know nothing of these type switches and no additional gas will be used by doing so.Using an unchecked block in this function would omit Solidity's over/underflow handling. Especially in the context of a Nonce (Number used only once), you don't want integer values to wrap and overflow to values that were once used before. But usually, a nonce is only increased by such a small value that exploiting this would be very expensive. In this specific case, the function is pure and the nonce is not stored, so whether it's safe to use unchecked block will depend on the function being used correctly.Answer C sounds rather ominous but it's simply pointing out that Nonces are commonly increased by one and not by such a weird number as 3.Arithmetic operations cannot be executed on custom types without unwrapping the number first.","question-4-of-8#Question 4 of 8":"Which statements are true in Test3()?\n A. bookIndex.nonce is incremented in the loop \n B. bookIndex.nonce cannot be incremented because NextBookNonce is pure \n C. i++ can be made unchecked \n D. memory can be changed to storage without any other changes \nCorrect is A, C.Both A and B should be clear from reading the code.The increment of i within the loop can indeed be made unchecked since it won't be able to overflow no matter what is supplied as n.The memory location can't simply be changed to storage without various further changes such as assigning it to a specific storage slot before being able to make use of it.","question-5-of-8#Question 5 of 8":"Which statements are true In Test4()?\n A. The function always reverts with ZeroAddress() \n B. The function always reverts with ZeroAmount() \n C. The function never reverts with ZeroAddress() \n D. The function never reverts with ZeroAmount() \nCorrect is C, D.The first array elements of both a and amounts will always be zero-like. Both 1 for ZeroAddress and 2 for ZeroAmount will be OR-combined resulting in 3. Once this value is set as an error, further iterations will not influence it. After the loop has finished, this error value is not checked for and instead, the function returns the total without reverting.","question-6-of-8#Question 6 of 8":"Which statements are true in Test5()?\n A. modifier checkInvariants will pause the contract if too much is minted \n B. modifier checkInvariants will never pause the contract \n C. modifier checkInvariants will always pause the contract \n D. There are more efficient ways to handle the require \nCorrect is B, D.While the checkInvariants modifier does intend to pause the contract if too much is minted, it'll be unable to ever do so since this will be reverted by the second require call.A single call to require would fix this issue and also be more efficient.","question-7-of-8#Question 7 of 8":"Which statements are true about the owner?\n A. The owner is initialized \n B. The owner is not initialized \n C. The owner cannot be changed \n D. The owner can be changed \nCorrect is A, D.Although not visible here, the owner is indeed initialized by the constructor inherited from Ownable, which also comes with functions allowing to change the owner at a later point.","question-8-of-8#Question 8 of 8":"Which statements are true in Test5() and related functions?\n A. pause is unsafe \n B. unpause is unsafe \n C. The emit is done right \n D. The emit is done wrong \nCorrect is A, D.The pause function is missing the onlyOwner modifier allowing anyone to arbitrarily pause the contract.The minted event's parameters appear to be in the wrong order."}},"/posts/2022/10/31/race-11-of-the-secureum-bootcamp-epoch":{"title":"RACE #11 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-11, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors.\nThis month‚Äôs Quiz was designed by the Secureum Mentor Emiliano Bonassi and I‚Äôd say it was pretty fair and doable to solve all 8 questions within the strict timelimit of 16 minutes.As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nNovember 1, 2022 by patrickd","code#Code":"All 8 questions in this RACE are based on the following contract. You will see them for all the 8 questions in this RACE. The questions are below the shown contract.","question-1-of-8#Question 1 of 8":"Which statements are true in withdraw()?\n A. Can be successfully executed when contract is paused \n B. User can withdraw only after _minDepositLockTime elapsed since last withdrawal \n C. Follows checks-effects-interaction pattern best practice \n D. User can withdraw more than deposited \nCorrect is D.The require statement right at the start of the function ensures that any attempt to call it will revert when the contract is paused.The time is not measured since the last withdrawal but since the last deposit.Does not follow the CEI pattern since calling safeTransferFrom() on the _token is an interaction with an external contract, and effects like the balance update happen after it.When a user attempts to withdraw an amount larger than their current balance, it'll simply be set to 0 and the requested amount would be send without any issue as long as the user does not attempt to send more tokens than the contract owns.","question-2-of-8#Question 2 of 8":"Which mitigations are applicable to withdraw()?\n A. Transferred amount should be minimum of amount and _userBalances[msg.sender] \n B. Move if/else block before safeTransferFrom \n C. Require amount to be <= user‚Äôs balance deposited earlier \n D. Remove if/else block and add _userBalances[msg.sender] -= amount before safeTransferFrom \nCorrect is A, C, D.Checking which one of amount or the actual current balance is smaller, then using that as the amount of tokens to transfer to the user, does indeed seem like an easy way to mitigate the bug allowing to withdraw arbitrary amounts.Moving the if/else block before safeTransferFrom would bring the function closer to following the CEI pattern. Although, it likely wouldn't mitigate any exploitable issue, since the _token is set by the deployer and, assuming it follows typical ERC20 implementations, it wouldn't allow for reentrancy by the token receiver. It still wouldn't follow the CEI pattern completely though, since Events too are considered effects.Using a require to ensure the amount isn't larger than the users actual balance is a more typical way to handle these situations. At least more typical than sending the minimum of amount and the users actual balance instead. With this change, the else block can also be removed since it'll become unreachable.The last mitigation suggestion makes use of the fact that Solidity ^0.8.0 will automatically check whether there'd be an integer underflow when subtracting the amount from the user's balance. This is likely the most gas efficient solution, although it won't offer a good error message for the user when it happens.","question-3-of-8#Question 3 of 8":"The security concern(s) in pause() is/are:\n A. Does not emit an event \n B. Access control is not strict enough \n C. Will always revert \n D. None of the above \nCorrect is A.The general best practice is, that all state changing functions should emit an event. This is especially true for functions that one wants to monitor off-chain, the pause/unpause functions being a perfect example for that.The access control is actually very strict. So strict in fact that the pause() function will always revert unless both _operator and _governance are the same address. The inline comment makes it clear that this behavior is unintentional and a bug.But the constructor isn't preventing both from being the same address and even then, anyone can call changeGovernance() and make it the same. So claiming it would always revert isn't correct either.","question-4-of-8#Question 4 of 8":"Which statement(s) is/are true for unpause()?\n A. Will unpause deposits and withdrawals \n B. Will unpause withdrawals only \n C. Anyone can successfully call the function \n D. None of the above \nCorrect is B, C.Although the withdraw() function does, the deposit() function does not ensure that nobody can use it while the contract is paused. The best practice would be, if possible, to have it the other way around. When a contract needs to be paused due to an emergency, such as a discovered bug, it should become impossible for users to deposit new funds into the vulnerable contract while still allowing users to withdraw their funds.Even though unpause() function appears to correctly require the caller to be the _governance address, anyone can call changeGovernance() to set it to themselves.","question-5-of-8#Question 5 of 8":"Which statement(s) is/are true in depositFor()?\n A. Can be executed when contract is paused \n B. Allows a user to deposit for another user \n C. Allows a user to fund the deposit for another user \n D. None of the above \nCorrect is A, B.Although the withdraw() function does, the deposit() function does not ensure that nobody can use it while the contract is paused. The best practice would be, if possible, to have it the other way around. When a contract needs to be paused due to an emergency, such as a discovered bug, it should become impossible for users to deposit new funds into the vulnerable contract while still allowing users to withdraw their funds.In order to make a deposit for another user, that user needs to have approved the contract to make use of their tokens. It's not possible for one user to use their funds for a deposit for another user.","question-6-of-8#Question 6 of 8":"The issue(s) in depositFor() is/are:\n A. Cannot be paused for emergency \n B. Exploitable re-entrancy attack \n C. User withdrawals can be delayed indefinitely via DoS attack \n D. None of the above \nCorrect is A, C.Although the withdraw() function does, the deposit() function does not ensure that nobody can use it while the contract is paused. The best practice would be, if possible, to have it the other way around. When a contract needs to be paused due to an emergency, such as a discovered bug, it should become impossible for users to deposit new funds into the vulnerable contract while still allowing users to withdraw their funds.The only external call made is one to the _token. The token is chosen by the operator and, assuming that it can be trusted and doesn't behave in an unexpected way, there should be no other external call that give the caller an opportunity to re-enter.There's indeed an opportunity to Deny another user the Service to withdraw their funds. That is because anyone can call the function with an amount of 0 and the victim's address as depositor. In that case, no matter whether the victim has an open allowance with the contract, an attacker can keep increasing _userLastDeposit indefinitely to delay when the withdrawal is possible. The attacker would have to regularly keep calling the function and pay for the gas that uses though.","question-7-of-8#Question 7 of 8":"Which of the following statement(s) is/are true?\n A. Withdraw event is emitted with incorrect amount \n B. Withdraw event is emitted with correct user \n C. Deposit event is always emitted incorrectly \n D. Deposit event is emitted with incorrect user \nCorrect is B, D.The event emitted during withdrawal appears to be used correctly.It seems more correct to log the user that the deposit is being made for instead of the calling address.","question-8-of-8#Question 8 of 8":"Potential gas optimization(s) is/are:\n A. Use immutable for all variables assigned in constructor \n B. Use immutable for _token, _operator and _minDepositLockTime \n C. Use unchecked \n D. None of the above \nCorrect is B, C.Most internal variables assigned in the constructor are currently using expensive storage space. It would cost much less gas to use immutable variables which are placed into the bytecode during the deployment of the contract.There is however the _governance() variable which is intended to be changeable through the changeGovernance() function. This one should stay a storage variable, although one could argue it should become public to make its current state more easily readable.There are a few places where unchecked blocks can be used without much risk to skip integer overflow checks and save gas. These are places that are unlikely to overflow due to their nature such as adding an amount of tokens to a balance or adding seconds to a timestamp."}},"/posts/2022/12/15/ethereum-smart-contract-auditors-2022-rewind":{"title":"Ethereum Smart Contract Auditor's 2022 Rewind","data":{"":"December 15, 2022 by patrickd\nThis article is the result of reviewing the technical details from many of this year's Smart Contract Vulnerabilities and Exploits in and around the Ethereum ecosystem.","the-novelties#The Novelties":"","phantom-functions#Phantom Functions":"In January, Dedaub discovered a Bug in the Multichain Project that might be a novel attack vector to look out for.What might come closest to this issue is the surprise many developers have when call()ing an arbitrary function on an address with no code deployed. Intuitively, most would expect it to fail, but it does not. One way to explain this is that for the EVM, all bytecode implicitly ends with the STOP opcode, even if it is not present. That is true as well for accounts without any code. And STOP tells the EVM to return without any errors.On the other hand, this behavior usually changes for deployed Solidity contracts. If you call a function the contract did not implement, the EVM will be told to REVERT. The exception to this rule is contracts that have a fallback function implemented that deals with any calls that do not have any or a matching function signature.And this is basically where the crux of this attack vector lies: Developers expect that (1) callees will revert if the function they are calling does not exist and that (2) if it does exist, the function will revert when something goes wrong during the call. But what if the function does not exist, but the callee has a fallback function that will accept any input and never revert?\nIn the specific case of Multichain, the devs expected that the transferFrom() call would only be reached when the caller passed a valid signature since in any other case permit() would revert. However, some tokens, like WETH, don't implement permit() but implement a fallback function that never reverts. In that case, one could have passed an invalid signature and made arbitrary transfers with the WETH that a user had already approved for the calling contract.\nSpeaking of January Phantoms, Qubit Finance's bridge was exploited due to incorrectly having whitelisted the zero-address as a valid WETH implementation. Additionally, when depositing funds into the bridge, they did not check for contract-existance (account.code.length > 0) which lead to the success-response mentioned above when transferFrom() was executed. This allowed the attacker to mint arbitrary amounts of xETH on the other side of the bridge and then drain the funds on mainnet by \"transferring them back\".","double-entry-point-tokens#Double-Entry Point Tokens":"In March, ChainSecurity discovered an issue with the TrueUSD stablecoin while auditing Compound.Compound has a sweepToken() function, a common strategy to rescue funds that were accidentally sent to a contract managing a specific underlying token. While anyone can call this function, they will only ever be sent to an address belonging to the protocol's admins. Furthermore, to prevent this from being used as a possible rug-pull vector, only non-underlying tokens (tokens that the contract isn't supposed to work with) are allowed to be withdrawn.Problems with this arise when there are multiple contracts for the same token, as was the case with TUSD: There's a separate Legacy Contract that can be used just like TUSD, but all it does is forward all actions to the actual TUSD contract. So whenever an address has a TrueUSD balance, it also has an equal balance in the Legacy Contract, and either of them could be called to transfer this balance.Typically, this would only break the \"anti-rug\" protection, but as long as the admins can be trusted, this wouldn't allow anyone to run away with the funds. But in Compounds' case, this sudden drop in the contract's TUSD balance would have affected the token/cToken exchange rate and could have been exploited like a price oracle manipulation.\nShortly after, OpenZeppelin determined that issues like these weren't unique to Compound but had broader implications for DeFi. In the end, they worked together with TrueUSD's Team to fix the issue at its root by blocking the Legacy Contract and basically disabling its usage.\nIn May, Balancer was notified of a similar issue with Synthetix tokens, which also offered a double entry point. Balancer's vaults can be DoS-attacked by such tokens through their flashloan feature: The attacker would borrow all of the vault's tokens from one entry point but zero from the other. When the loan is repaid, it would then mistakenly think that all of the tokens from the second entry point were sent back as a fee and would forward them to a governance-controlled ProtocolFeesCollector contract, basically removing all of the vault's tokens. Assuming that the governance can be trusted this would not allow anyone to steal them, but it would temporarily cause the vault to stop working.","fancy-native-tokens#Fancy Native Tokens":"In March, Gnosis Chain's native token XDAI turned out to have a callAfterTransfer() hook, which some projects seemingly did not anticipate. Agave (an AAVE clone) and Hundred Finance (a Compound Clone) were exploited via a reentrancy attack introduced by the native token's feature, allowing contracts to react to receiving tokens similarly to ERC777.The original projects, that the exploited protocols are based on, are well established and clones like these happen all the time. What the copy-cats forgot to replicate, though, were the strict guidelines that were in place to prevent listing tokens allowing for reentrancy, precisely for the reason that this would make the protocols vulnerable.Even so, the fact that Gnosis Chain added such behavior to their official bridged token seems like a bad design decision that'll likely cause more confusion and exploits like these in the future.\nJust a couple weeks later, still in March, Fuse Network surprised compound clone Ola Finance. Fuse, similar to Gnosis, decided to implement the ERC-677 standard for their bridged token, which calls onTokenTransfer() on receiving addresses with contracts.","nft-flashloan-attacks#NFT Flashloan Attacks":"In March, BAYC intended to AirDrop the ERC20 APE Tokens to owners of their NFTs. Owner could call the claimTokens() function and get APEs based on the amount of BAYC/MAYC NFTs they currently hold.Projects like NFTX have attempted to bring DeFi mechanics into the NFT space, and it appears that the BAYC devs were not paying enough attention to this development. A form of fungibility was introduced with fractionalization: Minting fungible tokens based on a non-fungible one by locking it up.NFTX also offered the possibility to flashloan these fungible \"vTokens\", which effectively allows flashloaning the actual NFTs they represent. Being able to flashloan BAYC's NFTs meant that the first person to do so would be able to claim the airdrop for them. And that is what happened, although the community isn't sure whether this was an exploit or fair game.","read-only-reentrancy#Read-Only Reentrancy":"In April, Chainsecurity found a new issue that various protocols integrating with Curve were vulnerable to.These projects could remove liquidity from Curve's ETH/stETH-pool using the remove_liquidity() function, which had a reentrancy guard preventing any other calls to maliciously re-enter the pool contract while the state had not finished updating yet. The function would burn all of the liquidity tokens being redeemed first, and only then would it start iterating over each underlying token and send them out one by one. Using a reentrancy guard makes sense since the first underlying asset being sent out is raw ETH which will trigger the fallback() function on a receiving contract. A malicious receiver won't be able to re-enter any of Curve's state-changing functions to exploit that the pool is imbalanced from the fact that the underlying stETH has not been sent out yet.However, Curve's view functions had no such protection, and other protocols that relied on get_virtual_price() would have received a manipulated LP token price. A new best practice might establish itself where external protocols will be allowed to easily check the mutex state of another contract that uses reentrancy-lock patterns. Then external protocols will be able to ensure that fetching information from a view function will not be reentrancy and the returned value won't be based on an incomplete state.\nNot long after post-mortems of this attack vector were published, multiple projects didn't get the memo and were exploited in a price manipulation attack using the exact read-only reentrancy described above.","cross-protocol-reentrancy#Cross-Protocol Reentrancy":"In July, Sherlock's EulerStrategy was vulnerable to a sophisticated cross-protocol attack vector involving Sherlock, Euler, and 1inch.The calculation of a staked Sherlock position's value relied on the atomicity of the deposit action into Euler: When a user wanted to swap their USDC to Euler's eUSDC token, Sherlock expected this to happen in a single atomic step.This assumption did not hold when the swap was done using 1inch: While the underlying USDC balance had already increased, the attacker's contract would be called back before the total supply of eUSDC is updated. During this time, the exchange rate would be reported incorrectly, and an attacker could exploit this by redeeming their staked Sherlock position at an inflated rate.","the-usual#The Usual":"","missing-input-validation#Missing Input Validation":"In March, the NFT Marketplace Treasure DAO's buyItem() allowed attackers to purchase NFTs without payment by specifying a quantity of 0. The total was calculated by multiplying the per-item price with the quantity, then a transferFrom() with the resulting amount of zero did not revert, and from this, the protocol assumed that payment must have been successful.\nA couple of weeks later, Paraluni's depositByAddLiquidity() did not check whether the supplied token-pair addresses matched with the specified pair-id. An attacker exploited this by specifying a pair of malicious tokens which the protocol will call transferFrom() on to deposit into a real pair based on the ID. During this call, the attacker re-entered the protocol via the deposit() function, causing the deposited LP tokens to be accounted for the depositByAddLiquidity() as well, doubling the attacker's overall LP token balance.\nIn May, an MEV bot's Uniswap callback function uniswapV2Call() used vulnerable example code to use flash swaps for arbitrage trades. The issue with the code snippet Uniswap provides is that the initiator of the flash swap is not checked, basically allowing anyone to trigger the callback and have it execute a swap. The attacker exploited this by using flashloans to create a large spread in the pool that the MEV bot used for its arbitrage functionality which it happily traded suffering a large slippage.\nIn October, EarningFarm's EFLeverVault contract was similarly exploited when their flashloan callback did not validate the initiator. The Vault withdraw() function made use of flashloans because it had to repay some debt on Aave to withdraw stETH which it automatically converted to native ETH before transferring it to the user. The attacker could drain the Vault by first triggering the flashloan callback without calling withdraw(), causing a large sum of ETH to lay waiting on the contract. Then the attacker withdrew a small legitimate amount they deposited before, but withdraw() always sent the entire current contract balance.\nEnd of October, Team Finance's LiquidityLock contract allowed projects to migrate locked LP positions from Uniswap v2 to v3. Unfortunately, the migrate() function did not correctly validate whether the specified liquidity belonged to the caller. It merely checked whether they had any token locked at all, even fake tokens were accepted. Having bypassed these checks, the attacker could specify that only 1% of the liquidity should be migrated, and the rest was refunded to the attacker.\nIn November, Oasis platform allowed users to delegate-call whitelisted services within the context of the OperationExecutor contract and not only in the context of the user's DSProxy smart wallet as intended. One of these whitelisted services was Aave's InitializableUpgradeabilityProxy which has an initialize() function that will delegate-call to arbitrary addresses as long as the initialized flag has not been set. Checking this flag did not evaluate to true within the context of the OperationExecutor contract, which would have allowed for its self-destruction.","missing-access-control#Missing Access Control":"In January, BURG token's public burn() method together with flashloans was exploited on BSC. The ability to arbitrarily decrease the number of tokens from pools allowed attackers to manipulate the prices of AMMs and drain tokens paired with BURG.\nIn April, Hospo token had a public burn() method that was similarly exploited, draining the UniswapV2 Hospo/ETH pool.\nSometime later in April, Rikkei Finance was exploited via its public setOracleData() function allowing the attacker to set their own malicious price oracle. Being able to manipulate prices freely, they drained the protocol through borrowing.\nLastly, for April, Aave had a \"fallback oracle\" in place, which would have taken over price determination when chainlink failed. This fallback had a public setAssetPrice() function where any price could have been set. The fact that Aave used chainlink's legacy latestAnswer() function instead of latestRoundData() increased the likelihood of this being exploitable, but it was still unlikely overall.\nThen in June, Gym Network upgraded their protocol by adding a depositFromOtherContract() function, which was intended to allow their \"bank\" contract to update the protocol's balances when a deposit happened there. An attacker exploited the fact that the caller was not checked, used this  to change their balance arbitrarily, and finally withdrew tokens that they never deposited in the first place.\nIn August, Reaper Farm's withdraw() function allowed to withdraw anyone's funds from the protocol. There was no check whether the msg.sender matches the specified owner or whether they have the owner's permission to handle their funds.\nShortly after, in August, Energyfi had a similar issue in their bridge: The teleport() function allowed to bridge anyone's funds to the other side without checking whether the msg.sender owns those funds or has any permission to make use of them. Fortunately, the impact was small since they, unlike most other protocols, did not make use of unlimited approvals.\nEnd of August, DDC token had a public handleDeductFee() function that, similarly to a public burn() function, could be used to remove arbitrary amounts of tokens from a specified address. By deducting most of the AMM pool's tokens, the attacker manipulated the price and sold his few DDC for exorbitant returns.","incorrect-signature-scheme#Incorrect Signature Scheme":"In February, OpenSea's signatures were vulnerable to something similar to \"Hash Collision\" attacks, where variable length values lead to different concatenated values resulting in the same signed hash. Additionally, OpenSea used \"replacement patterns\", which can be understood as bit-masks that specify which parts of the signed order a user is allowed to change for fulfillment. An attacker could have added and removed bytes from the replacement pattern without invalidating the signature, allowing them to modify fields of the signed order that were supposed to be unchangeable.\nThen, in May, Fortress's submit() function was supposed to accept price updates only from certain verified accounts with enough \"power\"-tokens to do so. But according to a code comment, this feature wasn't turned on yet as they were waiting to have proper \"DPoS\". What the function did instead was simply count the amount of provided signatures and ensure that those signers were unique. At no point it actually checked who the signer in question was, so the attacker could easily satisfy these checks and submit their own price data. Additionally, the attacker created and voted for a malicious governance proposal that was unnoticed for three days until he could execute it.\nIn June, ApolloX had two contracts with similar claim() functions that allowed callers to claim APX tokens if they could provide a valid message signed by an ApolloX administrator key. Individually these functions were not vulnerable, but the problem was that they were so similar that a signed message for one function could be replayed on the other. So the attacker simply had to extract all of the signatures one function had already been called with and call the other function with them.","composabilitycomplexity-issues#Composability/Complexity issues":"June, Equalizer Finance's staking vaults were drained due to an error when minting liquidity tokens. The number of liquidity tokens, created depended on the current deposited amount of underlying tokens which by itself was not an issue. Still, the FlashLoanProvider contract made use of the vault's funds by lending them out during flashloans. The attacker exploited this by first borrowing most of the vault's funds and then gaining disproportionate amounts of LP tokens by depositing into the vault. After paying back the flashloan, the ill-gained LP tokens could now be used to drain the vault of its funds.","reentrancy#Reentrancy":"In February, HypeBears NFT's mint() function had a check that was supposed to ensure a whitelisted address could only mint NFTs once by marking it as used when the function completed. Before this marking was done, though, it used _safeMint(), which calls onERC721Received() callback functions when the receiver is a contract. This allowed for reentering the mint() function again before the caller was marked as used, bypassing the check entirely.\nLater in March, Bacon Protocol was exploited via its bHOME tokens, which implemented ERC-777 allowing reentrancy via the tokensReceived() hook. Their lending() function first issued the tokens and only later updated the amount being tracked for price determination. Fash loans were used to obtain a disproportionate number of bHOME tokens.\nEnd of March, Revest's ERC-1155 token FNFT was exploited when an attacker was able to re-enter the token contract through the onERC1155Received() hook during minting. The issue was that the variable tracking the amount of minted FNFTs, which was also responsible for determining the next FNFT-ID, was only updated once the mint was finished (after hook execution). This led to the same ID being reused when the attacker re-entered through another function which updated the existing entry making it much more valuable than it should be.\nIn May, Bistroo's bridged token implemented ERC-777. This was exploited on their staking contract's emergencyWithdraw(), function which had a textbook style issue: Users could emergency-withdraw all of their staked tokens at once, but it would only update the user's balance after the transfer finished. This left the attacker the opportunity to call it repeatedly via the tokensReceived() hook, draining the contract by re-using the same user balance.\nIn July, Omni used ERC-721 NFTs as collateral for borrowing but was vulnerable to reentrancy through onERC721Received() during liquidation: The collateral NFT is exchanged for the payment of the debt, and once this transfer has finished, the account is marked as debt-free and healthy. The attacker exploited this by re-entering while receiving the liquidated NFT by depositing many more NFTs and taking a large debt with them, which was immediately \"forgiven\" with the liquidation function's logic finishing. The attacker could withdraw those NFTs again since the system thought they were currently not being used as collateral.\nIn November, DFX Finance suffered another textbook reentrancy exploited when the attacker could deposit() the flashloan back into the pool. The balance checks passed, and the protocol assumed the floashloan was paid back, but now the entire sum was double-accounted for, and the attacker could simply withdraw() the previously lent funds.","arbitraryunchecked-external-calls#Arbitrary/Unchecked External Calls":"In March, LI.FI's swapAndStartBridgeTokensViaCBridge() function, which, as the name implies, was a convenience function to swap tokens before bridging them, allowed making arbitrary external calls. This could be quite easily exploited by passing an array of SwapData structs (containing target address and calldata to call the address with) which were then iterated over and executed one by one. As a result, an attacker could call transferFrom() on any token for any LIFI user who had given the protocol an allowance.\nEnd of March, Auctus had an external write() function that allowed storing arbitrary addresses via its setExchange() modifier. Then it made an internal call to a _sellACOTokens() function which could be used to make arbitrary calls to this address. Once again, this was exploited by stealing approved user balances.\nIn April, StarStream's Treasury was drained via a public execute() function that allowed making arbitrary external calls. The function belonged to the DistributorTreasury contract, which was registered as the treasury's owner. Via this, the attacker called the treasury's onlyOwner-protected withdrawTokens() function to steal its STAR tokens.\nIn September, NFTX deployed a new contract to source liquidity from 0xProtocol for their users. An internal function _fillQuote(), which was callable via various other public functions, allowed making arbitrary calls to any user-specified address with any user-specified calldata. These inputs were intended for 0x's API response, and it apparently wasn't considered that a malicious user could simply manually call these functions with arbitrary inputs draining both the contract's funds and funds of any user who gave approval to it.\nAlso in September, an MEV bot was exploited yet again through a flashloan callback function, likely trusting that callers being restricted to the trusted protocol dYdX would be sufficient protection. The attacker exploited the fact that anyone could initiate a flashloan to this contract and exploited an arbitrary call made within the callback giving themselves approval for the bot's funds.","oracle-manipulations#Oracle Manipulations":"In March, Deus DAO was exploited when an attacker flashloaned DEI, decreasing the Solidly pool's holdings and, therefore, the current LP token price reported by its oracle. Thanks to this manipulation, the attacker could liquidate users who were borrowing on Deus DAO's protocol with these LP tokens as collateral.\nJust one month later in April, Deus DAO was again exploited similarly. This time though, Deus had added Muon as an additional oracle which monitors transactions of the same Solidly pool to calculate a Volume Weighted Average Price (VWAP). It was implemented as an off-chain oracle getting its pricing data from on-chain events, but it misread a large flash-swap as an actual trade, significantly affecting its price feed.","twap-oracle-manipulations#TWAP Oracle Manipulations":"In January, Float Protocol's Pool at RariCapital relied on Uniswap V3's FLOAT/USDC pair as a price oracle. Due to the low liquidity in this pair, the FLOAT price increased significantly when an attacker bought FLOAT with around 47 ether of own funds. After waiting a few minutes for the TWAP to be affected, the attacker deposited overvalued FLOAT to borrow other assets.\nSometime later in January, Rari's Pool 19 was attacked similarly. The attempt failed with a loss of 68 ETH due to an arbitrage bot's rebalancing.\nOnce again, but in April, Rari's Pool 45 was at risk of a price manipulation attack since the UniV3 pool that it relied on as a price oracle had extremely low liquidity. Additionally, the TWAP-window of 10 minutes was very short and would have caused an exponentially increasing price a few minutes after a single low-volume buy. This extremely overblown collateral value would have allowed running away with all of the vault's borrowable funds with very little start capital.\nShortly before that, in April, Inverse Finance was exploited under similar circumstances of relying on a low liquidity AMM & a short TWAP-window. This attack stood out by the large deployment of capital and how meticulously they intended to prevent MEV bots from correcting the price and generalized frontrunners from reacting: Splitting initial tornado cash funds to many clean addresses, deployment of fake exploit contracts, manipulation of prices in multiple markets, spamming transactions.\nIn June, Sense Finance's onSwap() function could be called by anyone while feeding it with dummy swaps that would have influenced the TWAP's price calculation. An attacker could have exploited this to cheaply manipulate asset prices by calling it every few minutes and driving the TWAP of the pool in whichever direction they‚Äôd like. The function was intended to be used as view-only for non-authorized users to provide previews of swaps without executing them.","incorrect-integration#Incorrect Integration":"May, Feminist Metaverse's token had its reserves drained when an attacker exploited the fact that it wasn't adding liquidity to an AMM's swap pair correctly. The _transfer() function was intended to provide a chunk of tokens as liquidity whenever a transfer happens. But instead of actually staking these tokens as liquidity, it simply transferred them to the market pair's contract. The attacker exploited this by making hundreds of small transfers, moving all of the reserves to the AMM, and then finally calling the public skim() function to steal the excess of unstaked tokens.\nIn June, Inverse Finance was exploited once more. This time, it used Curve's USD-BTC-ETH-pool balances as a price oracle, which could be manipulated to significantly increase the collateral value within Inverse and borrow unproportionate sums of money. The more interesting aspect of this incident, though, is that even if this had not been vulnerable to price manipulation through flashloans, it would still have been vulnerable because Inverse did not determine prices the same way Curve did. While Curve keeps account of token balances within its storage, Inverse relied on the actual token balances for the calculation. An attacker could have exploited this difference by sending Curve tokens without actually depositing them, effectively causing a discrepancy between the resulting prices of LP tokens at Inverse and Curve.","vulnerable-rebalancingbuyback-mechanics#Vulnerable Rebalancing/Buyback Mechanics":"End of April, bDollar's algorithmic stablecoin price was raised in multiple pools of pancake swap through flashloans. This price manipulation was exploited by calling the public claimAndReinvestFromPancakePool() function of its CommunityFund contract, which attempted to re-balance these markets at disadvantageous prices.\nIn June, TraderJoe's protocol fees in the form of liquidity tokens were stolen from a vulnerable buyback mechanism that was supposed to reward xJOE holders with JOE tokens. Normally this works by using the collected liquidity tokens to withdraw the underlying tokens from the pair contract and then converting these to JOE. Things get problematic when one of the fee liquidity tokens collected is from a pair where one of the tokens is itself a liquidity token, a liquidity token that has been accumulating for its own pair contract. When the conversion was attempted here, the vulnerable contract withdrew liquidity tokens and swapped them as usual instead of using them for a withdrawal. This causes valuable LP tokens to be swapped in illiquid markets allowing the attacker to exploit slippage to obtain them cheaply.","faulty-native-token-handling#Faulty Native Token handling":"In February, MeterIO's Bridge had a bug in its automatic wrap/unwrap logic for native tokens. There were two functions allowing for deposits, one for native ETH and one for ERC20. In the native case the function would automatically wrap the value that was sent as part of the transaction and immediately transfer it to the handler. For the ERC20 case, the handler would transferFrom() the tokens from the depositor. Attackers could use the ERC20 deposit function to make it look as if they deposited native tokens, the handler would then assume that it already received those native tokens in a wrapped form and therefore skip attempting to transfer anything from the user.","frontrunning#Frontrunning":"In January, Zora's NFT sale contract had a vulnerability that shows how infinite approvals can bite one back. To buy an NFT, users had to give the contract an allowance before calling the function to trigger the sale in a separate transaction. A malicious seller could frontrun the second transaction to change the NFT's price and take all of the buyer's ERC20 tokens that were approved beforehand. Since unlimited approvals tend to be the default, that would quite possibly be all of the tokens the buyer owns.","serializationparsing-issues#Serialization/Parsing Issues":"In February, Superfluid's use of \"context objects\" was exploited. These represent a serialized state shared between multiple contracts. An attacker crafted calldata such that the process of serialization in one contract and succeeding de-serialization in another contract caused the system to operate on a context object forged specifically to impersonate other accounts. De-serializing contracts trusted calls from the serializing contract without further validating the provided context.\nLater in March, Gearbox's UniswapV3-Adapter parsed swap-paths (tokenA, tokenB) by selecting the first and last elements in the path array. UniswapV3, on the other hand, parsed the path using absolute offsets within a byte-array, meaning that one could simply add another element to the array and both protocols would end up parsing a different end of the path (tokenA, tokenB-uni, tokenB-gb). This would have allowed a borrower to bypass Gearbox's collateral health checks since it would end up checking a different token's value.","naive-trust-assumptions#Naive Trust Assumptions":"In February, EarnHub trusted a user-supplied address to be an honest pool that users could move their funds to. To move said funds to this \"pool\", it was given an unlimited allowance to all of the protocol's funds, not just the user's. The protocol's funds were drained by an attacker moving funds to their own malicious pool contract.\nIn May, the Feed Every Gorilla Project gave user-supplied addresses approval for the user's deposited funds. The attacker exploited this by making a deposit and then having the contract approve multiple addresses to use the same deposited amount. These addresses were under control of the attacker and could each spend the allowance given to them, effectively spending multiples of the user's actual balance, draining the contract.\nIn August, Talent Protocol planned to switch their contracts between maturity phases depending on whether their native token TAL had been finished yet. The public setToken() function allowed setting this token as long as it claimed to implement the ERC-20 standard and returned the symbol 'TAL' when asked. Since anyone could call this function, an attacker would have been able to set a malicious token causing the protocol to switch phases, effectively locking all funds unless one had access to the said tokens. This could have been used to hold the protocol's funds at ransom.\nIn October, BabySwap trusted a user-supplied address to be a valid swap-pair factory. An attacker exploited this by deploying a malicious factory that returns fake swap pairs for a real token pair. This allowed them to claim real BABY token rewards for fake swaps.\nYet again in October, TempleDAO‚Äôs STAX protocol was exploited via its migrateStake() function. Users would specify the address of the old staking contract to migrate funds from and the amount to migrate. However, the contract trusted this user-supplied address without any further checks on whether it belongs to a valid staking contract. A user could specify any as long as the function call to it would not revert. An attacker noticed this and started \"migrating\" ~2.3M worth of tokens from nowhere.\nOctober, Bond Protocol's BondFixedExpiryTeller.redeem() function trusted a user-supplied address to be a legitimate OHM bond token of the protocol. The contract would call the supplied token's burn function and then send an arbitrary amount of the underlying token from the contract to the caller.\nOctober, BitBTC's Bridge between mainnet and Optimism trusted a user-supplied layer-2-token to return the appropriate layer-1-token it represented. An attacker simply had to deploy a fake token that returned an actual layer-1 token address when its l1Token() function was called. The bridge did not validate this return value. It would have processed the withdrawal by paying out the specified amount of valuable layer-1 tokens in exchange for the fake token on layer 2.","uninitialized-proxies#Uninitialized Proxies":"In January, Ondo Finance had several minimal proxies that used the TrancheToken implementation contract, which was not initialized. An attacker could have created a fake vault contract which would be passed to TrancheToken as an initialization parameter. This vault contract would then have been able to call destroy() on the implementation, permanently bricking all proxies delegating calls to it.\nA big one in May, where Wormhole's UUPS style proxy had an uninitialized implementation after a recent upgrade. An attacker could have exploited this to delegate-call to another contract causing the implementation to self-destruct, effectively bricking the proxy and likely locking up all funds forever.\nYet again in May, Agave ignored the news and missed that Aave, the project it was based on, was notified by Trails of Bits that its implementation's initialize() function was still callable. Here an attacker could have set a malicious _addressesProvider contract which the liquidationCall() function fetches an collateralManager address from that it'll delegate-call to.","storage-collisions#Storage Collisions":"In July, Audius' contracts used OpenZeppelin's proxy upgradability pattern, where functions that are only supposed to be called once during deployment are protected by the initializer() modifier. Unfortunately, Audius had overridden the standard implementation adding logic that used storage slot 0 for the proxyAdmin address while this same slot was also used for the initializer's state booleans. The address that was currently set caused these booleans to flip in a way that allowed the initialization functions to be called again, which an attacker exploited to take over control of the project's governance.","reinitialization-vulnerability#Reinitialization Vulnerability":"August, Genome DAO's liquidity token staking contract was drained when an attacker discovered a public initialization function that was always callable. The function allowed setting the address of the underlying liquidity token that could be staked in the contract. The attacker exploited this by temporarily setting a worthless token, staking them, re-setting to the original LP token, and finally withdrawing valuable LP tokens using the ill-gained staking tokens.","integer-overflows#Integer Overflows":"In March, Umbrella Network's Reward Pool was drained via its withdraw() function, subtracting the user-supplied amount from the user's current balance. It appears they wanted to rely on the subtraction reverting on overflows to prevent users from withdrawing more than they own, but the Solidity version used was 0.7.5, and they did not use any SafeMath library either. So instead of reverting, attackers were able to make arbitrary withdrawals from the pool.","incorrect-special-character-handling#Incorrect Special Character Handling":"In April, ENS domain names could be duplicated by re-registering an existing name with a 0x00 byte appended at the end. Most off-chain services would listen to the emitted registration event and terminate the string at the null-byte, effectively treating it like the original name.","botched-upgrades#Botched Upgrades":"In March, ENS's governance voted for a proposal to change the pricing oracle for ENS names. Due to bad release management, this updated pricing oracle returned two integer values while the calling code only expected one. The second return value would have been ignored, effectively making the domains cheaper than intended.\nIn August, Nomad's Bridge was upgraded, introducing a fatal issue in cross-chain message verification: The process() function would look up the merkle-root of the user-supplied message from the messages map. As usual with maps, if no value has been set (therefore, the message has no known merkle-root), it would return the zero value. Unfortunately, the zero-value had been set as a valid merkle-root during the initialization of the contract, basically allowing any user-supplied message to be mistakenly verified. This allowed the draining of the bridge's funds by sending fake withdrawal-transfer messages.","governance-takeovers#Governance Takeovers":"In April, Beanstalk Farms' governance was taken over by the execution of a malicious proposal. The attacker temporarily obtained significant amounts of the overall voting power by using a flashloan and used it to vote on their proposal to execute a malicious contract. The voting power was sufficient to bypass the 2/3rds threshold required to call emergencyCommit(), allowing them to execute it after waiting for only a single day.\nIn September, GnosisGuild's Reality module caused several DAOs a loss of funds when it turned out that nobody was monitoring optimistic proposals for their validity. In the end, the attacker's malicious proposals were executed since nobody challenged them.","flawed-math#Flawed Math":"In April, Saddle Finance's swapping function was attacked when it didn't scale the amount of LP tokens correctly. The issue is complex to understand but was likely known since it was already fixed in the verified code. Unfortunately, the swap code did not make use of the fixed version of the library doing these calculations. Boiled down, the attack simply consisted of taking a flashloan and swapping between saddleUSD and synthetix' sUSD back and forth.\nIn October, Timeless' Bunni Vault allowed the first depositor to be sandwiched by a malicious MEV bot. In practice, the attacker would frontrun a user depositing eg. $10 and first deposit 1 wei so that the vault is bootstrapped with 1 share being equal to 1 wei. Then the attacker would provide $11 of liquidity on behalf of Bunni at Uniswap without making a deposit minting shares. Because  rounded down as per _mintShares(), would result in 0 shares for the victim, the attacker would now be able to use their 1 share to withdraw theirs and the victim's liquidity.","transaction-replay-attack#Transaction Replay Attack":"In June, Optimism intended to send millions worth of tokens to the liquidity provider Wintermute on their L2 chain. There must have been a miscommunication though, since Wintermute had a Gnosis Safe on this address on mainnet but on Optimism the destination that the tokens were sent to was completely uninhabited. The attacker funded the Gnosis deployer address and then replayed the transactions that deployed the Gnosis Safe factory. The factory created Safes using the CREATE opcode, which uses the contracts nonce of contract creations to determine the following address. The attacker exploited this by repeatedly increasing the nonce until the same nonce that the Wintermute team had used to create their safe was reached. With this, the attacker could deploy their own Gnosis Safe on the said address and freely use the \"lost\" funds.","logic-errors#Logic Errors":"In January, Notional Finance got a Bug Report about a function that wasn't exposed via any User Interface but was still externally callable. The function was part of a feature that had not been used in production yet and, even then, would likely have remained a niche functionality few would have used. Deposited assets in a User's Account within Notional were supposed to belong to either one of two possible types. The function in question would have allowed switching from one type to another but due to a logic error, this switch did not always function properly and caused assets to be counted twice as either type.\nAlso in January, REDACTED‚Äôs wxBTRFLY token had a transferFrom() function that incorrectly updated the allowance in a way that effectively would have allowed attackers to steal allowances. The error was when it attempted to load the current allowance. Instead of doing so for the msg.sender it loaded the allowance of the specified recipient. It then subtracted the amount being transferred from it and updated the allowance by setting msg.sender as the spender. An attacker could first make a transfer with amount 0 to a recipient who was given an allowance by the sender. By doing so, the attacker would have obtained this allowance and would have been able to arbitrarily spend the senders tokens.\nEnd of January, Yearn's SSB Strategy for their yvUSDT Vault was reported to be vulnerable. The issue was that upon withdrawal, it tried to return the user's requested amount without regard to how much pool token it burned through. Effectively allowing to burn more tokens than the attacker actually owned, distributing losses among the other shareholders. This vulnerability was only exploitable under particular circumstances and the usage of flashloans to manipulate the USDT price, though, likely stemming from the assumption that a stablecoin would always be stable.\nIn February, Tecra's burnFrom() function, intended to allow burning tokens that one was approved to make use of, instead allowed burning tokens of others by giving them an allowance. This was exploited for price manipulation on Uniswap by burning tokens from the pool.\nIn April, Bunker Finance's NFT-wrapping contract would allow minting multiple wrapper-tokens of the same NFT. An attacker could have used that to redeem NFTs that were sold by the attacker to the victim and then added back into the protocol by the victim.\nIn June, XCarnival's lending platform was exploited when an attacker noticed that NFTs deposited as collateral could still be borrowed upon, even after they had been withdrawn. This effectively allowed depositing the same NFT over and over again, with each deposit one gained an \"orderId\" which could be used to borrow with the NFT as collateral even if it was already withdrawn.\nIn August, KaoyaSwap had an error in its swap-to-WETH function when the swap path contained the same pair twice. An attacker set up pairs and liquidity for their own tokens A and B against WETH. Then exploited the protocol with the swap path A ‚Üí WETH ‚Üí B ‚Üí A ‚Üí WETH where the same money is swapped from A to WETH twice, effectively reducing the amount of WETH in that pair twice. The issue was that KaoyaSwap used the difference of WETH in the last pair before and after the swap as the amount it should send to the user - which the attacker could double this way.\nIn November, BribeV2 provided a mechanism for holders of Curve's governance token to lend their voting power to entities needing them to vote for their preferred Curve Reward Gauges. However, the contract's reward calculations incorrectly relied on the amount of locked CRV instead of the effective voting power from veCRV (which decays over time).\nIn December, 88MPH's vesting03 contract allowed withdrawing MPH token rewards before the deposit had matured. The reason was that a variable used for the calculation of rewards was not set as intended during the deposit of funds.","exploiting-approvals#Exploiting Approvals":"In July, Quixotic's NFT marketplace allowed attacks to create sell orders with worthless NFTs and fill them using the funds of users who approved the marketplace's contract. The code would only verify the attacker's sell order signature and then pay the worthless NFT with any buyer address that the attacker chooses, as long as the victim approved a sufficient balance of the specified ERC20 token.\nIn August, SZNS's BountyBoard contract allowed filling bounties with NFTs for which collection a user once gave approval for. Suppose a user once had given full approval to participate in a bounty offer to obtain ERC20 tokens for an NFT. In that case, an attacker could have created their own bounty and filled it with the user's NFT for worthless ERC20 tokens. Alternatively, an attacker could have participated in a legit bounty and obtained the valuable ERC20 reward while using another user's NFTs who had previously given the contract approval to use them.","gas-siphoning#Gas Siphoning":"In February, dYdX's Gassless Deposit service could be misused to make arbitrary (and potentially expensive) calls to other contracts. To prevent misuse like this from happening, the service was already restricted to a single whitelisted address but the user specifiable parameters exchangeProxy and exchangeProxyData are still allowed to make arbitrary calls during the deposit.\nIn October, FTX didn't check whether the receiver during an ETH withdrawal was a contract, nor did it place reasonable limits on the transaction's gas usage. An attacker could exploit this by having FTX send ETH to a malicious contract triggering its fallback function. Gas-intensive operations to mint XEN tokens were used, which FTX paid the bill for.\nIn October, Ethereum Alarm Clock's four-year-old TransactionRequestCore contract was exploited when transaction agents were refunded with more gas than they had actually spent for the transaction's execution.","ui-issues#UI Issues":"In June, DXDao's Treasury allowed their ContributionReward contract access to funds to pay contributors through governance proposals. The contract allowed requesting rewards with a \"period\", which lets contributors redeem the specified reward amount multiple times. However, this parameter wasn't shown in the UI so that a malicious user could have created a proposal asking for a seemingly small reward amount with a big hidden period acting as a multiplier.","other#Other":"In February, Wormhole's Solana Bridge was hacked in ways that I still don't fully grasp. Apparently, the attacker apparently replaced Rust pre-compiled code that the Bridge on Solana used with their own code that allowed him to verify forged relayer/guardian signatures. This vulnerability was exploited to mint weETH on Solana, which was then \"bridged back\" to Ethereum draining the Bridge on mainnet.\nA Bug in Optimism's Geth fork was also found in February. \"Unbridled\" is not a smart contract issue per se, but still interesting enough to mention: Native OETH token on Optimism's layer2 chain could be duplicated because a contract's account balance was not set to 0 after triggering its self-destruction, but this amount was still also accredited to the specified target address.\nIn October, BNB Bridge was manipulated to mint 2 billion BNB on Binance Smart Chain. The issue was a precompiled contract that the bridge used for Merkle proof verification and that the library behind it was not meant to handle untrusted user input. Without any further verification of the user-provided proof, the attacker could craft one that made use of the fact that the library would validate proofs of multiple values in an efficient but naive manner.\nNote that being mentioned in this list is by no means attacking any of the projects. Many of the incidents here only made it to this list because of their well-written post-mortem articles. Incidents not mentioned here are more likely to be worthy of criticism: Some projects didn't bother to write a post-mortem analysis at all, while others did it with botched technical explanations and stock full of lazy excuses. Not to mention the many closed-source projects on BSC that lost millions of funds and make one wonder about investors' lack of due diligence."}},"/posts/2022/12/6/race-12-of-the-secureum-bootcamp-epoch":{"title":"RACE #12 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-12, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors.\nI once again had the honor of designing it and gave my best to find something that is both challenging and interesting. I imagine it was very hard to solve all 8 questions within the strict timelimit of 16 minutes, but I recommend you‚Äôll try to see how far you come with using more time before looking at the solutions.As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nDecember 6, 2022 by patrickd","code#Code":"All 8 questions in this RACE are based on the following contracts. You will see them for all the 8 questions in this RACE. The questions are below the shown contracts.","question-1-of-8#Question 1 of 8":"Sensible gas optimization(s) would be\n A. Making MIGRATOR_ROLE state variable constant \n B. Making UNDERLYING state variable constant \n C. Making MIGRATOR_ROLE state variable immutable \n D. Making UNDERLYING state variable immutable \nCorrect is A, D.While MIGRATOR_ROLE can be made both constant or immutable, using constant makes the most sense since the value is known during compile time.UNDERLYING on the other hand can only be immutable since the value is passed in during the contract's construction.","question-2-of-8#Question 2 of 8":"What would a caller with MIGRATOR_ROLE permission be capable of?\n A. Manipulating TokenV1's storage \n B. Deleting TokenV1's stored bytecode \n C. Changing TokenV1's stored bytecode to something different \n D. With the current code it's not possible for anyone to have MIGRATOR_ROLE permission \nCorrect is A, B.A contract that was given the MIGRATOR_ROLE will be capable of triggering TokenV1's fallback function which will delegate-call them back. During this delegate-call, the contract will be capable of manipulating storage as well as self-destructing TokenV1.Replacing the bytecode would only be possible if TokenV1 was deployed via CREATE2, allowing for a redeployment at the same address with a different bytecode.The public grantRole() function is inherited from AccessControl and allows callers with DEFAULT_ADMIN_ROLE to grant the MIGRATOR_ROLE to any address.","question-3-of-8#Question 3 of 8":"Vault initialized with TokenV1 as underlying\n A. Can be drained by re-entering during withdrawal \n B. Can be drained during withdrawal due to an integer underflow \n C. Allows stealing approved tokens due to a phantom (i.e. missing) function \n D. None of the above \nCorrect is C.A Vault initialized with TokenV1 offers no opportunity to re-enter although the Checks-Effects-Interactions pattern is not being followed. For this to be exploitable the underlying token itself must either be malicious or implement something akin to receive-hooks like ERC-777.The fact that a Solidity version >0.8.0 is used without any unchecked blocks should prevent any integer over- or underflows from happening.The depositWithPermit() function is relying on the call to TokenV1's permit() method to revert if it's not implemented or the provided signature is invalid. However, TokenV1's fallback() function will make any calls to permit() succeed, making permit() a \"phantom function\". With any calls succeeding an attacker would be able to make deposits using the allowances of other users without the need for a valid signature.","question-4-of-8#Question 4 of 8":"If Vault were to use safeTransferFrom instead of transferFrom then\n A. It would be able to safely support tokens that don't revert on error \n B. It would ensure that tokens are only sent to contracts that support handling them \n C. It would introduce a re-entrancy vulnerability due to receive hooks \n D. None of the above \nCorrect is A.The way Vault is currently implemented, its deployer needs to be careful to not use a token as underlying that returns success booleans instead of reverting on error. Using the SafeERC20 library would add the saveTranserFrom() function and allow both kinds of token implementations to be used.Answers B and C talk about the \"save\" methods of NFT standards such as ERC721. These would check whether a receiving contract declares supporting them and would also offer an opportunity for re-entrancy via the onERC721Received() hook.","question-5-of-8#Question 5 of 8":"Who would need the MIGRATOR_ROLE for TokenV2 to function as intended?\n A. The deployer of the TokenV2 contract \n B. The TokenV1 contract \n C. The TokenV2 contract \n D. The PermitModule contract \nCorrect is C.The deployer of TokenV2 would need DEFAULT_ADMIN_ROLE for granting it the MIGRATOR_ROLE.TokenV2 is the only contract that requires the role since it needs to (ab)use TokenV1's fallback function trigger a delegate-call to the PermitModule's contract.","question-6-of-8#Question 6 of 8":"With TokenV2 deployed, a Vault initialized with TokenV1 as underlying\n A. Is no longer vulnerable in the depositWithPermit() function \n B. Becomes more vulnerable due to a Double-Entry-Point \n C. Stops functioning because TokenV1 has been replaced \n D. None of the above \nCorrect is B.A Vault initialized with TokenV1 will always be vulnerable in the depositWithPermit() function. A new Vault would need to start using TokenV2 for the depositWithPermit() function to no longer be vulnerable to the permit()-phantom.TokenV2 now acts as a Double-Entry-Point for TokenV1 (and vice versa). This can be exploited via the sweep() function which allows rescuing any stuck tokens as long as they are not the underlying token. Sweeping TokenV2 on a Vault using TokenV1 would effectively drain the Vault.","question-7-of-8#Question 7 of 8":"Vault initialized with TokenV2 as underlying\n A. Can be drained by re-entering during withdrawal \n B. Can be drained during withdrawal due to a integer underflow \n C. Is not vulnerable in the depositWithPermit() function \n D. Is vulnerable due to a Double-Entry-Point \nCorrect is C, D.Answers A and B haven't changed from Question 3 with the introduction of TokenV2.In TokenV2 calling the permit() function will no longer call the fallback but its actual implementation from ERC20Permit. Therefore a Vault with TokenV2 will no longer be vulnerable to the permit()-phantom.TokenV1 acts as a Double-Entry-Point for TokenV2 (and vice versa). This can be exploited via the sweep() function which allows rescuing any stuck tokens as long as they are not the underlying token. Sweeping TokenV1 on a Vault using TokenV2 would effectively drain the Vault.","question-8-of-8#Question 8 of 8":"The PermitModule contract\n A. Acts as a proxy \n B. Acts as an implementation \n C. Allows anyone to manipulate TokenV2's balances \n D. Can be self-destructed by anyone \nCorrect is B, D.TokenV2 will forward delegate-calls made from TokenV2 to the PermitModule contract. Effectively TokenV2 abuses the migration logic in TokenV1 turning it into a proxy while PermitModule acts like the implementation.The way that Context's _msgSender() function has been overridden, it allows any caller to arbitrarily set what is expected to be the msg.sender. By crafting a call with TokenV1's address appended they will gain the DEFAULT_ADMIN_ROLE. With that, they can grant themselves the MIGRATOR_ROLE and make PermitModule delegate-call back. Since all token balances are stored at TokenV1's address this will not allow for a manipulation of any real balances. But it would allow delegate-calling to a contract that'll execute a self-destruct."}},"/posts/2022/2/10/secureum-bootcamp-care-audit-sushi-bentobox-strategies":{"title":"Secureum Bootcamp CARE: Sushi's BentoBox Strategies","data":{"":"February 10, 2022 by patrickd\nThis report is the result of Secureum Bootcamp's Epoch 0, second phase, CARE (Comprehensive Audit Readiness Evaluation) which is intended as a way to improve a project's security stance in preparation for an Audit, allowing Auditors to spend more time looking for critical issues. As such, this review centers mostly around the Security 201 Pitfalls & Best Practices that were taught during the Bootcamp. Despite that, and the fact that the review is done by me alone, this is still a best-effort approach to find as many issues beyond that as possible.","scope#Scope":"Focus of this review are BentoBox Strategy contracts belonging to the Sushi/SushiSwap ecosystem, of the commit hash be407e27cd1a9ca8060ef9a389d1cf01b4e5540e:\ncontracts/BaseStrategy.sol\ncontracts/strategies/AaveStrategy.sol\nThe following additional information was provided:\nKnowledge of how BentoBox interacts with the strategies is crucial for understanding the bentobox-strategies. Pay special attention to the BentoBox ‚Äúharvest‚Äù and ‚ÄúsetStrategy‚Äù methods.Tests and Utilities:\nSet up a .env file and include an alchemy API key. (note: the block number used for forks might have to be updated if there is no access to an archive node).Key risks:\nEnsure Base strategy does not misreport profit or losses to BentoBox.\nEnsure no loss of funds can happen from external actors.\nEnsure BentoBox can always swap a strategy for a new one (‚Äòstrategy.exit()‚Äô call should never fail)\nEnsure the Aave strategy correctly deposits and withdrawals from the Aave lending platform.","timeline#Timeline":"The review was conducted in December 2021 over a period of two weeks, from Monday the 6th until Friday the 17th. In the first week, the focus was on gaining a general understanding of the BentoBox project and the Sushi ecosystem from reading documentation, blog posts and exploring the codebase. After checking for 201 Secureum Security Pitfalls & Best Practices on contracts in scope, analyzers such as Slither v0.8.2, MythX, and smartdec's SmartCheck were run. The second week was focused on manual review for the above-mentioned Key Risks and finished by setting up fuzzable properties for them in Echidna.","findings#Findings":"Count\tSeverity\tDescription\t0\tCritical\tDirectly exploitable security vulnerabilities that need to be fixed.\t1\tMajor\tSecurity vulnerabilities that may not be directly exploitable or may require certain conditions in order to be exploited. All major issues should be addressed.\t5\tMedium\tObjective in nature but are not security vulnerabilities. Should be addressed unless there is a clear reason not to.\t11\tMinor\tSubjective in nature. They are typically suggestions around best practices or readability. Code maintainers should use their own judgment as to whether to address such issues.\t\nBefore publishing the report, the code was roughly checked for whether and which issues have been addressed. Those findings have been marked as FIXED fixed when sufficient mitigation was found.","1-major-utility-function-enables-privilege-escalationabuse##1 Major: Utility function enables privilege escalation/abuse":"Identified during manual review. Related checklist items would be 160. Trusted actors, 161. Privileged roles and EOAs and 181. Trust issues of Security Pitfalls & Best Practices 201.","description#Description":"Governance of the BentoBox contract and its Strategies is, according to documentation, under control of the Sushi Operations MultiSig, which requires at least 3 signatures for a change to pass.This is rather centralized and requires some trust of the users in the Ops Team. To restrict the powers of the operations team, some measures were taken such as the 2-weeks-delay when setting new Strategies (STRATEGY_DELAY) allowing the Sushi community to review Strategies before they take effect. There's also the BaseStrategy.afterExit() function that allows powerful arbitrary calls to be made for the purposes of rescuing funds, but it can only be used once the Strategy has been exited and all funds have been withdrawn from the Strategy.The utility function swapExactTokensForUnderlying() introduced by the abstract BaseStrategy contract allows bypassing these measures and will affect all Strategy implementations inheriting it. The requirement of the exploit scenario is that either the Ops MultiSig Wallet has been compromised or that at least 3 bad actors within the Operations Team collude.","exploit-scenario#Exploit Scenario":"Ops MultiSig (OMS) calls BentoBox.setStrategy() in order to replace the current AaveStrategy with a new Strategy, waits for the 2-week-delay to pass, but does not call it again yet for the Strategy-change to take effect.\nOMS adds a new path via BaseStrategy.setAllowedPath() allowing swapping of aTokens to WETH (or any other asset that is not the invested strategyToken itself).\nOMS adds an Executor it has full control over via BaseStrategy.setStrategyExecutor(), if one isn't already under control of OMS.\nOMS uses Executor and calls BaseStrategy.swapExactTokensForUnderlying() to swap all aTokens with newly added swapping path.\nOMS now calls BentoBox.setStrategy() again for the Strategy-change to take effect, which makes BentoBox call BaseStrategy.exit().\nExit appears to have succeeded, but all of the Strategy's funds are still in the Strategy contract.\nOMS is now able to do arbitrary calls via BentoBox.afterExit(), allowing all of the Strategy funds to be stolen.\nThe impact of this scenario can further be increased by raising the Strategy target percentage (via setStrategyTargetPercentage()) to 95% (MAX_TARGET_PERCENTAGE) of the BentoBox's funds and rebalancing them into the active Strategy (using harvest(token, true, 0)).","recommendation#Recommendation":"To prevent this specific exploit scenario, ensure that all swap paths must end with the strategyToken to be valid. This should ensure that when swapping happens with Strategy funds, they will be withdrawn by BentoBox when the Strategy is exited. That way the powers of the Sushi Operations MultiSig should again be appropriately restricted, at least within expectations.","1-mediumfixed-setallowedpath-incorrectly-logs-array-length-instead-of-index##1 MediumFIXED: setAllowedPath() incorrectly logs array length instead of index":"Identified during manual review. A possible corresponding checklist item would be 173. Auditing/logging issues of Security Pitfalls & Best Practices 201.","description-1#Description":"The LogSetAllowedPath event logs the pathId which is the index of a path within the _allowedSwapPaths array:contracts/BaseStrategy.sol#L43\nThe setAllowedPath() function incorrectly logs the array length after a new path was added instead of the index of the new path:contracts/BaseStrategy.sol#L269-L272\nIncorrect logging of the pathId could cause wrong assumptions for the operations team (owner) when maintaining the _allowedSwapPaths array contents. It could for example lead to an incorrect path being disabled via disallowPath().","recommendation-1#Recommendation":"Correctly log the index as pathId instead, for example, by calculating _allowedSwapPaths.length - 1.","resolution#Resolution":"Allowed swap path logic has been changed and the LogSetAllowedPath event does no longer exist.","2-medium-unclear-possibly-incorrect-access-control##2 Medium: Unclear, possibly incorrect, Access Control":"Identified during manual review. The corresponding checklist items would be 148. Access control specification, possibly also 4. Incorrect access control and 150 Missing modifiers of Security Pitfalls & Best Practices 101 and Security Pitfalls & Best Practices 201.","description-2#Description":"There's no specification regarding Access Control, the system actors, and the actions they are allowed to take. This forces Auditors to infer intention from the implementation making it difficult to distinguish between oversight and a purposeful choice.An example of this issue would be the BaseStrategy.skim() function, which has no access control at all, allowing any external actors to trigger the investment of funds, even if the caller isn't the BentoBox contract or even when the strategy was already exited and is therefore considered inactive:contracts/BaseStrategy.sol#L142-L144\ncontracts/strategies/AaveStrategy.sol#L73-L75\nWhile this is not a direct vulnerability that could cause a loss of funds under normal circumstances, I was unable to infer a good reason for this function to lack all of the modifiers similar functions such as harvest(), withdraw() and exit() have.","recommendation-2#Recommendation":"Create an Access control specification that describes how the various system actors, their access control privileges and trust assumptions are intended to be in great detail.Add appropriate modifiers (most likely isActive and onlyBentoBox) to the skim() function or add inline documentation on why these were skipped on purpose.","3-medium-lack-of-specification##3 Medium: Lack of Specification":"Identified during manual review. The corresponding checklist item is 136. System specification of Security Pitfalls & Best Practices 201.","description-3#Description":"The lack of any specification forces Auditors to infer the intent from the implementation and the context of it being a BentoBox Strategy. In comparison, the BentoBox contracts have formal specifications, validated by the Certora prover. The BentoBox repository also contains a CompoundStrategy.sol implementation with its corresponding specification compoundStrategy.spec. It's likely that these specifications can be reused for the github.com/sushiswap/bentobox-strategies repository with reasonable effort.","recommendation-3#Recommendation":"While it would have been best to create a human-readable specification before the implementation was done, in this case, it would still be beneficial to create a formal specification that can prove, together with automated tests, that the implementation was done correctly and will stay so after future changes.I would recommend creating a formal specification based on the Certora prover specs already available in the BentoBox repository.","4-medium-lack-of-documentation##4 Medium: Lack of Documentation":"Identified during manual review. The corresponding checklist items are 137. System documentation and possibly 188. Clarity issues of Security Pitfalls & Best Practices 201.","description-4#Description":"The documentation of the contracts in scope currently consists only of their inline comments and a single README.md file. While these have decent quality, there's still a lack of higher-level documentation, especially compared to the documentation that the BentoBox repository is offering in comparison (eg. on dev.sushi.com).Specific examples of a lack of documentation:\nWhile some instructions on how to run the tests were provided together with the CARE project information, they should have been part of the public documentation in the repository. Additionally, the instructions were insufficient and unclear requiring a lot more setup and investigation until I was finally able to run the tests locally. For example, it is not sufficient to specify an Alchemy API key, a Infura Key was required as well to get them running. And while it is mentioned that the block number used for the fork might need to be adjusted if the user doesn't have a very expensive infura subscription to access mainnet archive data, it is not explained that this needs to be done within the JavaScript code of the tests themselves. These difficulties in combination with the fact that on GitLab CI the tests are currently failing as well, does not induce confidence in the testsuite.\nWhether Aave's V1 or V2 contracts are used is not clearly documented anywhere and requires some research to be determined. Similarly, there's the question of why Uniswap V2 is used instead of the current V3, especially since in the code it is specified as being \"legacy\".\nThere's also an apparent lack of documentation on how to use and handle the lifecycle of a BentoBox Strategy for Sushi's operations team. Setting and replacing strategies, harvesting rewards properly, handling emergencies, and available ways to attempt the rescue of funds, are all manual steps members of the Ops Team have to understand and apply properly. A lack of clarity on how to handle them can lead to security issues or loss of funds, especially if people in the team, and therefore implicit knowledge, are lost. A specific example would be the fact that Strategies are one-time use, once exited, they can still be set as valid Strategy and they will still be able to receive tokens from BentoBox, but skimming will fail, requiring the use of afterExit() to rescue the funds. The fact that Strategies must not be re-used should be clearly documented both for the Ops Team and the community.","recommendation-4#Recommendation":"Additionally to what already exists, write high-level documentation similarly to how it was done for BentoBox.Specific recommendations for the mentioned examples:\nCreate public and more detailed instructions on how tests can be run and explain how to do so while lacking access to costly subscriptions. Consider the introduction of more environment variables that allow easily tweaking test parameters without the need of changing hardcoded constants. Fix the CI pipelines on GitHub and add badges for test success and coverage to the readme file.\nDocument the versions used of all external contracts and libraries, and explain the choice of using versions that are older than what is currently available. Additionally to documenting the commit hash that was used during deployment in the readme, also document the configuration values that were passed to the constructor during deployment and link them to the appropriate transaction on etherscan.\nCreate public, playbook-like documentation for the Sushi Ops Team to ensure that even after a long time or after the team was replaced by new members, knowledge on the maintenance and incident handling for BentoBox Strategies isn't forgotten and can be made use of without requiring long investigations. Especially during emergency-like situations, where time is of the essence to save funds, this will likely prove useful.","5-medium-lack-of-test-coverage##5 Medium: Lack of Test Coverage":"Identified during manual review. The corresponding checklist item is 155. Tests of Security Pitfalls & Best Practices 201.","description-5#Description":"The coverage of the existing automated tests is lacking, especially in regards to the key risks that were identified for CARE.Specific cases that have been identified as lacking a test:\nFunction and access control of BaseStrategy's getAllowedPath(), setAllowedPath(), disallowPath() and setStrategyExecutor() functions.\nWhether BaseStrategy's isActive modifier functions correctly, preventing access after the Strategy was exited, and is applied to all appropriate functions.\nWhether the BaseStrategy.afterExit() function can only be accessed by the contract owner (testing for onlyOwner modifier) and that it can only be successfully called once the Strategy was exited.\nWhether the maxChangeAmount parameter, set by BaseStrategy.safeHarvest() and used by BaseStrategy.harvest(), works as expected.\nWhether the slippage protection offered by the amountOutMin parameter of BaseStrategy.swapExactTokensForUnderlying() functions correctly.\nWhether the internal BaseStrategy._swap() function still works correctly for paths with more than 2 assets.\nWhether BaseStrategy.exit() still works as expected during exceptional situations (eg. Aave protocol was hacked and less strategy tokens are available than the contract has aTokens, withdrawal failed with error but exit still succeeds).\nWhether checks in BaseStrategy.harvest(), preventing harvest of senders other than the strategy itself, are implemented correctly.","recommendation-5#Recommendation":"Improve test coverage by implementing the specific cases above and by checking the coverage report. Aim for a minimum measured coverage of 100% and make sure that paths aren't only covered but also properly tested. At the moment many of the tests make simple greater-than-checks instead of checking for specific values, which counts towards coverage but might still allow for issues to slip through in regards to profit reporting. The tests currently also always assume that a profit was made, the handling of losses is currently not tested.Furthermore, I'd recommend a separation of tests between the AaveStrategy implementations and the BaseStrategy. Many tests are specific to the base contract but are duplicated in both AaveStrategyMainnet.ts and AaveStrategyPolygon.ts. To do that, you could create a barebones implementation contract or simply use the ExampleImplementation.sol contract that already exists.","1-minor-unnecessarily-copied-and-adjusted-code##1 Minor: Unnecessarily copied and adjusted code":"Identified during manual review. The corresponding checklist item is 190. Cloning issues of Security Pitfalls & Best Practices 201 and 3. Multiple Solidity pragma as a side effect, see Security Pitfalls & Best Practices 101.","description-6#Description":"The file contracts/bentobox/BentoBoxV1.sol is a copy from contracts/flat/BentoBoxFlat.sol of the BentoBox. However, the STRATEGY_DELAY constant was adjusted from 2 weeks to 0, which was not documented (except for the inline comment // yeehaw). This was likely done to be able to more easily set and change strategies during testing, it can however cause incorrect assumptions and confusion for developers and auditors. It also causes issues with some analysis tools that are unable to handle multiple incompatible Solidity versions within the same project, requiring manual assistance to be run.","recommendation-6#Recommendation":"The BentoBoxV1 contract appears to be only used within tests, and in those, it is not actually deployed but only attached to an existing BentoBox contract from a chain fork. Using an Interface here instead should be sufficient and allow the removal of BentoBoxV1.sol.If there's a reason for keeping the copied code in the repository, this should be properly documented, especially where it was copied from, which version, and what changes were made for what reasons.","2-minor-missing-address-validation##2 Minor: Missing Address Validation":"Identified during manual review. The corresponding checklist item is 49. Missing zero address validation of Security Pitfalls & Best Practices 101 and potentially 165. Configuration issues of Security Pitfalls & Best Practices 201.Also identified by Slither v0.8.2:","description-7#Description":"The BaseStrategy.afterExit() function is available to the Sushi Ops Team as a way to rescue funds after usage of the Strategy has ended. One such case would be rescuing ether funds that were sent to the contract on accident. Due to the missing zero-address validation, it might happen that instead of rescuing the funds they will be burned on accident.Other than that, due to the usage of a send-funds-and-skim pattern, invalid strategyToken and bentoBox parameters used during construction could lead to a scenario where funds have already been sent to the strategy but cannot be skimmed. In such a case the exit() function might not be callable either, which would prevent any rescue of the funds using the afterExit() function.","recommendation-7#Recommendation":"Consider introducing a zero-address validation for the BaseStrategy.afterExit() function and constructor parameters.Introduce a process that makes sure the correct values have been set post-deployment before a strategy is used.","3-minor-emergency-exits-delayed-by-setstrategy##3 Minor: Emergency exits delayed by setStrategy":"Identified during manual review. The issue is related to checklist item 135. Guarded launch via emergency shutdown of Security Pitfalls & Best Practices 201.","description-8#Description":"BentoBox's setStrategy() function enforces a two-week delay for setting new strategies allowing the community to review them before they actually take effect. But since Strategies can only be exited when a new one is set, this also prevents emergency exits eg. when there are issues with the Strategie's underlying protocol.A way to bypass this restriction is using setStrategyTargetPercentage(), setting the percentage to 0, and triggering a harvest for re-balancing.","recommendation-8#Recommendation":"Since BentoBox has already been deployed and further code changes are difficult to introduce, it would make sense to at least document the bypass for the Sushi Ops Team to ensure awareness of this measure once an emergency happens and urgent action needs to be taken.","4-minor-limit-usage-of-new-strategies##4 Minor: Limit usage of new Strategies":"Identified during manual review. The corresponding checklist item is 128. Guarded launch via asset limits of and 129. Guarded launch via asset types of Security Pitfalls & Best Practices 201.","description-9#Description":"Each BentoBox Strategy implementation is likely to use different underlying protocols which means that there could be undiscovered issues when first using one. The MAX_TARGET_PERCENTAGE limit of 95% is extremely high and its immediate usage could lead to a near-total loss of funds when used with a vulnerable Strategy. There's also no restriction to how many BentoBox assets can be using the same Strategy code at once.","recommendation-9#Recommendation":"The process of introducing a new Strategy to a BentoBox asset should always include an initial limitation to the target percentage to ensure that the impact of undiscovered issues is limited. This limit to the percentage can then slowly be raised over time with increased confidence in the Strategy's implementation. It would also be better to initially only use a Strategy for a single or few assets and only increase the number of assets once confidence in the Strategy is sufficient.","5-minor-administrative-functions-should-emit-events##5 Minor: Administrative functions should emit events":"Identified during manual review. The corresponding checklist items are 45. Missing events of Security Pitfalls & Best Practices 101 and 173. Auditing/logging issues, 201. Principle of Compromise Recording of Security Pitfalls & Best Practices 201.","description-10#Description":"No events are emitted in BaseStrategy for afterExit() function that allows arbitrary calls to be made by the owner and swapExactTokensForUnderlying() which allows executors to convert funds.","recommendation-10#Recommendation":"Add events for these administrative functions and ensure they are being monitored for misusage.The other critical functions harvest(), withdraw() and exit(), can only be called by the BentoBox contract which already has separate events for logging after calling these functions.","6-minorfixed-public-functions-should-be-external##6 MinorFIXED: Public functions should be external":"Identified by Slither v0.8.2:\nThe corresponding checklist item is 72. Uncalled public functions of Security Pitfalls & Best Practices 101.","description-11#Description":"Visibility of BaseStrategy's swapExactTokensForUnderlying() and afterExit() can be changed from public to external since they are not called internally and it's also unlikely that inheriting contracts will attempt calling them due to their modifiers.","recommendation-11#Recommendation":"Consider changing the visibility of mentioned functions or document why a public visibility was chosen.","7-minorfixed-redundant-check-can-be-removed##7 MinorFIXED: Redundant check can be removed":"Identified during manual review. The corresponding checklist item is 157. Redundant constructs of Security Pitfalls & Best Practices 201.","description-12#Description":"The BaseStrategy.disallowPath() checks whether the pathIndex\nis within the bounds of the _allowedSwapPaths array before setting a zero-value. This check is already done by Solidity and therefore unnecessarily increasing gas cost.contracts/BaseStrategy.sol#L274-L278\nSolidity 0.8.x does also not yield an INVALID opcode when accessing an array out of its bounds, consuming all available gas, as older versions did. It will instead REVERT, the same way that a require() would.","recommendation-12#Recommendation":"Remove the redundant require statement.","8-minor-making-afterexit-payable##8 Minor: Making afterExit() payable":"Identified during manual review. This is a subjective suggestion with no reference to any best practice.","description-13#Description":"The BaseStrategy.afterExit() function allows making arbitrary calls for the purpose of rescuing funds, allows sending a value too. There are currently only two ways how any ether funds can be sent to a strategy: As the beneficiary (coinbase) of the block reward or via selfdestruct() of another contract.contracts/BaseStrategy.sol#L254-L263\nAside from rescuing locked ether funds from a Strategy contract, there might be a case where ether value needs to be sent for a call to be made. Then one of the above-mentioned workarounds would be necessary to inject the required balance.","recommendation-13#Recommendation":"Consider making the BaseStrategy.afterExit() function payable. Additionally, to increasing flexibility, it also reduces gas usage since Solidity's 0-value-check is skipped.","9-minorfixed-incorrect-usage-of-unlocked-pragma##9 MinorFIXED: Incorrect usage of Unlocked Pragma":"Identified during manual review. This suggestion is related to checklist items 2. Unlocked pragma of Security Pitfalls & Best Practices 101.","description-14#Description":"The file BaseStrategy.sol makes use of an unlocked pragma (pragma solidity >=0.8;). While the intention might have been to regard this abstract contract like a library, leaving it up to the actual Strategy implementations to choose a specific version, in practice that won't be possible since the dependencies included (IStrategy, IUniswapV2Pair, IBentoBoxMinimal, and UniswapV2Library) all have a locked pragma, forcing implementations to use that specific version.","recommendation-14#Recommendation":"Either purposely use an unlocked pragma for BaseStrategy and its dependencies, to allow Strategy implementations to choose their specific version (and documenting that intention), or use the same locked pragma throughout all contracts.It might also be a good idea to explicitly mention the practice of specifying a locked pragma for implementations within ExampleImplementation.sol as an inline code comment.","10-minor-avoid-variable-shadowing##10 Minor: Avoid variable shadowing":"Identified by Slither v0.8.2:\nThis issue is related to the checklist items 39. Dangerous shadowing and 40. Dangerous state variable shadowing of Security Pitfalls & Best Practices 101.","description-15#Description":"The state variables aaveLendingPool and incentiveController of AaveStrategy are shadowed within the constructor of AaveStrategyMainnet.contracts/strategies/AaveStrategy.sol#L54-L60\ncontracts/strategies/AaveStrategyMainnet.sol#L22-L31","recommendation-15#Recommendation":"Avoid shadowing of variables whether local or state. Instead prefix or suffix said variables with _.","11-minorfixed-use-safeerc20-in-exampleimplementation##11 MinorFIXED: Use SafeERC20 in ExampleImplementation":"Identified with the help of Slither v0.8.2:\nRelated to the checklist items 142. Function return values of Security Pitfalls & Best Practices 201.","recommendation-16#Recommendation":"ExampleImplementation constructor should use SafeERC20.safeApprove() to promote correct usage of approval calls for Strategy developers. It currently uses the normal ERC20 approve() function and doesn't check its return value.","missed-findings#Missed Findings":"The following is a list of findings that were additionally found by other participants of the Sushi CARE process (based on Report Draft from 08.01.2022):\nPreferring less risky two-step instead of single-step change of contract ownership\nTypographical errors in code comments, inaccurate comments, missing NatSpec comments\nReturn values of various function calls are ignored\nPublic variable visitbility can be made private to prevent deriving contracts from accidental modification\nDowncasting (from uint256 to int256) is not checked for potential overflow scenarios\nChecks-Effects-Interactions (CEI) pattern is not followed\nMissing sanity/threshold checks for various inputs\nImplicit check for disabled swap paths could be confusing\nCode readability could be improved\nAccount existence check for low-level calls\nChanging critical parameters should be time-delayed\nAvoid usage of named returns or use them consistently\nMissing new != old checks in setter functions\nPrefer custom errors, instead of messages to save gas\nUse OpenZeppelin's Address library for afterExit function\nIncorrect operator (>=) causing breakeven to be reported as profit\nPut vendor (eg. IAaveIncentivesController) interfaces into separate files","disclosure#Disclosure":"This Report was created as part of the Secureum Smart Contract Auditor Bootcamp Epoch 0 without compensation. Publication was held back until the Sushi Team had reviewed the findings and enough time to make corrections.The Report is not an endorsement of the Sushi project, its ecosystem or products, and does not guarantee its safety. The purpose of this Report is to help the Sushi Team improve the security and best practices of the BentoBox Strategy contracts before an actual Audit is undertaken, allowing those Auditors to focus on critical issues.","resources#Resources":"sushi.com\ngithub.com/sushiswap\ndocs.sushi.com\ndev.sushi.com\napp.sushi.com/bentobox\ngithub.com/sushiswap/bentobox\ngithub.com/sushiswap/bentobox-strategies\nSushi Third Course ‚Äî 2021 Q3 Update\nIntroducing, The Sushi Next Generation AMM: Trident"}},"/posts/2022/2/21/damn-vulnerable-defi-v2-6-selfie":{"title":"Damn Vulnerable DeFi V2 - #6 Selfie","data":{"":"February 21, 2022 by patrickd\nThis write-up continues the series on Damn Vulnerable DeFi V2. Please consider attempting to solve it on your own first since it's a lot less fun after being spoiled!\nChallenge #6 - SelfieA new cool lending pool has launched! It's now offering flash loans of DVT tokens.Wow, and it even includes a really fancy governance mechanism to control it.What could go wrong, right ?You start with no DVT tokens in balance, and the pool has 1.5 million. Your objective: take them all.","code-review#Code Review":"From the description alone it sounds like we might be able to take over the governance mechanism allowing us to approve a proposal that would drain the pool.But first, let's look at the test cases again to figure out the winning conditions.\nThe first contract that is deployed, sounds like a DVT token with snapshot capabilities. It immediately mints 2 million tokens of initial supply of which 1.5 million are transferred into the SelfiePool. And as expected it appears that the Damn Valuable Token, which is available for flash loaning through SelfiePool, is also used as the governance token.\nThe challenge's success conditions are quite simply that all DVT have been moved from the pool and into the attacker EOA account.A quick look into DamnValuableTokenSnapshot.sol shows that the assumption of it being a simple ERC20Snapshot token was correct.Next, looking at SelfiePool.sol we see the same flash loaning pattern we've already seen in previous challenges but this time there's also a governance function:\nSo it appears that there's already a quite (for our purposes) helpful fund draining function available that we can make use of once we've taken over control of the governance contract.SimpleGovernance.sol has a bunch of new code that takes a bit to wrap your head around but the essential bits are the following:\nIn order to successfully queue an action, we have to own more than half of the total supply (1 million DVT + 1wei). Since there's nothing checking that we didn't borrow them using a flash loan this can be quite easily bypassed.We can't immediately execute the action though since there's a delay of 2 days we have to wait first. But all we have to do here is fast forward the time by 2 days since there's nothing ensuring that we still hold those governance tokens during the delay.For calling queueAction() we know that the receiver we want to call is the SimplePool contract but we have to build the calldata that is passed to it, which is the 4-byte function signature of drainAllFunds(address) + the function parameters: the receiver that the funds should be drained to, the attacker account's address. We'll be returned an actionId which we can then later pass into executeAction() once the delay time has passed.","exploit#Exploit":"Now adjust the selfie.challenge.js file to deploy the exploit contract and execute its function with sufficient delay for the queued action to be executed:\nAfter initially creating the exploit I ran into the following error:\nThis was because I assumed that the governance contract would automatically create the snapshot of governance tokens before calling _hasEnoughVotes(), which uses the latest snapshot to determine governance token balances.The solution was to simply call IDVTSnapshot(token).snapshot();{:solidity} to create that snapshot before calling queueAction() where it'll be used in.","conclusion#Conclusion":"Flash loaning governance tokens in order to manipulate a DAO has happened in the real world several times and there are now various established ways to prevent this from happening. But before building your governance system and figuring out all these intricacies on your own, instead consider using one of the governance contracts that are publicly available and well tested like OpenZeppelin's Governor which even has an interactive contract creation wizard. More information in regards to securing governance protocols can be found in the Strategies for Secure Governance with Smart Contracts workshop."}},"/posts/2022/2/22/damn-vulnerable-defi-v2-7-compromised":{"title":"Damn Vulnerable DeFi V2 - #7 Compromised","data":{"":"February 22, 2022 by patrickd\nThis is part 4 of the write-up series on Damn Vulnerable DeFi V2. Please consider attempting to solve it on your own first since it's a lot less fun after being spoiled!\nChallenge #7 - CompromisedWhile poking around a web service of one of the most popular DeFi projects in the space, you get a somewhat strange response from their server. This is a snippet:\nA related on-chain exchange is selling (absurdly overpriced) collectibles called \"DVNFT\", now at 999 ETH eachThis price is fetched from an on-chain oracle, and is based on three trusted reporters: 0xA73209FB1a42495120166736362A1DfA9F95A105,0xe92401A4d3af5E446d93D11EEc806b1462b39D15 and 0x81A5D6E50C214044bE44cA0CB057fe119097850c.Starting with only 0.1 ETH in balance, you must steal all ETH available in the exchange.\nThis might be the most obscure challenge so far throwing some strings of hexdecimal characters at us right at the start. So what might they be? First thing to notice is that both start with 4d 48 and a quick google search for these two characters leads to a whole bunch of other Damn Vulnerable Defi write-ups ‚Äì so let's quickly abandon that lead to not get spoiled.Maybe decoding them to ascii will reveal something?\nWell it's certainly interesting that this yields readable strings, but they seem too long to be a private keys and too short to be signed transactions..","code-review#Code Review":"For now, let's move on to reviewing the testcases, the setup of the challenge scenario is quite a bit of code but it can basically be summarized as follows:\nEach of the above mentioned oracle \"trusted reporter\" addresses is given 2 ether.\nAttacker EOA account is given 0.1 ether.\nA TrustfulOracleInitializer contract is deployed with the \"trusted reporter\" addresses,  \"DVNFT\" as token symbol and the initial price if 999 ether as parameters. It appears that this contract also deploys TrustfulOracle during construction.\nFinally a Exchange contract is deployed with an initial balance of 9999 ether and it seems to deploy a DamnValuableNFT during construction.\nThe success conditions are that, as described above, the Exchange had all its ether stolen and moved to the attacker's account. Additionally, once everything is over, the attacker may not own any NFT and, quite interestingly, the price of the NFTs according to the oracle's getMedianPrice() function must not have changed from the initial price. I had initially expected that some kind of oracle price manipulation would be the goal, but maybe not?The TrustfulOracleInitializer contract doesn't seem to do much: It has a constructor in which TrustfulOracle is first deployed, then an setupInitialPrices() function is called (we should check whether it can be called again after initialization) and finally a NewTrustfulOracle event is emitted:\nThe TrustfulOracle contract appears a lot more complex. It extends OpenZepplin's AccessControlEnumerable adding an access roles system that allows enumeration of addresses assigned to each role.One role is INITIALIZER_ROLE which is temporarily assigned to the TrustfulOracleInitializer during construction, a requirement for calling setupInitialPrices(), and once initialized it's immediately revoked again. This logic does not appear to have any issues, so we can't make use of any re-initialization vulnerability.The second role is TRUSTED_SOURCE_ROLE which each of the oracle's \"trusted reporter\" addresses is assigned to. This role is required for calling the postPrice() function that allows updating a symbol's price.But that's just about it really, most of the other functions are view or internal and not doing much. We can come back to them once we find out which of these are actually used by the Exchange we're supposed to exploit.The Exchange contract is quite simple and can be reduced to the following essential parts to understand:\nIn simple terms, this contract allows us to buy and sell DVNFT tokens for whatever the oracle claims the current median price is. Since we're supposed to drain all ether from this contract, we'll likely need to use sellOne() as part of the exploit.For completeness, let's also look at the DamnValuableNFT contract which only appears to have one significant function:","decoding-leaked-secrets#Decoding leaked secrets":"So far I've not been able to identify any critical issue in the smart contract implementation itself. Guessing from the challenge's title \"Compromised\" I still think the key to solving it lies behind the 2 strings we were given at the beginning. They probably allow us to somehow compromise the oracle's trusted reporters and post incorrect prices, which would also explain why each of those accounts are given 2 ether at initialization of the scenario. On the other hand I wonder why the success conditions require the price to be the same as the initialization price; why would the challenge require us to reset the price after the exploit has happened?At this point I kept looking at my cryptography notes, tools and scripts for private key recovery, thinking that I might be able to plug those hex values into some of them, when I read \"base64\" somewhere and then remembered that it was weird that those strings are perfectly fine when encoded to ascii. And yes, in hindsight, it should've been obvious even without the typical base64 = padding character that both of these strings can be decoded yielding two hexdecimal numbers:\nThese sure look a lot like 32 byte long private keys! Pasting them into eth-toolbox.com we can see that these are indeed private keys for the trusted reporter addresses 0xe92401a4d3af5e446d93d11eec806b1462b39d15 and 0x81a5d6e50c214044be44ca0cb057fe119097850c!","exploit#Exploit":"Two out of three price reporters are now compromised and we can use them to call postPrice() to manipulate the NFT price for the exchange's selling and buying functions.The median of three prices is always the one in the middle of a sorted array. So initially it's 999 from [999, 999, 999]. We can report 0 as a price twice in order to end up with a median price of 0 ([0, 0, 999]). After we bought an NFT for free, we can raise the price to 9999 in order to sell it again for all of the exchange's funds ([999, 9999, 9999]).\nWhile writing the exploit I noticed that we can't actually exploit it with 0 as a price since buyOne() has a check that ensures the sent value must be bigger than 0. So instead we'll set 1 wei as a price, which is just about the same as nothing as well.","conclusion#Conclusion":"Well, the obscurity of having to decode private keys from a \"data leak\" aside, this challenge was actually quite easy once you figure out the trick. Consider watching OpenZeppelin's workshop on The Dangers of Price Oracles in Smart Contracts to learn more about secure design and integration of oracles.---"}},"/posts/2022/2/23/damn-vulnerable-defi-v2-8-puppet":{"title":"Damn Vulnerable DeFi V2 - #8 Puppet","data":{"":"February 23, 2022 by patrickd\nThis is part 5 of the write-up series on Damn Vulnerable DeFi V2. Please consider attempting to solve it on your own first since it's a lot less fun after being spoiled!\nChallenge #8 - PuppetThere's a huge lending pool borrowing Damn Valuable Tokens (DVTs), where you first need to deposit twice the borrow amount in ETH as collateral. The pool currently has 100000 DVTs in liquidity.There's a DVT market opened in an Uniswap v1 exchange, currently with 10 ETH and 10 DVT in liquidity.Starting with 25 ETH and 1000 DVTs in balance, you must steal all tokens from the lending pool.\nThis sounds like a classic case of AMM price manipulation allowing us to take an undercollateralized loan...","code-review#Code Review":"First, looking at the testcases in puppet.challenge.js we can see that this challenge uses an already compiled build of Uniswap V1 (build-uniswap-v1 folder). Since I doubt that this challenge wants us to do any reverse engineering, we can probably safely assume that this is the original Uniswap V1 build without changes and we can use the official documentation to interact with it (which was linked in the challenge text above).The scenario setup code is quite long but can be summarized like this:\nSend 25 ether to attacker EOA\nDeploy DVT token contract\nDeploy UniswapExchange contract as a factory template and the UniswapFactory itself\nUse UniswapFactory to create an exchange for the DVT token\nDeploy the vulnerable PuppetPool lending contract for DVT and tell it to use the exchange as price oracle\nProvide 10 DVT and 10 ETH as liquidity to the Uniswap exchange\nGive 1000 DVT to attacker and 100000 to the PuppetPool\nThe success condition is simple: The PuppetPool no longer has any DVT because the attacker account now owns all of them.Aside from the Uniswap contracts and the DVT token, which we should already be quite comfortable with at this point, there's only one contract to look at this time: PuppetPool.sol which does exactly what it was described to do, allowing to borrow DVT in exchange for depositing two times of their value in ether as collateral. Functions for paying back the loan or interest rates were apparently omitted for simplicity.The most interesting part of PuppetPool is the \"oracle\" function though:\nBy changing the balances of ether and DVT in the uniswap exchange pair we can manipulate the price and therefore the amount of collateral required for borrowing tokens from the PuppetPool. We want all of the DVT for as little ether collateral as possible, and for that we first have to cause a price dump. To do that we need to decrease the uniswap pair's ether balance and increase the DVT token balance as much as possible.","exploit#Exploit":"The exploit first swaps all of the attacker's DVT for ETH causing a price imbalance and then it takes an undercollateralized loan that we don't intend to ever pay back since we got it so \"cheap\"!While writing the exploit I noticed that the success condition requires us to have MORE tokens than the pool did, so just stealing all of them isn't enough, we actually have to swap some back (or swap less for ether to begin with).\nThe exploit allows us to borrow 100000 DVT for a collateral of only around 20 ether instead of 200000 ether!For simplicity, I didn't bother writing an exploit contract here and instead did it in multiple transactions via ethers. In reality this wouldn't be very practical since arbitrage bots would likely pick up on our price manipulation and balance it out for profit before we'd be able to exploit it.I also didn't find the official Uniswap V1 documentation to be very useful to get a simple swap working and had to resort to reading the source code in the end.","conclusion#Conclusion":"This challenge shows quite well that you should avoid trusting on-chain AMM's as price oracles since they can be manipulated easily by wales and flash loans. Here I'd again recommend watching OpenZeppelin's workshop on The Dangers of Price Oracles in Smart Contracts to learn more about securely integrating oracles."}},"/posts/2022/2/24/learning-ethereum-virtual-machine-opcodes-with-evm-puzzles":{"title":"Learning Ethereum Virtual Machine Opcodes With EVM Puzzles","data":{"":"February 24, 2022 by patrickd\nFranco Victorio, one of the devs from Hardhat, created a collection of \"EVM Puzzles\" that we'll be solving here. Consider attempting them yourself first before getting spoiled!","puzzle-1#Puzzle #1":"So first of all, how do these puzzles work? They all consist of the puzzleCode, which is a smart contract's binary code, that was annotated with human-readable opcode names as comments. These contracts are deployed and then we can send data (calldata) and value (wei) to them as part of a transaction. The goal of each puzzle is to create a transaction (by setting data and value) that will succeed (and doesn't revert).\nThe evm.codes website has a handy table of all existing opcodes and what they do. Of these opcodes we need to prevent the instruction pointer to ever reach a REVERT or INVALID. Instead, we need it to either reach the end of the code or reach a STOP operation.When our solution is sent as a transaction to the contract's address, the EVM will always begin executing it with the instruction pointer on the first opcode. In the first puzzle that would be the CALLDATASIZE opcode. This pushes the length of our solution's data field onto the stack.The JUMP instruction tells the EVM to have the instruction pointer \"jump\" to another location in the bytecode. It gets this location by popping off the first value from the stack. It's important to note that only offsets where the JUMPDEST opcode is located are valid jumping targets.So knowing that we can now see that we have to influence the value that CALLDATA pushes onto the stack to ensure that JUMP makes the program continue at JUMPDEST reaching the valid end of bytecode. In this case, we can determine the offset of JUMPDEST simply by counting the number instructions, it's at 8 (start counting at 0) and that's also the number of bytes we have to send as data.To check this solution, edit it like this:\nAnd then execute the script:","puzzle-2#Puzzle #2":"This puzzle introduces the CODESIZE opcode which simply puts the size of the current contract's bytecode on top of the stack. Once deployed this value is static, and in this case we can once again simply count the number of instructions which is 9, and once again the JUMPDEST location is at 8.The SUB instruction subtracts the second value, from the stack, off the first one. In this case the stack would look like [CODESIZE, CALLDATASIZE] before the subtraction and [CODESIZE - CALLDATASIZE] after, because both values were popped off and the result was pushed on.This means we have to solve the following: 9 - CALLDATASIZE = 8 - and to do that we simply send a single byte as data. This will leave the value 8 on top of the stack and just like last time the JUMP can do its job.","puzzle-3#Puzzle #3":"The new PUSH1 instruction simply pushes a single byte on top of the stack, the byte that follows immediately after the instruction in the bytecode: 00. This zero value is then consumed by CALLDATALOAD as an offset of where to start reading the call data. It reads 32 bytes from our transaction data and pushes it onto the stack. If it finds less than 32 bytes to copy it will add zeros as padding to reach that length.So whatever we send as calldata here will get pushed onto the stack and then consumed by JUMP. This means we simply have to send the correct offset for the jump destination as data, which is 10 in this case or 0x0a in hexadecimal. But since this would become  after padding, which is a really large number, we have to make sure to send the full 32 byte word with 0x0a being the least significant byte.","puzzle-4#Puzzle #4":"The XOR opcode is similar to SUB, it takes two inputs from the stack, applies the exclusive-or operation on them, finally pushing the result back.What's effectively happening here is the following: jumpTo(calldata ^ codesize). And the location we want to jump to is 12 (0x0c) while the codesize is 13 (0x0d). This means we have to solve calldata ^ 0x0d = 0x0c which we can by doing 0x0d ^ 0x0c = calldata = 0x01.Sending a 32 byte word with a value of 1, will be XORed with 13, which results in 12, pointing to the correct JUMPDEST:","puzzle-5#Puzzle #5":"This time the puzzle uses the value, which is the amount of wei, sent with the transaction, instead of the transaction data itself. CALLVALUE simply pushes this amount of wei onto the stack. After that a PUSH2 operation pushes the two bytes following it onto the stack: 0004 or simply a 4.You have probably already guessed that ADD stands for addition. In pseudocode this puzzle can be rewritten as jumpTo(value + 4). The JUMPDEST is at offset 10, which means we need to send 6 wei to make it jump there.","puzzle-6#Puzzle #6":"A whole bunch of new opcodes are introduced in this puzzle. Starting with the easiest one, MUL which is the opcode for multiplication and works just like SUB, ADD and XOR did. EQ is also quite similar, but instead of calculating with the first two stack items, it compares them and pushes a 1 if they're equal and otherwise a 0 back onto the stack.Then there's DUP1 which simply duplicates the first stack item. For example if your stack looks like [1, 2, 3] it will be [1, 1, 2, 3] after DUP1 was executed.Finally there's JUMPI which is a conditional JUMP. Additionally to the destination offset, it consumes another stack item and checks whether it equals 0. If it's anything other than 0 it will jump to the location specified by the first stack item.The puzzle can be rewritten like this:\nThis time the correct jump destination is already hardcoded. Instead, we have to specify a value of wei to send which ensures that JUMPI does jump at all. And as you can already tell from the pseudocode, that value must be 3.\nTo easier understand what's happening, here's how the stack changes after each instruction:","puzzle-7#Puzzle #7":"This puzzle is quite a bit more complex but it only introduces one new opcode: LT or lower-than, is similar to EQ in that it compares the first two items on stack and pushes 1 or 0 as a result:\n[1, 2] - 1 < 2? - Yes (1)\n[2, 1] - 2 < 1? - No (0)\nThe first part of the puzzle (until the first REVERTs) can be rewritten like this:\nThe second part like this:\nSince the length of our calldata must be greater than 3, and when multiplied with the value it must result in 8, it must be that we have to send 4 bytes and 2 wei.\nLet's once again see how this transaction is actually being executed within the EVM:","puzzle-8#Puzzle #8":"It doesn't take much to guess that GT indeed stands for greater-than and works just like LT. And yet another comparing opcode is introduced with ISZERO, but this one only consumes one item of the stack's top and pushes a 1 or a 0 depending whether that value was a zero or not.Next is the MOD operator, which allows calculation of the modulo and works just like the other arithmetic instructions.The use of SWAP1 in this puzzle appears to have been added for creating confusion by switching the first two value of the stack around (eg. [1, 2] becomes [2, 1]).With that in mind, puzzle can again be broken down into two parts:\nThe size of the bytecode is a static value of 26, and the first condition is that the sent wei value is smaller than that. It's also required that CALLDATASIZE % 3 = 0, which means we can send any length of data as long as it's a multiple of 3 - let's just send 3 bytes!The final JUMPDEST is at offset 0x19 and depends on the wei value sent: 0x0a + CALLVALUE = 0x19 (or simply 10 + 15 = 25). Sending 15 wei is also smaller than 26 and therefore passing the first condition as well.\nOnce more, step by step:\nThat's it already! Although these puzzles were rather simple they were still good fun and I'm looking forward to solving more of them in the future."}},"/posts/2022/2/28/damn-vulnerable-defi-v2-9-puppet-v2":{"title":"Damn Vulnerable DeFi V2 - #9 Puppet V2","data":{"":"February 28, 2022 by patrickd\nThis is part 6 of the write-up¬†series on Damn Vulnerable DeFi V2. Please consider attempting to solve it on your own first since it's a lot less fun after being spoiled!\nChallenge #9 - Puppet v2The developers of the last lending pool are saying that they've learned the lesson. And just released a new version!Now they're using a Uniswap v2 exchange as a price oracle, along with the recommended utility libraries. That should be enough.You start with 20 ETH and 10000 DVT tokens in balance. The new lending pool has a million DVT tokens in balance. You know what to do ;)","code-review#Code Review":"As usual, we start by looking at the testcases. In puppet-v2.challenge.js we find a quite complex looking scenario setup script, essentially doing the following things:\nGives the attacker account 20 ether\nDeploy DamnValuableToken and Wrapped Ether token contract (WETH9)\nDeploys the UniswapRouter and uses it to create a DVT-WETH pair with 100 DVT and 10 WETH of liquidity\nDeploys PuppetV2Pool and provides it with 1.000.000 DVT\nGives the attacker 10.000 DVT\nThere are also things happening like the initialization of a UniswapFactory contract that gets passed to the Router contract. Or the fact that addresses of WETH, DVT, the exchange pair, and the factory contracts, all get passed as construction arguments into the PuppetV2Pool - but for now we can just skip over those details since they're unlikely to play much of a role in solving the challenge.The success conditions are simple: The pool must have 0 DVT while the attacker must have at least as much DVT as the pool initially had - meaning the attacker must steal all of the pool's token.Unlike in the previous challenge, this time the Uniswap contracts are included via the NPM package manager, so there's little doubt that they're in their original state and unlikely to be vulnerable. The WETH9 contract, used for wrapping ether into the WETH ERC20 token, is a well established contract and was copied without changes as well.Therefore, the only thing we have to look at right now is PuppetV2Pool.sol, which looks quite similar to PuppetPool.sol from the previous challenge. The first significant change I noticed was the fact that now 3 (instead of 2) times as much ether as the borrowing value needs to be deposited as collateral.Furthermore, as explained by the challenge description, the function determining the token price is now indeed using the official utility library of Uniswap:\nLooking at this naively would make you think along the lines of: This is the recommended way of determining the price, so this should be safe to use!But if you actually look at the library you'll realize that the \"magic\" is not much different to how price calculation worked in V1:\nNote that, since this is an internal library function it will be inlined into the PuppetV2Pool contract during compilation.","exploit#Exploit":"Just like in the previous challenge, we again simply have to manipulate the amount of reserves: By selling as many tokens as possible we can increase the reserve amount of DVT while decreasing the amount of ether. This will effectively cause a price drop that will allow us to again take an undercollateralized loan that we don't intend to ever pay back.Now we just have to rewrite the previous exploit to use Uniswap V2 instead of V1 for making the swap, and to use WETH instead of ether:\nAt least this time the Uniswap documentation was quite informative and it didn't take long to implement the changes.","conclusion#Conclusion":"There's not much new to say since this challenge was very similar to the previous one. But if there's one key take away, it's that you can't blindly trust the \"recommended way\" of doing things. You should still try to understand what is actually happening under the hood, what drawbacks it has and whether you're actually using it as intended."}},"/posts/2022/2/9/secureum-bootcamp-february-race-4":{"title":"Secureum Bootcamp Epoch‚àû - February RACE #4","data":{"":"Bootcamp Epoch 0 has finished, but Epoch Infinity has just begun!This is the a write-up of first quiz from Secureum Epoch Infinity. Note that it was given the name RACE 4 because several quizzes of the previous Epoch 0 were selected to represent the first RACEs (slot-4 -> RACE-0; slot-5 -> RACE-1; slot-7 -> RACE-2; slot-8 -> RACE-3).\nThis quiz had a strict time limit of 16 minutes for 8 questions, no pause. Choose all and only correct answers.Syntax highlighting was omitted since the original quiz did not have any either.\nFebruary 9, 2022 by patrickd","code#Code":"All 8 questions in this quiz are based on the InSecureum contract. This is the same contract you will see for all the 8 questions in this quiz. InSecureum is adapted from a widely used ERC20 contract.","question-1-of-8#Question 1 of 8":"InSecureum implements\n A. Atypical decimals value  \n B. Non-standard decreaseAllowance and increaseAllowance  \n C. Non-standard transfer  \n D. None of the above  \nCorrect is A, B.The decimals value follows the standard but it typically returns 18 (8 is atypical), imitating the relationship between Ether and Wei. The decreaseAllowance and increaseAllowance functions were introduced in the OpenZeppelin ERC20 implementation to mitigate frontrunning issues of the standard approve, but they are not part of the ERC20 standard. The transfer function is part of the standard though.","question-2-of-8#Question 2 of 8":"In InSecureum\n A. decimals() can have pure state mutability instead of view  \n B. _burn() can have external visibility instead of internal  \n C. _mint() should have internal visibility instead of external  \n D. None of the above  \nCorrect is A, C.Since decimals() returns a constant hardcoded value without accessing storage other non-calldata information it can indeed be declared as pure. Generally, functions prefixed with underscores should be internal or should not have the prefix. Making _burn() external would currently allow anyone to burn anyone else's balance. And the fact that _mint() is currently external allows anyone to mint as many InSecureum tokens as they wish.","question-3-of-8#Question 3 of 8":"InSecureum transferFrom()\n A. Is susceptible to an integer underflow  \n B. Has an incorrect allowance check  \n C. Has an optimization indicative of unlimited approvals  \n D. None of the above  \nCorrect is A, B, C.The subtraction within the unchecked block effectively allows anyone to steal anyone else's full token balance since subtracting from an allowance of 0 will cause an integer underflow and the allowance value will wrap (0 - 1 == type(uint256).max){:solidity}. The fact that the function won't revert when subtracting from the allowance due to the unchecked block, can by itself be seen as an incorrect allowance check. The other check, skipping allowance subtraction when an \"infinite approval\" was given by setting the allowance to the maximum value of uint256, appears to be correct. Since the special handling for unlimited approvals prevents unnecessary storage updates, it is indicative of an optimization.","question-4-of-8#Question 4 of 8":"In InSecureum\n A. increaseAllowance is susceptible to an integer overflow  \n B. decreaseAllowance is susceptible to an integer overflow  \n C. decreaseAllowance does not allow reducing allowance to zero  \n D. decreaseAllowance can be optimised with unchecked{}  \nCorrect is C, D.Neither function make use of the unchecked block which would allow integer overflows to happen in this version of solidity. The decreaseAllowance function does indeed not allow reducing the allowance to zero since the requirement enforces that the subtractedValue must always be smaller than currentAllowance. It would be better to use >= here to allow allowance reductions to zero. That requirement does make solidity's own integer underflow check for currentAllowance - subtractedValue redundant, so it could indeed be optimised with unchecked.","question-5-of-8#Question 5 of 8":"InSecureum _transfer()\n A. Is missing a zero-address validation  \n B. Is susceptible to an integer overflow  \n C. Is susceptible to an integer underflow  \n D. None of the above  \nCorrect is D.All of the addresses _transfer() uses are checked to make sure they're not zero-addresses. The _transfer() function is making use of an unchecked{} block, which could allow integer wrapping in this Solidity version. But it cannot be susceptible to overflows since there's no addition happening within the block. It contains a subtraction that could potentially underflow, but for that to happen amount would need to be larger than senderBalance which is not possible due to the preceding require() statement.","question-6-of-8#Question 6 of 8":"InSecureum _mint()\n A. Is missing a zero-address validation  \n B. Has an incorrect event emission  \n C. Has an incorrect update of account balance  \n D. None of the above  \nCorrect is A, C.The _mint function is currently not ensuring that the receiving address is non-zero. The event emission appears to be correctly following IERC20: event Transfer(address indexed from, address indexed to, uint256 value);{:solidity}. This mint implementation overwrites the accounts current balance instead of adding to it.","question-7-of-8#Question 7 of 8":"InSecureum _burn()\n A. Is missing a zero-address validation  \n B. Has an incorrect event emission  \n C. Has an incorrect update of account balance  \n D. None of the above  \nCorrect is B.It correctly applies zero-address validation on the account to burn from. The event permission is incorrect, from and to need to be switched around to follow the IERC20 interface: event Transfer(address indexed from, address indexed to, uint256 value);{:solidity}. The balance update is correct and although an unchecked block is used, no underflow can happen thanks to the requirement before.","question-8-of-8#Question 8 of 8":"InSecureum _approve()\n A. Is missing a zero-address validation  \n B. Has incorrect error messages  \n C. Has an incorrect update of allowance  \n D. None of the above  \nCorrect is B, C.Although no zero-address validation is missing the error messages have been confused with each other. The update of allowances is currently incorrect since it only adds the amount to the current allowance instead of setting it to the amount overwriting the old value."}},"/posts/2022/3/2/damn-vulnerable-defi-v2-10-free-rider":{"title":"Damn Vulnerable DeFi V2 - #10 Free Rider","data":{"":"March 2, 2022 by patrickd\nThis is part 7 of the write-up series on Damn Vulnerable DeFi V2. Please consider attempting to solve it on your own first since it's a lot less fun after being spoiled!\nChallenge #10 - Free riderA new marketplace of Damn Valuable NFTs has been released! There's been an initial mint of 6 NFTs, which are available for sale in the marketplace. Each one at 15 ETH.A buyer has shared with you a secret alpha: the marketplace is vulnerable and all tokens can be taken. Yet the buyer doesn't know how to do it. So it's offering a payout of 45 ETH for whoever is willing to take the NFTs out and send them their way.You want to build some rep with this buyer, so you've agreed with the plan.Sadly you only have 0.5 ETH in balance. If only there was a place where you could get free ETH, at least for an instant.\nSounds like something with flash loans.. But what about this ominous buyer? Is that a smart contract separate from the marketplace?...","code-review#Code Review":"First thing to notice, looking at the scenario setup in free-rider.challenge.js, is that Uniswap V2 is used once more - probably we'll use it to take flash loans? Essentially the following happens here:\nThe attacker gets 0.5 ether as promised\nA Uniswap exchange pair is deployed and provided with 9000 wrapped ether (using WETH9) and 15000 DamnValuableToken as liquidity\nDeploys the FreeRiderNFTMarketplace contract and gives it 90 ether - but why is it given any value at all? Can we steal it somehow?\nIt appears that the FreeRiderNFTMarketplace deploys the DamnValuableNFT contract during construction and created 6 NFTs, all with the deployer account as owner\nAll of the tokens are then approved for the marketplace to handle and offered for a price of 15 ether each\nAnd finally, the buyer mystery is solved: A FreeRiderBuyer contract is deployed with the 45 ether - which the attacker can probably obtain by sending the NFTs to this contract?\nThe challenge's success conditions, found at the bottom of the same file, already clarify some of these questions:\nThe 45 ether of the FreeRiderBuyer contract should have moved to the attacker account\nThe buyer should be able to transfer all of the tokens to themselves\nAnd most interestingly, the marketplace shouldn't only lose all of its NFTs but also some of its ether balance\nI'm really curious what the ether balance of the NFT marketplace is all about, so let's look at FreeRiderNFTMarketplace.sol - and I immediately have all alarm bells ringing:\nAt least that's what should happen whenever you see a payable function, a loop, and the use of msg.value within it.This pattern effectively means that we can buy multiple NFTs for the same sent value and the payment of the seller will continue functioning, despite the fact that we sent too little ether, because it will instead use the marketplace's own balance of 90 ether.We only have 0.5 ether and we would need at least 15 ether to buy all of the NFTs using this vulnerability. We could make use of Uniswap V2's Flash Swap feature to obtain the initial funds necessary for the exploit but that would mean we'd have to use 15 ether of the reward we get from the buyer to pay back the loan. This would not allow us to pass this challenge's success conditions!But what if we're buyer and seller at the same time? This market place allows us to put the NFTs we bought from it back on sale for a price of our choosing via the offerMany() function.\nWe could flash borrow 30 ether to legitimately buy two of the NFTs\nThen put them back on sale for 90 ether each\nUsing the msg.value-reuse vulnerability we could now buy both for only 90 flash-borrowed ether (instead of 180), draining the marketplace of its entire balance\nThis would give our attacker account 180 ether of which we have to pay back 120 for the flash loans\nThe net \"profit\" of 60 ether is exactly the amount we need to buy the 4 remaining NFTs\nNow the only question is how to give our spoils to the buyer in exchange for the reward.So let's look at FreeRiderBuyer.sol to figure that out:\nEssentially we just have to send all of the NFTs to the buyer's contract, and once the last one was received we'll immediately get the job payout via the onERC721Received() hook. The buyer can then simply take them all from the contract, since it has given him approval to do so in the constructor.","exploit#Exploit":"Note that things like constants have been hardcoded and security measures have been omitted to ease readability!\nFinally we have to add the following to the test cases in free-rider.challenge.js in order to deploy and execute the exploit:","conclusion#Conclusion":"I really liked this one, it was complex enough to be a little challenging but absolutely solvable without much guesswork. It sure was my favorite challenge so far!Re-use of the same msg.value within a contract, commonly either through a loop or some kind of batching/multicall function, is a very important pitfall to be aware of. Keep your eyes open for this pattern since it has already put millions of USD at risk in the past!Examples of this vulnerability:\nSushiSwap Miso Vuln (Aug 2021)\nOpyn Hack (Aug 2020)"}},"/posts/2022/4/28/fuzzing-for-memory-bugs-in-solidity":{"title":"Fuzzing For Memory Bugs In Solidity","data":{"":"April 28, 2022 by patrickd\nWhen reviewing Solidity code that makes use of assembly, one of the most common errors is that the memory is incorrectly read from or written to. This article explains how fuzzing can be used to look for these types of issues that aren't simple to notice due to the difficult readability of Yul.To discuss this, we'll look at the same library that I've used as an example for the Solidity Fuzzing Boilerplate template: Gon√ßalo S√°'s Solidity Bytes Arrays Utils, more specifically its slice(bytes,start,length) function. You're probably already familiar with slicing functions: In this implementation, it takes a byte array and returns a part of it as a new byte array, with the specified length, starting at the specified offset. In languages like JavaScript it's a built-in feature, while in Solidity it's (currently) only available for byte arrays located in the msg's calldata.So to slice byte arrays located in memory, we can, and should, make use of established utility libraries instead of rolling our own. The one we'll be using here is optimized to minimize gas usage and for that it makes use of assembly. Most of the function is, in fact, written in assembly.When reviewing it, the following questions should arise:\nWhen it accesses the memory belonging to the passed byte array, does it only read as much as it should?\nHow does it deal with pollution of bytes that are within the same 32-bytes \"slot\" that the byte array lives in but aren't part of the array?\nAre there start or length parameter values that will make it read memory not belonging to the byte array?\nDoes it correctly handle empty byte arrays that only have a length but no value following it?\nWhen initializing the new byte array for the return value, does it correctly increase the free memory pointer, so it won't be overwritten by new memory values later?\nIs there a case where it writes to an incorrect location in memory?\nDoes it write more bytes to the last memory \"slot\" of the slice than it should, potentially polluting it?\nAlthough the EVM allows accessing bytes within memory individually, Solidity manages memory in chunks of 32-bytes, which is why they're called \"slots\" throughout this article.","basic-differential-fuzzing-testcase#Basic Differential Fuzzing Testcase":"To start, we'll write a simple differential fuzzing test, meaning we'll give the fuzzer's input to two implementations and compare the outputs, if they differ, we've found an issue. Since the fuzzers will call our testcase as an external function, we can access the input-bytearray via the calldata data location allowing us to test the library against Solidity's own slicing function:\nThis basic testcase can now be extended to check for memory issues too, but first, a quick reminder of Solidity's memory layout:\nIn the beginning, when a contract's bytecode is invoked, Solidity reserves the first 128 bytes for internal purposes. Of these, you're usually only interested in the \"Free Memory Pointer\" when writing assembly embedded within Solidity. It should always contain the current address of the first free byte - at the start always 0x80, which is the first available byte after the reserved memory space.Before the assertion is checked, memory should look similar to this for a call of slice(hex'f00b4334', 1, 2):\nFirst, note that byte arrays are always stored in memory as two parts: The first 32 bytes are the array's length, and based on that the following slot(s) store the actual value (which is left-aligned). Related to this, it's important to note that an empty byte array only has a length, there's no value following after it. You might also be wondering why the input is stored after outputA. Remember that the input isn't initially stored in memory but taken from calldata. Only when it is passed to the slice function (where it's declared as bytes memory _bytes), Solidity will copy it to memory.With that, the test should pass as expected, since both Solidity and the library returned 0x0b43 as results.","extending-with-memory-checks#Extending with Memory Checks":"A simple way to now extend the testcase to also check for memory access issues is by adding a bunch of junk:\nBy adding junk around the input in memory, we can check whether the slice function tried reading the input from the wrong memory address. If there's a case where this happens, the junk will likely influence the output value of the library, which will be detected by the original assertion. Whether the library overwrote the passed input byte array (or any of the junk around it), can also be easily covered with a few more assertions.Lastly, we can check whether there's a case when the library does not properly increase the Free Memory Pointer for the return value. This can be done by simply setting a new byte array after we got the output - if the pointer wasn't increased, the library's output should be overwritten by junk.Although this won't make cases fail where the slice function makes unnecessary reads or writes that don't influence the output (but waste gas), it should at least detect the most critical errors made when assembly is used.Here's how the same call should look in memory with these changes:\nYou can notice that because of the byte array lengths, the input isn't actually immediately surrounded by junk as you might have expected. But at least when checking for issues that overwrote the memory that shouldn't be much of a concern, since a changed length will also mean a changed keccak256-hash of that variable. But if you want the junk to be as close as possible you can, instead of putting it into bytes, put it into a struct that has a bytes32 property (which has no length prefix in memory).","conclusion#Conclusion":"There's still more that we can extend the testcase with to add even deeper memory checks, but many of these will require assembly as well and that'd be a bit much for a single article. As an exercise in that regard, you can check out the full example testcase that adds a check for memory pollution of the library's output."}},"/posts/2022/3/8/secureum-bootcamp-epoch-march-race-5":{"title":"Secureum Bootcamp Epoch‚àû - February RACE #5","data":{"":"This is a write-up of the Secureum Bootcamp Race 5 Quiz of Epoch Infinity with solutions.\nThis quiz had a strict time limit of 16 minutes for 8 questions, no pause. Choose all and only correct answers.Syntax highlighting was omitted since the original quiz did not have any either.\nMarch 8, 2022 by patrickd","code#Code":"All 8 questions in this quiz are based on the InSecureum contract. This is the same contract you will see for all the 8 questions in this quiz. InSecureum is adapted from a well-known contract.","question-1-of-8#Question 1 of 8":"InSecureum balanceOf()\n A. May be optimised by caching state variable in local variable\n B. May be optimised by changing state mutability from view to pure\n C. May be optimised by changing its visibility to external\n D. None of the above  \nCorrect is D.Since the _balances state variable is only accessed once and immediately returned, caching doesn't make sense.State mutability can't be changed to pure since the function accesses a state variable, that requires at least view.It can't be changed to external because it is currently being called internally by the balanceOfBatch() function.","question-2-of-8#Question 2 of 8":"In InSecureum, array lengths mismatch check is missing in\n A. balanceOfBatch()\n B. _safeBatchTransferFrom()\n C. _mintBatch()\n D. _burnBatch()\nCorrect is A, B, C, D.The public function balanceOfBatch() receives a list of accounts and a list of ids, both of which items get passed on to balanceOf(accounts[i], ids[i]);{:solidity}. To ensure that neither array is accessed out-of-bounds, it should be checked whether both lists are of the same length.Neither the internal function _safeBatchTransferFrom() nor its public caller function safeBatchTransferFrom() check the length of passed ids and amounts. Therefore the check is missing.The internal functions _mintBatch() and _burnBatch() are currently never called, but a contract extending InSecureum might. It would make sense to check the lengths of passed ids and amounts in them, so that public functions calling them do not need to remember to do so.","question-3-of-8#Question 3 of 8":"The security concern(s) with InSecureum _safeTransferFrom() is/are\n A. Incorrect visibility  \n B. Susceptible to an integer underflow  \n C. Missing zero-address validation  \n D. None of the above  \nCorrect is A, B, C.It is prefixed with an underscore, which is usually an indication of an internal visibility, and it's also called by a similarly named public safeTransferFrom() function that applies more input validation before calling it. This validation ensures that the sender actually has approval for the transfer of funds, which would be bypassed by this function being public. It should instead be internal allowing an inheriting contract to internally call it.The new fromBalance is calculated within an unchecked{} block, bypassing integer underflow prevention measures of Solidity version 0.8.0^. Since the fromBalance isn't checked for whether there's a sufficient balance for a transfer, this effectively allows sending unlimited amounts to the specified recipient.Neither safeTransferFrom() nor _safeTransferFrom() are checking whether the to address is non-zero, making it possible to accidentally burn tokens.","question-4-of-8#Question 4 of 8":"The security concern(s) with InSecureum _safeBatchTransferFrom() is/are\n A. Missing array lengths mismatch check \n B. Susceptibility to an integer underflow  \n C. Incorrect balance update \n D. None of the above \nCorrect is A, C.The fact that the array lengths mismatch check is missing has already been determined in Question #2.There's no usage of an unchecked{} block, therefore an integer underflow cannot happen with this Solidity version.The new value of fromBalance is calculated but it's never actually updated in storage. This effectively allows sending the same tokens unlimited amount of times.","question-5-of-8#Question 5 of 8":"The security concern(s) with InSecureum _mintBatch() is/are\n A. Missing array lengths mismatch check \n B. Incorrect event emission  \n C. Allows burning of tokens  \n D. None of the above  \nCorrect is A, B, C.The fact that the array lengths mismatch check is missing has already been determined in Question #2.Comparing the emission of the TransferBatch event to other occurrences, it appears that ids and amounts have been accidentally swapped.The zero-address check incorrectly ensures that the sender is non-zero (which would never be possible anyway) instead of ensuring that the receiving account is non-zero. This effectively allows minting to the zero-address, burning all minted tokens immediately.","question-6-of-8#Question 6 of 8":"The security concern(s) with InSecureum _burn() is/are\n A. Missing zero-address validation  \n B. Susceptibility to an integer underflow  \n C. Incorrect balance update  \n D. None of the above  \nCorrect is D.The zero-address validation exists and is correctly checking the value of from.There's no usage of an unchecked{} block, therefore an integer underflow cannot happen with this Solidity version.The balance appears to be correctly updated after subtraction.","question-7-of-8#Question 7 of 8":"The security concern(s) with InSecureum _doSafeTransferAcceptanceCheck() is/are\n A. isContract check on incorrect address \n B. Incorrect check on return value \n C. Call to incorrect isContract implementation \n D. None of the above  \nCorrect is B, C.The isContract() function is correctly called on to, which is the receiving address that is potentially a contract that this function is supposed to check support of ERC1155, before tokens are sent to it, since they'd otherwise be stuck in a contract not supporting this standard.Comparing _doSafeTransferAcceptanceCheck() and _doSafeBatchTransferAcceptanceCheck() shows a clear discrepancy when checking the return value, with the batch function's implementation correctly checking support for the ERC1155 standard. This function is in fact currently doing the opposite, ensuring that tokens are only sent to contracts that do NOT support it.The isContract() function currently returns true if the passed address is in fact NOT a contract (has a code length of 0). It should instead return true only when the address has a code length larger than 0, showing that there's currently a contract residing at account.","question-8-of-8#Question 8 of 8":"The security concern(s) with InSecureum isContract() implementation is/are\n A. Incorrect visibility \n B. Incorrect operator in the comparison  \n C. Unnecessary because Ethereum only has Contract accounts  \n D. None of the above  \nCorrect is B.A visibility of internal allowing inheriting contracts to use it appears appropriate.The comparison should indeed be \"bigger-than-zero\" instead of \"equals-zero\", for the reasons explained for the previous question.Ethereum not only has Contract accounts but also EOA (Externally Owned Accounts), which do not have any contract code but an off-chain public-private keypair instead."}},"/posts/2022/5/16/secureum-bootcamp-epoch-may-race-6":{"title":"Secureum Bootcamp Epoch‚àû - February RACE #6","data":{"":"This is a write-up of the Secureum Bootcamp Race 6 Quiz of Epoch Infinity with solutions.\nThis quiz had a strict time limit of 16 minutes for 8 questions, no pause. Choose all and only correct answers.Syntax highlighting was omitted since the original quiz did not have any either.\nMay 17, 2022 by patrickd","code#Code":"All 8 questions in this RACE are based on the InSecureumLand contract. This is the same contract you will see for all the 8 questions in this RACE.InSecureumLand is adapted from a well-known contract. The questions are below the shown contract.","question-1-of-8#Question 1 of 8":"The security concern(s) with InSecureumLand is/are\n A. Single-step ownership change \n B. Incorrectly implemented KYC check using Merkle proofs \n C. Missing time-delayed change of critical parameters \n D. Accidentally sent Ether gets locked in contract \nCorrect is A, C.A. Ownership management is inherited from OpenZeppelin's Ownable abstract contract, which only allows for single-step ownership change. If the ownership is mistakenly changed to an incorrect address, it could be permanently lost.B. Contract appears to correctly make use of OpenZeppelin's MerkleProof library for KYC purposes.C. Considering attributes like operator a critical parameter, it can indeed be argued that a time-delay would improve the contract's security.D. Contract owner is be able to call the withdraw() function to extract any accidentally sent ether.","question-2-of-8#Question 2 of 8":"The security concern(s) with InSecureumLand setOperator() is/are\n A. Missing zero-address validation \n B. Missing event emission \n C. Incorrect modifier \n D. None of the above \nCorrect is A, B.A. There's indeed no check for zero-addresses, which could accidentally lead to no one being the operator. This would have little impact though, since the owner is able to correct the mistake by calling the function again.B. There's also no event emitted when the operator is changed. This makes monitoring the contract for critical changes difficult.C. Assuming that the intention is that only the owner should be able to update the operator, there seems to be no problem with the modifier that was chosen.","question-3-of-8#Question 3 of 8":"The security concern(s) with InSecureumLand mintLands() is/are\n A. Minting could exceed max supply \n B. Minting could exceed maxMintPerTx \n C. Minting could exceed maxMintPerAddress \n D. None of the above \nCorrect is A.A. While the function checks currentNumLandsMintedPublicSale for whether the maximum supply has been exceeded, it doesn't actually ever increase this variable after minting. So it'll be possible to continue minting beyond the MAX_PUBLIC_SALE_AMOUNT value.B. The maxMintPerTx value appears to be correctly checked against the numLands parameter.C. The maxMintPerAddress value appears to be correctly checked against the overall amount of tokens that'll have been minted by the sender.","question-4-of-8#Question 4 of 8":"Missing threshold check(s) on parameter(s) is/are a concern in\n A. mintLands \n B. startPublicSale \n C. contributorsClaimLand \n D. None of the above \nCorrect is B, C.The startPublicSale should have some sanity checks for passed parameters like _publicSaleStartPrice and _publicSaleEndingPrice, especially since these cannot be corrected once set. The contributorsClaimLand function doesn't ensure the amount parameter, of how many tokens should be claimed for the contributor, is actually lower or equal to the amount of tokens they should be able to claim according to contributors[msg.sender]. It also doesn't update this amount allowing the contributor to claim the same amount multiple times.","question-5-of-8#Question 5 of 8":"The security concern(s) with InSecureumLand contributors claim functions is/are\n A. Anyone can call startContributorsClaimPeriod \n B. Anyone can call stopContributorsClaimPeriod \n C. Anyone can call contributorsClaimLand \n D. None of the above \nCorrect is C.The first two functions can only be called by the operator. The contributorsClaimLand function appears to be only callable by contributors. But when looking at the onlyContributors modifier, callers are considered contributors even when they have not made any contribution (contributors[_contributor] >= 0). This error effectively allows anyone to call the contributorsClaimLand function.","question-6-of-8#Question 6 of 8":"The security concern(s) with InSecureumLand random number usage is/are\n A. It depends on miner-influenceable block.timestamp \n B. It depends on miner-influenceable blockhash \n C. It depends on deprecated Chainlink VRF v1 \n D. None of the above \nCorrect is C.It doesn't make use of miner-influenceable values for randomness. But it does indeed make use of a deprecated version of Chainlink's VRF. Projects should aim to make use of the most recent stable version of their dependencies before deployment.","question-7-of-8#Question 7 of 8":"The documentation/readability concern(s) with InSecureumLand is/are\n A. Stale comments \n B. Missing NatSpec \n C. Minimal inlined comments \n D. None of the above \nCorrect is B, C.There are no NatSpec comments at all, and the few inline comments that exist mostly just repeat what the code already states instead of explaining what is going on and what is the intention.","question-8-of-8#Question 8 of 8":"Potential gas optimization(s) (after appropriate security considerations) in InSecureumLand is/are\n A. Removing nonReentrant modifier if mint addresses are known to be EOA \n B. Using _mint instead of _safeMint if mint addresses are known to be EOA \n C. Using unchecked in for loop increments \n D. None of the above \nCorrect is A, B, C.A. By checking msg.sender == tx.origin it can be known that the mint address, which is the mint function's caller, is an EOA, an account with a keypair and no bytecode. With that, transfer hooks are certain to not be triggered which means that nonReentrant can safely be omitted in this case.B. If the mint address is known to be an EOA, _mint can be directly called, instead of _safeMint which first checks whether the receiver implements the hook function onERC721Received. An EOA has no bytecode, which means it's not possible that it implements this interface. A check like this also isn't necessary for EOAs since tokens cannot get stuck in them as they could in contracts.C. Most loops increment with ++i for which the solidity compiler will add overflow checks that cost additional gas. But the loop conditions (eg. i < alphaTokenIds.length) already ensures no overflow can happen. Therefore using an unchecked block around the increment can reduce gas cost by removing the unnecessary check. To implement this would require using a different loop though, since adding an unchecked block around the for loops primary expression would cause a compiler error."}},"/posts/2022/6/6/more-evm-puzzles-part-3":{"title":"More EVM Puzzles - Part 3","data":{"":"June 6, 2022 by patrickd\nThis continues the series on Dalton Sweeney's \"10 more EVM puzzles\" collection. If this is all new to you, you should probably check out Part 1 first.","puzzle-5#Puzzle #5":"This one starts easy: The calldata we send must be longer than 32 (0x20) bytes for the first JUMPI to succeed. Then all of the calldata is copied to memory at offset 0x0. Then CALLDATASIZE is subtracted from MSIZE and the result of this subtraction must be 3 for the final jump to succeed as well.evm.codes tells us that MSIZE returns \"the size of active memory in bytes\". But there's an important detail: \"The size is always a multiple of a word (32 bytes)\". So even if we only sent 33 bytes as calldata which were copied to memory, MSIZE would instead return 64 as value.This also means the solution is quite simple: Sending 61 bytes will yield a difference of 3 from the subtraction and is also of a greater size than 32 to make the first jump.\nKnowing this little fact on how memory expansion works in the EVM made solving this puzzle quite easy.","puzzle-6#Puzzle #6":"Seems like the \"gimmick\" on this one is integer overflows. Right at the beginning, a really large number is pushed onto the stack, so large in fact that just adding 16 will make it overflow and flip to 0. But for the jump condition to succeed it needs to be a 1 - so we need to send 17!","puzzle-7#Puzzle #7":"This one's got a loop! Right after pushing the initial amount of GAS onto the stack, we enter the first loop:\nIf we'd send a value of 0 it would end up looping until it runs out of gas, because the integer will underflow resulting in the largest possible number to count down from. But as long as we send any value that ensures we have enough gas left at the end of the loop, we should be fine. If we'd for example send a 1 as a value, the result of the subtraction would immediately yield a 0 and it would never loop to begin with. Sending a 2 would loop exactly once and then exit, and so on..\nImmediately after the loop it pushes the now still left gas onto the stack again. You can also see that the final value from the loop, which will always be 0, is still left on the stack even though it's never used again, which is why there are a bunch of confusing swaps happening. But all that's actually being done here is determining the gas that was consumed during the loop by subtracting the second GAS instruction's value from the first one. The result of this subtraction is then compared with 0xA6, or 166 in decimal.In summary: We have to send a transaction value that'll cause the loop to consume exactly 166 gas.\nThe instructions around the loop will consume 5gas while a normal loop execution will cost 43gas. The final loop skips a few instructions, pricing it at 32gas.So what we have to determine is the CALLVALUE we have to send for 5 + 43*(CALLVALUE-1) + 32 = 166, which is 4.\nIt should be noted though that gas prices for instructions aren't completely fixed. Their cost might change in the future which would likely break this puzzle."}},"/posts/2022/7/2/damn-vulnerable-defi-v2-13-junior-miners":{"title":"Damn Vulnerable DeFi V2 - #13 Junior Miners","data":{"":"July 2, 2022 by patrickd\nThis is the (final until a new challenge is released) part 10 of the write-up series on Damn Vulnerable DeFi V2.\nNote that this challenge involved a lot of guesswork and finding the solution wasn't the straight path like usual. I decided to leave it that way to show the thought process from start to finish.\nChallenge #13 - Junior minersSomebody has sent +2 million DVT tokens to 0x79658d35aB5c38B6b988C23D02e0410A380B8D5c. But the address is empty, isn't it?To pass this challenge, you have to take all tokens out.You may need to use prior knowledge, safely.\nHuh? No contracts?And the setup is really just transferring the tokens to an address with nothing on it... hm.\"use prior knowledge\" - Maybe this address was already used in a previous challenge?Maybe we can deploy a vulnerable contract from a previous challenge that ends up being at this address and exploit it again to get the tokens?Maybe we can somehow produce this address with create2?Ah, the #11 Backdoor challenge used Gnosis which used create2 for Wallet creation!Oh! Didn't something like this happen recently? Didn't the Optimism Team send a whole bunch of tokens to an address that was supposed to be habited by a Gnosis Wallet but that turned out to be an incorrect assumption on Layer 2? And then someone somehow redeployed the Gnosis Wallet of the intended recipient from Mainnet to Optimism's chain to that very address and stole them?I was wondering how that was pulled off but I hadn't looked into it yet. So let's try to figure it out...","unfruitful-guesswork#Unfruitful Guesswork":"Going by the hint to \"use prior knowledge, safely\" and the fact that the real incident involved Gnosis Safe just like the Backdoor challenge did, I'm thinking about deploying those same beneficiary Wallets but without any tricks to see whether any of them end up producing the said address.\nUnfortunately, this and many other things I tried along these lines didn't produce the target address.Now the problem is that there's no way to \"reverse engineer\" an Ethereum address. Basically each of the parameters in the createProxyWithCallback call influences the addresses that the Wallets will get since they're hashed and used as salt for the create2() call.Challenges like this can be quite frustrating since you have no clue whether you are even looking at the right place at all or whether there's simply a single parameter that you chose incorrectly.After some more clueless poking, I thought, what if this address is actually being used for real on mainnet? Maybe there's a Gnosis Safe Wallet deployed and I can look at its creation transaction to find out more?blockscan.com/address/0x79658d35aB5c38B6b988C23D02e0410A380B8D5cBut it doesn't seem to be used anywhere, not mainnet, not L2 nor any other chains.Ok, I need more clues or at least an affirmation that I'm somewhere close to the right track with my thinking...Let's look at the commit that added the new challenge.\nThat's pretty much all that changed aside from the newly added challenge test file.Strange he removed the Link to Twitter, hm, might as well take a look there too, he wouldn't completely spoil his own challenge, right?\nthe new mining industry nobody was expecting has made it to Damn Vulnerable DeFi! üëá\nI only see the tweet of when the challenge was added - which I already retweeted a while ago...But! Looking at other Quote Retweets there's one by Tincho himself and it's in reply to a thread talking about the Optimism incident I remembered before!I won't further look into that thread for now, but at least this confirms that my initial guess was right.Okay, so clueless poking, although fun, isn't yielding anything. So let's approach this more strategically:\nI'm quite certain that at the lowest level this will require making use of create2, since I assume that the \"prior knowledge\" hints at the use of GnosisSafeProxyFactory from the Backdoor challenge. Addresses resulting from deploying contracts via create2 can be predicted, and are calculated based on\nThe deploying contract's address\nA bytes32 salt\nThe hashed initialization bytecode\nThe GnosisSafeProxyFactory has 2 callable functions that make use of create2:\ncreateProxyWithNonce where the salt is determined by the passed initializer and saltNonce\ncreateProxyWithCallback where the salt is additionally determined by the callback\nFirst of all, the deploying contract address shouldn't be a problem, it should be the GnosisSafeProxyFactory. But, while normally this contract would already be deployed by the Gnosis Team and available for us to use - this is not the case here. All that was deployed during the setup was the DamnValuableToken contract. We're not supposed to edit the scenario setup section, and I'm not sure whether we're allowed to make use of the deployer account that is normally used to do the scenario setup. So should we deploy the factory with the attacker account? That seems strange. But well, there are just two choices here so I guess we can try both if need be.The salt in Gnosis is determined by other parameters passed to its Factory functions. I expect the saltNonce to either be 0, or a low value that we would be able to quickly find by brute force simply by continuously incrementing it by 1. The initializer and callback require more guesswork, however. Is the callback from the Backdoor challenge used at all? If not, the initializer could call other functions than setup. And in the first place, we had to use a very specific initializer during the Backdoor challenge to be able to sweep the tokens, but trying to add something here would change the resulting address! Maybe we need to make use of a function other than setup to be able to pull it off despite that? Was the reason the WalletRegistry insisted on using setup because there are other attack vectors available?Well, at the very least the initialization bytecode is something we don't have to worry about. At least I'd be very surprised if this challenge suddenly started using a different master copy than the one used in Backdoor.The Backdoor challenge's callback required us to use the setup function to initialize wallets. Are there other functions that allow placing a \"backdoor\" without influencing the salt? What happens if we don't use an initializer at all?Let's take a closer look at GnosisSafe.sol to find out....I don't really see another option than initializing a Wallet with setup.And while I thought it might handle the \"default\" case, something like setting the tx.origin as the single owner, the GnosisSafeProxy's constructor isn't doing anything besides storing the mastercopy's location:\nWhich I honestly find rather strange since the initializer is optional in the Factory:\nThis would leave your newly created Wallet uninitialized and open for anyone to take away before you can - oh wait, is this it?Let's give this a try: Calling createProxyWithNonce, no callback, no initializer, and bruteforcing 1000 saltNonces. I'll first try deploying the GnosisSafeProxyFactory from deployer and then the attacker account.\nThis quickly thrown together script to test the hypothesis unfortunately did not produce the desired address either.","looking-into-the-optimismwintermute-incident#Looking into the Optimism/Wintermute Incident":"Well, this was the best idea I had. This is getting quite frustrating again, so let's look for more clues in the Twitter thread about the Optimism incident that the challenge creator had replied to.Within the replies of this thread I found an interesting bit:\n...in case of gnosis safe, the hacker mimicked gnosis safe fabric contract and was able to create the same addresses, so he didn't have to have the right private key. It's another issue by @gnosisSafe team that was probably due to non-existing CREATE2 opcode for old gnosis safes. :-( Replay attack basically.\nOh. Wait. So no create2? But what about the whole \"You may need to use prior knowledge, safely.\" thing? After all, the Gnosis Safe version that was used in the previous challenge offered creation via create2, wouldn't that have been the \"safely\" way to do it? hm.Anyway, what is meant by \"the hacker mimicked gnosis safe fabric contract\"? The thread doesn't go further into it but it provides a few links to related transactions on etherscan allowing us to see what actually happened.After some digging I was able to find one of the first transactions of this entire debacle: Optimism sending 19 million tokens to the (yet) empty address 0x4f3a120E72C76c22ae802D129F599BFDbc31cb81. On Ethereum Mainnet this address was occupied by the Wintermute: Multisig, a Gnosis Safe Wallet (or more accurately a proxy to it).Looking at its creation transaction there we can see that it was created by the Gnosis Safe: Proxy Factory 1.1.1 which is located at address 0x76e2cfc1f5fa8f6a5b3fc4c8f4788f0116861f9b. The internal transactions show that the Multisig was created via create_0, the normal CREATE opcode and not CREATE2.Confusingly, when I look at the source code of the Proxy Factory contract that was used, the usual createProxyWithNonce functions that we know from the Backdoor challenges are in fact available. But for some reason, createProxy which doesn't make use of create2 was used. So I don't really see the reason why this was \"another issue by @gnosisSafe team that was probably due to non-existing CREATE2 opcode for old gnosis safes.\"...Maybe this is enough information already? Let's try the strategic approach again, but this time with CREATE:\nThe address of a contract created via the normal CREATE opcode can be predicted as well by the sender address and the sender's nonce. But since the nonce is irreversibly increased each time it's used, it's more of a one-time thing compared to create2.\nBoth, creations made from Contract Accounts as well as Key-Pair Accounts (EOA) can be predicted this way. The only difference is apparently that nonces of EOA wallets start at 0, while nonces of Contract Accounts start at 1.\nSo deploying something to our target address might be as simple as continuously creating Wallets until we hit the correct nonce. But we once again have the problem that the GnosisSafeProxyFactory is not deployed yet.Since the created Wallet addresses will differ depending on the GnosisSafeProxyFactory's address, there's again the problem of who should deploy it. Deploying it via the deployer account will result in a different factory address than deploying it with the attacker account. And now that I think about it: Since both of these accounts have nonces too, we might also need to deploy multiple factories to hit the right one?Maybe the necessity for this kind of brute force is the reason why the challenge is called \"Junior miners\"...I assumed that the Gnosis Team must have already had deployed the Proxy Factory on Optimism but that doesn't appear to be the case: The Proxy Factory contract was created just 4 minutes before the attacker started transferring out the first batch of tokens.But only the Gnosis Team should have the private keys that would allow deploying the Proxy Factory to that exact address, so how was this possible? We can find very same Transaction ID on Mainnet from years ago, so what must have happened here is a replay attack. The same transaction from back then was submitted to the Optimism sequencer and created the very same Proxy Factory from the same sender to the same address.Since contract creations cost gas, the attacker made sure to fund the sender address before sending the transaction. And not just that, he had to make sure that the correct nonce will be used for the Factory Contract creation, so he first replayed 2 other transactions that happened before it as well.That's quite impressive, at least I'd not have thought of that!Okay, back to the challenge. Could it be that we're supposed to replay a transaction from another chain here too to deploy the factory?Well, that would surprise me. If that were the case I'd expect that our target address has been used on any other chain, but I just can't find it. Or I'd have expected it to come up in one of the countless Gnosis Proxies that were created in order to increase the GnosisSafeProxyFactory's nonce up to the point where Wintermute's address will be next.But maybe our target address 0x79658d35aB5c38B6b988C23D02e0410A380B8D5c is based on a nonce that wasn't used yet on any chain?","fruitful-guesswork#Fruitful Guesswork":"Before we start replaying anything, let's try the brute force approach first since it's quite simple to implement and would go well with the Challenge's title, mining for addresses:\nAs you can see, I didn't bother with using the GnosisSafeProxyFactory this time since it doesn't matter what's being deployed with the CREATE opcode.\nLooking at it, I'm thinking - this can't possibly be the solution, right?\nBut it was. Hm.Honestly, I'm rather disappointed in how this challenge went. It was a lot of guesswork and it didn't really make use of the most interesting parts of the real-world incident that it's based on.For example, I think it'd have been much more fun if the target address had been deployed by some sort of Factory Contract on mainnet. Then we'd have had to replay the transactions that created the Factory locally, and finally, have it create contracts so many times until we hit the same nonce again.Oh well."}},"/posts/2022/8/12/paradigm-ctf-2021-smart-contract-challenges-write-up-1":{"title":"Paradigm CTF 2021 - Smart Contract Challenges Write-Up #1","data":{"":"August 12, 2022 by patrickd\nDue to being late to the party, I missed the Paradigm CTF of 2021 - but not this year!Paradigm CTF 2022 is coming up on the 20th of August, and it's time to get ready. So let's take a look at the challenges from last year!","babysandbox#BabySandbox":"I read that staticcall will keep my contracts safe.\nThis challenge consists of two contracts, the first of which, Setup.sol appears to both deploy the other (BabySandbox.sol) and also have a function to check for whether we succeeded:\nWhich appears to be the case when the code-size of the deployed BabySandbox contract becomes 0 - so we have to somehow manage to selfdestruct it?Looking at the contract in question, we're greeted by intimidating assembly, but only one function: run(address code) {} - which can be broken down to:\nIf the contract is calling itself, execute a delegate-call to the specified address (this should be where we can specify an address that allows us to selfdestruct this contract).\nOtherwise, forward the current call via a static-call to itself and only if it doesn't fail, make another normal call to itself.\nThe STATICCALL opcode \"is equivalent to CALL, except that it does not allow any state modifying instructions\". It's basically what ensures that Solidity view-functions are more than just syntactic sugar (unlike pure functions which do not have an opcode to ensure that no chain state is read). Since it's state-changing, SELFDESTRUCT is one of the opcodes that is disallowed during a static-call.First question is: How can we detect whether we've been static-called so that we can act differently when we're not?If we'd attempt to change state, that would revert. How can we catch a revert? Assuming that all sub-calls made by a function that was static-called, also share the same restrictions, we should be able to use a normal call and see whether it reverted or not?We can check quickly this assumption with a quick & dirty script on remix:\nAnd indeed, amIbeingStaticCalled() can successfully differentiate how it has been called. Based on this we can add logic to either self-destruct, or not.\nThat works! Cool challenge!","bouncer#Bouncer":"Can you enter the party?\nIn this challenge we can again find both setup steps and success conditions within Setup.sol:\nBouncer contract is deployed with 50 ether\nNot sure what these lines do yet, but weth is Wrapped Ether as WETH9\nbouncer.enter{value: 1 ether}(address(weth), 10 ether);{:solidity}\nbouncer.enter{value: 1 ether}(ETH, 10 ether);{:solidity}\nA Party contract is initialized and the bouncer is passed to it\nThe success conditions are apparently to make it so that the bouncer's ether balance becomes 0.Opening Bouncer.sol reveals a whole bunch of functions, but which one allows removing the balance?\nOwner-authenticated functions aside, it seems there's only one: The internal payout() function, callable via redeem().Unlike the previous challenge, this one is using Solidity 0.8.0, so we won't be able to exploit an underflow when the balance is updated.There's only one function where we can influence the tokens variable:\nSo, entries can be converted to tokens which can be withdrawn..\nAnd that's where we come back to the enter() function from the setup.Based on the fact that the fee collection happening in it will lead to a fee ether balance to accrue that is only intended to be withdrawn via the owner-authenticated claimFees() function - it's safe to say that there must be some way to withdraw the same entry multiple times.And whenever it's about re-using the same ether value, there's one thing to look out for: msg.value being checked within a loop!The message value check is happening within the proofOwnership() function, which is called during the token conversion:\nAnd the loop we're looking for can be found in convertMany():\nAnd thanks to the fact that nothing is preventing us from sending the same entry id over and over again, we only have to pay the fee once!\nNot the most gas efficient way to solve it, but it works!A classic issue hidden within a lot of noise...","secure#Secure":"My contract is 100% secure, it's impossible to hack.\nThis time around Setup.sol appears to be setting up a Wallet. This Wallet seems to use modules for adding functionality such as the handling of ERC20 Tokens.The WANT constant demands that 50 ether worth of WETH are deposited from the Setup into the Wallet and it appears the goal is to get them out of the wallet back into the hands of the Setup contract.A first look at Wallet.sol reveals a contract that only has authenticated functions that we shouldn't be able to call. But what strikes me as rather strange is the Solidity version: 0.5.12 - maybe there's a compiler bug here?To check that, there's the official bugs_by_version.json file, where we can find all known issues for this compiler version:\nComparing these with the version before (0.5.11) and after (0.5.13) doesn't show a difference that would suggest that a bug was newly introduced or fixed - guess that would've been too easy..I checked each of the more detailed bug descriptions in bugs.json but none seemed relevant..So, maybe it wasn't a bug, but just a really weird and unexpected quirk in 0.5.x? Or is all that just a distraction?\nAfter some more intensive staring, I noticed that the operator management functions are actually broken. The owner will only be able to set and unset themselves as operator, but unable to set anyone else since the operator function parameter is simply not used. Probably not relevant to the actual solution, but maybe a hint that other \"dumb mistakes\" are hiding here? I don't see one..After a lot of trial, error, and debugging around the idea there might be some memory address wrapping going on with the bytes memory data of the execModule() allowing to overwrite the scratch space and therefore the calculation of the operators mapping slot... blah blah. Let's just say I spend a lot of time trying to find a way of exploiting a bug that doesn't exist.At some point I was like: Hm, the challenge's solve-condition is that the Setup contract needs to have 50 ether. That can't possibly mean that I simply have to send 50 ether from the player account to the Setup contract, right? That would be so stupid and lame, right?Well, I couldn't take it anymore and took a peek at the solution contract:\nwow.... excuse me while I'll find a table to flip."}},"/posts/2022/7/25/secureum-bootcamp-epoch-july-race-8":{"title":"Secureum Bootcamp Epoch‚àû - July RACE #8","data":{"":"This is a write-up of the Secureum Bootcamp Race 8 Quiz of Epoch Infinity with solutions.\nThis quiz had a strict time limit of 16 minutes for 8 questions, no pause. Choose all and only correct answers.Syntax highlighting was omitted since the original quiz did not have any either.\nAugust 2, 2022 by patrickd","code#Code":"Note: All 8 questions in this RACE are based on the following ERC721 implementation. This is the same contract you will see for all the 8 questions in this RACE.The implementation is adapted from a well-known contract. The question is below the shown contract.","question-1-of-8#Question 1 of 8":"The security concern(s) addressed explicitly in _mint include\n A. Prevent minting to zero address \n B. Prevent reminting of NFTs \n C. Transparency by emitting event \n D. None of the above \nCorrect is A, B, C.The _mint() function addresses both A and B with the first two requires. Also C is correct since the emission of the Transfer event allows for easy tracking of mints and therefore transparency.","question-2-of-8#Question 2 of 8":"The security concerns in _burn include\n A. Anyone can arbitrarily burn NFTs \n B. Potential integer underflow because of unchecked \n C. Incorrect emission of event \n D. None of the above \nCorrect is A.It appears that the _burn() function was intended to be internal (based on the underscore prefix) but is actually external which allows for A.Answer B is not a concern thanks to the ownership check ensuring that it cannot happen.The emission of the event follows the event declaration and therefore C is not a concern either.","question-3-of-8#Question 3 of 8":"The security concern(s) addressed explicitly in _safeMint include\n A. Validating if the recipient is an EOA \n B. Ensuring that the recipient can only be an EOA \n C. Validating if the recipient is an ERC721 aware contract \n D. None of the above \nCorrect is A, C.This function ensures that if (A) the recipient is an EOA the mint functions normally thanks to the to.code.length == 0 check, but if (C) the recipient is a contract (non-EOA) it must be \"ERC721 aware\" by implementing the ERC721TokenReceiverinterface.","question-4-of-8#Question 4 of 8":"Function approve\n A. Allows the NFT owner to approve a spender \n B. Allows the NFT spender to approve an operator \n C. Allows the NFT operator to approve a spender \n D. None of the above \nCorrect is A, C.The require shows that only (A) the NFT owner and (C) the operator that the owner gave access to manage all their NFTs have the ability to approve spenders. A spender cannot approve other spenders and especially not operators.","question-5-of-8#Question 5 of 8":"Function setApprovalForAll\n A. Approves msg.sender to manage operator‚Äôs NFTs \n B. Gives everyone approval to manage msg.sender‚Äôs NFTs \n C. Revokes everyone‚Äôs approvals to manage msg.sender‚Äôs NFTs \n D. None of the above \nCorrect is D.The setApprovalForAll() function authorizes an address (called the operator) to manage all of the owner's NFTs in the contract. A, B and C are therefore incorrect.","question-6-of-8#Question 6 of 8":"The security concern(s) in transferFrom include\n A. Allowing the msg.sender to transfer any NFT \n B. NFTs potentially stuck in recipient contracts \n C. Potential integer underflow \n D. None of the above \nCorrect is A, B, C.The transferFrom() function does not check ownership of the NFT. This allows any msg.sender to overwrite the current owner, basically allowing a transfer of any NFT.The safeTransferFrom() function ensures that NFTs will not be stuck in recipient contracts that don't communicate that they are able to handle them. This issue still exists for the normal transferFrom() function though for backwards compatibility reasons.Due to the missing ownership check, it's possible for the balance of the sender to underflow.","question-7-of-8#Question 7 of 8":"Which of the following is/are true?\n A. NFT ownership is tracked by _ownerOf  \n B. NFT balance is tracked by _balanceOf \n C. NFT approvals are tracked by getApproved \n D. NFT operator can transfer all of owner‚Äôs NFTs \nCorrect is A, B, C, D.The variables _ownerOf, _balanceOf and getApproved indeed keep track of the mentioned values.And NFT operators are by definition able to transfer all NFTs of the owners that elected them to be their operators.","question-8-of-8#Question 8 of 8":"ERC721 recognizes the following role(s)\n A. Owner  \n B. Spender (Approved address) \n C. Operator  \n D. None of the above \nCorrect is A, B, C.This is quite apparent from ERC721 implementation parameter names. They can also be found in the EIP721 spec."}},"/posts/2022/8/18/paradigm-ctf-2021-smart-contract-challenges-write-up-2":{"title":"Paradigm CTF 2021 - Smart Contract Challenges Write-Up #2","data":{"":"August 18, 2022 by patrickd\nAll tables have been un-flipped, and if you don't get the reference you're probably missing the first part of this write-up.Due to time constraints, I won't be able to solve the remaining Challenges in time before Paradigm CTF 2022 starts. The following are the notes I took of the Challenges I've been working on up to the point where I'm certain about the solution.","upgrade#Upgrade":"Circle released a new update to USDC but something seems off. Can you take a look?\nAs usual, we begin by checking the challenge setup to make sure no weird stuff's being done there, and this time there's actually something interesting going on in the chal.py deployment python script:\nIt appears two transactions are being crafted here that'll be executed after the Setup contract was deployed.The first transaction is sent from an EOA account to the USDC stablecoin contract (or rather, its upgradable proxy), which said EOA is an admin of. The data can be split in two parts: The first four bytes are the function signature being called and the rest is the deployed Setup's contract address padded to 32 bytes. A quick lookup of 0x8f283970 in the 4byte.directory reveals that the transaction calls the changeAdmin(address) function. So the admin is being changed to the Setup contract.The next transaction simply calls the upgrade() function on the Setup contract:\nA new Version 3 implementation is deployed and initialized with multiple calls, most likely to prevent the implementation from being initialized and potentially taken over by someone else. USDC's FiatTokenProxy is then told to make use of this v3 implementation and initialized. The initialization call for Version 2 is skipped though.. does that mean that on mainnet it already was upgraded once or was it \"forgotten\"?Finally, the admin is changed back to the original. And the isSolved() function tells us that 200 million USDC need to be transferred to the Setup contract in order to pass the challenge.Let's look at the actual mainnet state of the proxy in question. Checking etherscan we can see that the current implementation is in fact FiatTokenV2_1 which was deployed on the 17th of April 2021... But was that the one used during the CTF?From the information available I reconstructed the following timeline:\n3rd of August 2018: FiatTokenProxy was deployed with FiatTokenV1 as the implementation.\n20th of August 2020: FiatTokenV2 was deployed\n27th of August 2020: FiatTokenV2 was set as implementation\n5th of February 2021: The Paradigm CTF was being held\n17th of April 2021: FiatTokenV2_1 was deployed\n26th of April 2021: FiatTokenV2_1 was set as implementation\nIt appears safe to assume that the Version 2 (who's code also exactly matches that of FiatTokenV2.sol provided by the challenge) was the one active during the CTF. And the challenge's FiatTokenV3.sol is the actual challenge target since it doesn't exist outside the CTF.So my initial suspicion that the error might be that the initialization of Version 2 was \"accidentally\" skipped causing a re-initialization attack vector is incorrect. Next, we should look into possible storage clashes between versions...\nTurns out FiatTokenV3 simply extends FiatTokenV2. In terms of storage that means all state variables declared within FiatTokenV3 should be assigned fresh slots that come after those that were used by FiatTokenV2.That single _gap variable is weird though.. Usually, you see storage gaps within inheritable base contracts as arrays of 32 byte types. For example, OpenZeppelin's upgradable contracts always assume that each base contract uses 50 slots (state variables + gap array). If a new version of such a base contract needs more state variables, one just has to reduce the amount reserved by the gap array. This way introducing new state variables to inheritable contracts doesn't impact the storage locations of later state variables in the inheritance chain. But FiatTokenV2 makes no mention of gaps, so why start now with a single one?Could it be that Version 1 used more slots that became unused in Version 2 and are now clashing with v3?At first glance it appears that FiatTokenV2 is also simply extending v1:\nHowever, that assumption might not be correct since they had to upgrade the original Version 1's code, which used an older Solidity version. During this process, they might have changed a few things regarding the usage of storage slots?\nNo, seems like all slots match up nicely between v1 and v2...I feel like I've fallen for another distraction here. Let's take a look at the new functionality introduced by Version 3:\nThat seems like a rather naive Token lending implementation. You can lend USDC to another address and, in theory, you can take it back from there - but that'll only work if the account you've lent it to didn't transfer it somewhere else already... You better trust the person you do this with since they can just run off with your money.How can we get 200 million USDC out of this? An amount of that sum sounds like a job for Flash Loans... And looking at this, we should be able to \"repay\" a Flash Loan using the lend() function. And then, after the Flash Loan finished, we would be able to reclaim() the amount we have \"lend\" directly from the Pool...So, where can we that money from? Normally we'd be able to look at the USDC Token Holder list and search for one that offers  Flash Loans, but here we'll have to do it for a chain fork at a blockheight from a year ago. Many of those that hold 200+ million USDC now probably didn't have that amount yet at that time, and those who did probably don't do today.Well, it seems Block #11800000 was mined around the time of the CTF. How can we figure out the top USDC holders at that point? Often questions like this are asked for airdrops, so after some searching I found the erc20-snapshot project which should be able to provide us with a Token Holder list CSV file for back then.Next Steps:\nGenerate a list of USDC Token Holders, sorted descending by how much value they hold\nFind a contract in this list that provides Flash Loans\nCreate an Exploit contract that\nTakes a 200 mil USDC Flash Loan\nCalls the USDC lend() function to immediately pay it back within the callback function\nOnce the Flash Loan has been completed (outside of the callback function) call reclaim() to obtain the Tokens that were previously loaned","babyrev#BabyRev":"If I don't verify my source code, then hackers can't exploit my contract, right?\nThis time the Challenge contract is within the private directory, that calls for reverse engineering!It's strange though that the Setup constructor requires 50 ether which it doesn't do anything with... Maybe a copy & paste mistake?Anyway, let's start by trying to decompile the contract with ethervm.io/decompile which is able to tell us that there are in fact 4 public functions:\nThe disassembly itself appears to have failed, I suspect that the contract used assembly blocks that make it struggle to reconstruct high-level code from, or maybe it's because of the old Solidity version.. not sure.Also tried the other decompilers I know of: Panoramix, Heimdall and Dedaub - but all of them either errored or produced nothing useful.Well, with what we know so far, and some guessing about the return values, we can extend the ChallengeInterface and see what the discovered functions do using a debugger...\nThe decrypted() function always returns PCTF{v32y_53cu23_3nc2yp710n_4190217hm} (reads \"very secure encryption algorithm\"), which isn't the flag that we're looking for but most likely part of an example together with encrypted() which always returns . This suspicion is further confirmed by the fact that both of these bytestrings are directly PUSHed onto the stack from the bytecode, therefore they must be constants.The solved() function doesn't appear to be doing much more than loading and returning a boolean value from SLOT 0.Finally, the solve() function is where a lot of magic is happening. What I'd expect it to be doing: Try to decrypt the same bytestring that encrypted() returns with a key that we can pass as a parameter. Then compare the result of this decryption with the constant string of decrypted(), and only if it's the same, set the state variable of storage SLOT 0 to true.And here's what it actually does when following along with a debugger:\nadds 0x2000 (8192 bytes) to free memory pointer, which are exactly 256 chunks of 32 bytes\ncodecopys 0x2000 bytes from its own code starting at codesize - effectively filling memory with zeros\ncopies decrypted constant to memory after that\ncopies encrypted constant to memory after that\nchecks that both are same length\nagain adds 0x2000 (8192 bytes) to free memory pointer\nstarts filling newly reserved memory slots one by one with seemingly random single bytes\ncopies encrypted constant to memory again\nmost likely a loop starts, one round for each character of the constants (0x26 (38))\ninput parameter is shifted left by 0xf8 (248) - rightmost byte becomes isolated leftmost byte\nrightmost byte of encrypted constant becomes isolated leftmost byte\nboth bytes are XORed\nresult byte overwrites leftmost byte of decrypted constant (replacing character that was XORed)\nmost likely another loop starts\nrightmost input byte is isolated and multiplied by 0x20 (static)\nresult is ADDed to the pointer of the first seemingly random byte's slot\nbyte from that location is loaded and replaces the input byte\nloop starts from the beginning, with the next byte of the input\nit does so for all 32 bytes of the input (byte table acts as mapping eg. 0x00 becomes 0x63)\nthe rightmost byte is circular shifted to the leftmost side\nthe loop restarts with the new rightmost byte as input\nthe decrypted constant is stored in memory, right after the decryption attempt (overwritten encrypted constant)\nafter some weird memory management, a copy of the decrypted constant is SHA3 hashed\nafter some more weird memory management, a copy of the decryption attempt is SHA3 hashed\nboth hashes are compared against each other\nstores a static 0x08c379a0 to memory\nthen a static 0x736f6c76652f6e6f742d736f6c766564 (ascii: \"solve/not-solved\")\nreverts\nIt appears my guess was correct.Now, how to actually solve this? Since each byte of the input is individually XORed with each byte of the encrypted constant, we should be able to basically brute-force this character-by-character:\nIterate through all numbers from 0 to 255 and use it as new input byte\nCheck whether the resulting byte matches the byte from the decrypted constant\nIf it matches, we have probably found the right input byte and we can continue guessing the next one\nIf it does not match, continue iterating\nOne by one we should be able to determine the full key and then we'd simply have to call the solve() function with it."}},"/posts/2022/8/18/sznsdaos-bountyboard-unauthorized-transferfrom-vulnerability":{"title":"SZNS‚Äô BountyBoard: Unauthorized TransferFrom Vulnerability","data":{"":"August 18, 2022 by patrickd\nTL;DR: If you have ever given approval to the BountyBoard contract (at 0x879d25dB71DD9ff94452C510C13347fb49175D9D) for any of your NFT collections, please be sure to remove it asap. If you've never given this contract an approval, your jpegs are safu.","timeline#Timeline":"Thursday, 4th August: Possible vulnerability noticed. Contacted sznsDAO Team via Discord while further investigating the suspicion by creating a local PoC. Determined that a single person is most at risk of losing significant funds through this vulnerability. Through the ENS name, it's clear that this person is part of the SZNS Team, the person was directly contacted and removed approval to the vulnerable contract. A meeting with the Team was scheduled.\nFriday, 5th August: The PoC is presented to the Team during the meeting. Team was recommended to first warn a small circle and slowly increase the reach to ensure that affected users are reached first and potential malicious actors from outside of the community won't hear from it too soon. A list of likely affected addresses, of which some have ENS names, is provided to the Team. They'll work on trying to identify affected users and contact them directly. Although no official Bug Bounty program exists, the Team offers 1 ether as a reward, which is gladly accepted.\nMonday, 8th August: Got confirmation that the web Interface for interacting with the vulnerable contract was disabled.\nSunday 14th August: All individuals that could be identified were contacted and warned to revoke approvals.\nMonday 15th August: A warning to remove approvals from the affected contract (without further explanation) was posted on the SZNS Discord.\nFriday 19th August: Vulnerability is publicly disclosed.\nThe SZNS Team also followed general improvement recommendations:\nA security specific contact for disclosing security issues was added to the documentation.\nSpecific Bug Bounty rules were adopted and added to the documentation.\nThe documentation still claims that \"SZNS largely relies on battle-tested smart contracts that have been audited by Peckshield\", but it does not explain when these audits happened and for which parts of the SZNS ecosystem they were done. It's recommended to publish the original reports for transparency.","vulnerability#Vulnerability":"The BountyBoard contract allows owners of ERC20 Token Contracts with Minting capability to exchange their Tokens for NFTs. They can create an Order describing what NFTs they are looking for and owners of said NFTs can fill these Orders, giving away their NFT in exchange for freshly minted ERC20 Tokens.\nThe issue is that the function allowing to fill these orders does not check whether the caller owns the NFTs that they want to fill the order with. This means that the order can be filled with NFTs of other people who still own NFTs for a collection they approved the BountyBoard contract for.\nThe issue should easily be fixed by replacing erc721.ownerOf(id) with msg.sender. That way attempting to transfer the NFTs of other users will error since the callee does not own them.","attack-scenarios#Attack Scenarios":"Two ways by which this vulnerability could have been exploited were identified.","reward-stealing#Reward Stealing":"If the attacker is more interested in one of the offered ERC20 Tokens than any of the approved NFTs, they could \"tribute\" approved NFTs of other people to obtain the reward.\nBob created an order on the BountyBoard offering their ERC20 Tokens in exchange for specific NFTs\nAlice has multiple NFTs from the collection Bob desires and decides to exchange one of them\nAlice gives approval to the BountyBoard contract, effectively making it an operator over the NFTs she owns of the collection\nAlice fills Bob's order giving one of her NFTs in exchange for Bob's ERC20 Tokens\nEve notices Bob's order and is interested in obtaining the ERC20 Tokens but does not own the required NFTs to participate\nEve notices the vulnerability in the BountyBoard contract\nEve notices that Alice gave approval to the BountyBoard contract to manage all of her NFTs for the collection Bob is interested in, and that Alice owns more of said NFTs that she has not exchanged for Tokens yet\nEve can call the order filling function while specifying Alice's NFTs as tribute\nEve will receive Bob's ERC20 tokens as a reward, Bob receives Alice's NFTs\nAlice has lost all NFTs of the collection that she approved the BountyBoard to manage","nft-stealing#NFT Stealing":"If the attacker is interested in obtaining all NFTs that have been approved to the contract, they are able to create an Order with a worthless custom ERC20 Token and have the validator accept any ERC721 Tokens as meeting the criteria for the Order. Then the attacker can batch-fill their own order to obtain all of the NFTs they determined to be available for transfer.\nBob created an order on the BountyBoard offering their ERC20 Tokens in exchange for specific NFTs\nAlice has multiple NFTs from the collection Bob desires and decides to exchange one of them\nAlice gives approval to the BountyBoard contract, effectively making it an operator over the NFTs she owns of the collection\nAlice fills Bob's order giving one of her NFTs in exchange for Bob's ERC20 Tokens\nEve notices the vulnerability in the BountyBoard contract and is interested in obtaining Alice's NFTs\nEve creates an Order that accepts any NFT with herself as the beneficiary, in exchange for a worthless ERC20 Token she created\nEve fills her own order while specifying Alice's NFTs as tribute\nEve will receive Alice's NFTs, and also some of her own ERC20 Tokens\nAlice has lost all NFTs of collections that she approved the BountyBoard to manage","conclusion#Conclusion":"The likelihood of the vulnerability being exploited at some point seemed almost certain, but luckily the severity at this point was very low. The most significant amounts of funds at risk were quickly taken to safety and by carefully warning the most likely affected users it was not exploited by bad actors.Although this was the first security issue ever reported to them, the SZNS Team reacted openly and quickly to warnings and suggestions and showed that they intend to not only handle the current issue but want to improve processes for the future."}},"/posts/2022/8/22/paradigm-ctf-2022-electric-sheep":{"title":"Paradigm CTF 2022 - Electric Sheep","data":{"":"August 22, 2022 by patrickd\nThe CTF just finished last night and I'm still recovering from a weekend of sleep deprivation, too much caffeine, and a horrible diet.Even so, I'm quite proud of being one of the few people who managed to solve Electric Sheep and many have asked for an explanation so this is where I'll start this write-up series.","the-context#The Context":"Tags: PWN, YOU UP?\nThe file archive provided for this challenge doesn't give much:\nThere's some ERC20 Token and the Setup contract is supposed to end up owning more than 16 of them, which seems weirdly specific but okay. The Token is specified by its address, so that should mean it was already deployed on mainnet and we'll get access to a fork.And indeed, we're able to find it on etherscan. Turns out it's called \"CryptoDreamers Token (CRDRT)\" and, as you'd expect, it's worthless at this point and there haven't been any contract interactions for a long time. The deployer account doesn't easily give away who it belongs to, but a bit of Internet search yields a tweet belonging to \"MultisHQ\".Multis, Inc. is apparently dealing in \"Corporate cards and financial software for web3\". The most interesting thing I could take away from their website is \"Multis is built with Gnosis Safe,\nthe gold standard of self-custodial wallets\", but aside from that it's a lot of marketing stuff and not very helpful for the challenge.After some more searching, I found an old blogpost that mentions a Token that sounds very similar to the one we're dealing with: \"The CryptoDreamers Token (CDT) is a Token issued by Multis, simply holding it allows you to pay for gas in the process of creating a Multis account, with a 1:1 ratio to ETH.\"\n\"Technically, CDT is an implementation of the GSNRecipientERC20Fee interface, this is a rather complex payment strategy, but since it comes built-in with OpenZeppelin SDKs, integrating it was straight forward, since it boils down to extending your contract with both a GSNRecipientERC20Fee and a MinterRole so you can issue tokens.\"\nFinally some technical information on what we're most likely dealing with here. Reading this blogpost was very useful in obtaining a high-level understanding of what we'll be dealing with later.","the-funds#The Funds":"There's still the question of where exactly we'll get those 16 ether worth of Dreamer Tokens from. Since it's such a specific amount, I expect that we probably won't have to mint it, most likely we'll need to take it from one of the existing Token Holders. And there are actually only two accounts holding a balance high enough to do that, if combined, we could solve the challenge:\n13.175 held by a \"MultiSigWalletWithDailyLimit\" contract deployed by an unknown EOA account via the official Gnosis \"MultiSigWalletWithDailyLimitFactory\".\n7.70716595067373386 held by an \"AdminUpgradeabilityProxy\" pointing to \"GSNMultisigFactory\" which was deployed by the same EOA as the Dreamer token, so it most likely belongs to Multis as well.\nI can't imagine though that we're supposed to find an issue within a Gnosis wallet that still holds a lot of money on mainnet. And finding one in the Factory wouldn't give us enough Tokens to solve the challenge, we're missing something...","the-token#The Token":"Problem is, the Dreamer token's bytecode is not verified so we'll have to do some decompilation.I use EtherVM.io to obtain a nice list of all the public functions the contract has:\nThe decompilation results themselves take a lot of effort to understand with this one though. So next I try the \"palkeoramix\" decompiler on etherscan. It doesn't work well often, but when it does, it produces some really readable pesudo-code:\nWe're in luck!I copied out the most interesting parts: There's a \"primaryAddress\" that has unlimited allowance to anyone else's funds and access to the mint function. There's also a transferPrimary() function allowing to overwrite this priviledged address - obviously only the primary can do that.The count of initialization functions seems a bit weird. The compiler also claims that SLOT 0, which is the one that all of them check and write, isn't consistently accessed. This made me wonder whether it might be the case that one initialization function incorrectly resets the value of another. A quick test showed that this is not the case though, attempting to call any of them will fail.Anyway, let's get back to the \"primaryAddress\": Turns out that it's actually set to the \"GSNMultisigFactory\" contract we discovered previously, or more accurately, its proxy.","the-factory#The Factory":"The Factory matches the description of the information about Multis so far: It deploys Gnosis Safes and implements GSNRecipientERC20Fee allowing one to do so via a Relay.Within its source code we can actually find the code belonging to the Token we just decompiled: \"__unstable__ERC20PrimaryAdmin\". In its description, it's mentioned that \"This contract is an internal helper for GSNRecipientERC20Fee, and should not be used outside of this context.\" - sounds a bit weird.But what we're actually interested in, are any calls made to this Token - and you should immediately stumble upon the mint() function:\nIt's authenticated and the only minter registered is the same EOA that deployed it. So not this then.The only functions left doing interesting stuff are internal:\nThese look a lot like callback functions that are called before and after the factory's create() function is called by a relayer.Maybe anyone can call them?\nNope, turns out the external versions actually authenticate who's calling in.They're still interesting though: The preRelayedCall() is able to transfer Tokens from anyone's account to the Factory contract. And postRelayedCall() transfers Tokens from the Factory to another address. Maybe we can use this...","the-relayhub#The RelayHub":"The getHubAddr() function returns an address pointing to a \"RelayHub\" and well, things are about to get complicated.Here's the breakdown of how it works:An Owner can register their Relay by first staking a minimum of 1 ether via function stake(address relay, uint256 unstakeDelay){:solidity} and then calling registerRelay(uint256 transactionFee, string memory url){:solidity} from the Relay address. It's important to note that an Owner address can't be the Relay and a Relay can only be an EOA.A registered Relay can then call the relayCall() function:\nIn our case, the recipient should always be the Factory contract and, as expected, the hooks will be called during the process. Here's the order:\nacceptRelayedCall()\npreRelayedCall()\nCalldata specified in encodedFunction\npostRelayedCall()\nI ignored the Factory's acceptance function before since it didn't do what we were looking for, here it is:\nIt checks whether we have enough Dream Tokens for the maxPossibleCharge.Well, we don't have any of the Dream Tokens, so normally this would make our relayed call fail at this point. \"Normally\" because normally a Relayer would want to be paid back for sending the transaction for us, but if we're both the Relayer and the person that signed the message, couldn't we do it for \"free\"?\nYes! If we pass a gasPrice of 0 to the relayCall() function the resulting maxPossibleCharge will actually be 0. This also resolves any issues with the pre- and post-callback functions: No Tokens will be precharged and no Tokens will be refunded, the transfer-calls wont complain about an insufficient balance.At this point I realized: Oh, I can just sign a transaction that calls those hooks. The check on msg.sender will pass since the transaction is being relayed via the Hub. I guess normally you're supposed to use the context constructed by acceptRelayedCall() to ensure that they can't be called \"out of order\", but there are no such checks here.We can use the preRelayedCall() function as a gadget allowing us to transfer arbitrary Token balances of anyone to the Factory. Since we're calling it directly, the post-hook won't be triggered and this \"precharge\" won't be refunded to the original owner. Our victim will be the \"MultiSigWalletWithDailyLimit\" that we found previously that holds most of the Tokens.After that, we can relay another call, this time directly to postRelayedCall() which allows us to take arbitrary amounts of Tokens out of the Factory contract.","the-exploit#The Exploit":"There's a lot going on here and I hadn't really understood everything at this point. But it would make a lot of sense so I just wanted to give it a quick try in Remix:I staked 1 ether via a smart contract and registered the EOA provided by the challenge as Relay.\nWrote some helper functions: One that returns the hash of the message we need to sign and the other one returns the ready-to-send calldata with the signature that we created outside.\nQuick & dirty python script for signing these hashes:\nUsing this, we can easily build the first call to the pre-hook:\nFirst calling relayCallHash_preRelayedCall() will return us the hash we need to sign for the given relay parameters. Once signed (using the above python snippet) we can pass the signature into relayCall_preRelayedCall() and get back the calldata that we have to fire against the RelayHub contract.","the-interlude#(The Interlude)":"The first time I tried this, I immediately checked whether it worked and to great desperation, I noticed that the Token balance of the Factory contract was still the same as before. The way how RelayHub is built, any reverts happening during the relayed call will not cause the transaction to fail. This and the many moving parts involved made it rather hard to debug and after many hours of trial and error, I finally noticed that I had sent a gasLimit of 0. I thought that didn't matter since the gasPrice being 0 already would lead to a 0 in the overall multiplication anyways. What I overlooked was that this gasLimit was actually enforced on the relayed call:\nOops.","the-end#The End":"With that working, the Factory contract now has a balance of 20,88216595067373386 ‚Äì that's plenty to solve the challenge, now we just have to get it to the Setup contract.\nJust as before we first call relayCallHash_postRelayedCall(), sign the returned hash, pass the signature to relayCall_postRelayedCall() and send the returned calldata to the RelayHub.And indeed, it worked!And yes, this whole thing should've been done in a single Python/JavaScript, but.. it was supposed to just be a quick check and ended up taking hours of debugging. So the clean solution never happened.Oh well.Note: This can still be exploited on mainnet, but you shouldn't. Yes, the tokens are basically worthless, but they're still not yours. Use a fork."}},"/posts/2022/8/27/secureum-a-maze-x-stanford-ctf":{"title":"Secureum A-MAZE-X Stanford CTF","data":{"":"August 27, 2022 by patrickd\nSecureum organized a Capture The Flag Workshop on the 26th August 2022 during the First Annual DeFi Security Summit. Unfortunately I couldn't personally make it there, but even so I had the chance to take an early look and provide feedback on the CTF's GitHub repository.\nNote that a few things have changed since the time I solved it. So there might be some discrepancies but the overall Solutions should still be the same.\nThis CTF consists of 4 Challenges with increasing difficulty. Since it was happening in the context of a Workshop, there was no competetive aspect to it like it usually would, the goal was to focus on learning and having fun.","challenge-0-vitatoken-seems-safe-right#Challenge 0: VitaToken seems safe, right?":"Let's begin with a simple warm up. Our beloved Vitalik is the proud owner of 100 $VTLK, which is a token that follows the ERC20 token standard. Or at least that is what it seems...Is there a way for you to steal those tokens from him?\nThe target for this challenge is the Challenge0.VToken.sol contract. It's a very simple ERC20 implementation that makes use of OpenZeppelin's contracts.It only deviates from the default behavior in two ways:\nDuring its creation 100 of the tokens are minted directly to \"Vitalik's account\".\nThere is a public function called approve(), that allows for approvals.\nBut, you might know that the ERC20 implementation of OZ already comes with an exposed approval() function, so how can there be another one without the override keyword being used?The reason is what Solidity calls \"function overloading\", but is factually just a completely different function:\nOpenZeppelin's function approve(address spender, uint256 amount) public virtual override returns (bool){:solidity} has a function signature from approve(address,uint256){:solidity} which is 095ea7b3\nThe Challenge's function approve(address owner, address spender, uint256 amount) public returns (bool){:solidity} has a function signature from approve(address,address,uint256){:solidity} which is e1f21c67\nThis means that the ABI descriptions of these two functions can exist alongside each other without clashing.Now, let's compare the function bodies:\nThere's only one difference between them: OpenZeppelin only allows the msg.sender, meaning the owner of the tokens, to give another account an allowance to manage. While the Challenge's ERC20 allows anyone to give allowances for anyone else's tokens.Exploiting this \"feature\" to obtain \"Vitalik's\" token balance is quite simple:\nFirst we cast the address of the token (where the ERC20 VitaToken contract is located) to the VToken contract. This allows us to call its non-standard approve function and give us an \"unlimited\" allowance by specifying the maximum number that a 256 bit integer can represent.After obtaining this \"approval\" we can now make use of the standard transferFrom() function to send the entire balance of the tokens that \"Vitalik\" currently owns to the player address.\nHaving done the transfer will make the winning conditions pass: The total supply of VitaTokens in existance is now equal to the balance of the player account.","challenge-1-what-a-nice-lender-pool#Challenge 1: What a nice Lender Pool!":"Secureum has raised a lot of Ether and decided to buy a bunch of InSecureumTokens ($ISEC) in order to make them available to the community via flash loans. This is made possible by means of the InSecureumLenderPool contract.The idea is that anyone can deposit $ISECs to enlarge the pool's resources.Will you be able to steal the $ISECs from the InSecureumLenderPool?\nLet's first check the InSecureumToken contract we're dealing with. This time, no extra \"features\" have been added, it's a simple ERC20 token. All the constructor does is minting the specified amount of tokens to whoever will deploy it, the msg.sender.With that out of the way let's take a look at Challenge1.lenderpool.sol.The idea of this contract is quite simple: Users are able to deposit() and withdraw() tokens to and from the pool contract. Deposited tokens can be used by third parties to borrow large amounts of tokens from this pool to, for example, make a profit using them for arbitrage. After they are done using them, they have to return all tokens, otherwise the entire transaction will fail and so will their arbitrage trade. Usually flash loaning comes with a fee that is then distributed to depositors, which incentivices the deposit in the first place. But this was likely omitted for simplicity.If we are supposed to steal all of the deposited tokens, the issue likely lies in the flashLoan() function itself:\nAs is common, this flash loan works with callback-functions: We begin the loan by executing the flashLoan() function and specify a borrower contract that'll receive the loan. It'll then take note of the balance before and after it calls into a callback function that we specified as part of data. To make sure that we paid back the loanm it requires the balance after to be greater or equal to the balance before.You might also notice that it sets a and unsets a boolean state variable before and after the callback is executed. This is in a way a \"reentrancy protection\" to prevent us from re-depositing the loan in our own name back into the pool. That would basically allow us to ensure that the balance after is correct and at the same time allow us to withdraw() the tokens as if they were ours.Where things get interesting is when you notice that nowhere before the callback is executed, the loan is sent to the borrower. Apparently the Pool expects us to take as much as we need ourselves. It's doing that by delegate-calling the callback. That means the callback will be executed as if it were a function of the Pool itself, giving us access to the funds. But that also means we can do anything else we want with that power as long as we make sure that the balanceAfter i still correct.So what can we do while having full control over the Pool's \"identity\"? We can make use of something that we've learned about in the first Challenge: Token approvals.\nAs you can see, we're not actually borrowing any of the tokens in the Pool. All we do is make use of the Pool's identity to give the player an unlimited allowance. This allowance will still be there once the flashLoan() function has finished and determined that everything is in order since the balances still match. Then afterwards we can exploit the approval to transfer out the tokens from the Pool.The interesting thing about this way to exploit it is, that even if the delegate-call would be changed to a normal call, it would still be vulnerable! The flashLoan() function allows us to specify any address as a borrower and it would then make an arbitrary call to this address with the data we provided it. What if we specify the token as the borrower? What if we specify the approve function as data? The pool would make this external call and effectively the same as before would happen.But there's more! As mentioned previously, delegate-calling is as if the called function is part of the Pool. That also means we obtain access to its state variables:\nWe could replace the token in the inSecureumToken variable with some other worthless token that claims that the pool has the appropriate balance. We can just keep the real tokens!\nWe could toggle the _flashLoan variable and would be able to re-enter the Pool by calling deposit(). That way we can re-redeposit the loaned money as if it was ours and later withdraw() it too!\nBut we don't even have to bother with calling the deposit() function. Why not just directly change the balances variable and assign the entire Pool's funds as balance to us?\nExternal calls, and especially delegate-calls, are quite powerful and dangerous.","challenge-2-its-always-sunny-in-decentralized-exchanges#Challenge 2: it's always sunny in decentralized exchanges":"I bet you are familiar with decentralized exchanges: a magical place where one can exchange different tokens. InsecureDexLP is exactly that: a very insecure Uniswap-kind-of decentralized exchange. Recently, the $ISEC token got listed in this dex and can be traded against a not-so-popular token called $SET.\nThe dex has an initial liquidity for our pair of interest, which can be increased by anyone through token deposits.\nAdding liquidity to the dex, rewards liquidity pool tokens (LP tokens) which can be redeemed in any moment for the original funds.\nWill you be able to drain all of InsecureDexLP's $ISEC/$SET liquidity?\nThings certainly seem to get harder now! Let's start by looking at the success conditions:\nIt seems that, to beat this challenge, the InsecureDexLP needs to be completely drained and the player account should have 10 ether (10e18) worth of each token in the end.The challenge set-up looks complicated but all that happens is minting these tokens and then adding 9 of each as liquidity to the DEX and sending the rest to the player.The $ISEC Token is same one from the previous challenge, which used OpenZeppelin's contracts. The $SET Token on the other hand appears to be using a custom implementation called SimpleERC223Token.ERC223? It basically adjusts the ERC20 standard adding new features while staying backwards compatible, most importantly:\nTokens can no longer be accidentially sent to contracts that don't know how to handle them, locking them within the contract forever.\nRecipient contracts are able to react to receiving tokens by implementing a tokenFallback() function, basically a hook that is called immediately after the token balances were updated.\nIf you're familiar with common implementations of ERC20 Tokens, you might wonder why this apparently wasn't widely adopted, and the explanation for this is likely part of the solution...Let's move on and take a look at InsecureDexLP which is the actual target of this challenge. Now, this might look a bit overwhelming at first, but remember, the goal is to transfer-out all of the liquidity from this contract, not a complete code review.There's only one place in this contract where both tokens are transferred to the msg.sender, the removeLiquidity() function, making it a prime target:\nFirst, it checks that our liqidity balance is sufficient for the requested amount to withdraw.\nThen, it calculates the actual amounts for each token to withdraw.\nNext, the tokens are transferred to the msg.sender.\nFinally, the liquidity balance is updated, so on the next call we won't be able to withdraw what we have already withdrawn.\nUnder normal ERC20-circumstances, this shouldn't cause any issues, but here, one of the tokens is an ERC223 allowing a smart contract recipient to react upon receiving the Tokens. And it can do so before the liqudity balance is updated.This means we can re-enter into the InsecureDexLP and remove the same liquidity once more. And then, once more and once more?\nYou might still be struggling to grasp the concept of re-entrancy, in that case this call stack might help:\nremoveLiquidity: Balance check\nremoveLiquidity: Calculate withdrawal amounts\nremoveLiquidity: Transfer tokens\nSimpleERC223Token's transfer() function calls the recipient's tokenFallback()\nremoveLiquidity: Balance check\nremoveLiquidity: Calculate withdrawal amounts\nremoveLiquidity: Transfer tokens\nSimpleERC223Token's transfer() function calls the recipient's tokenFallback()\nremoveLiquidity: Balance check\nremoveLiquidity: Calculate withdrawal amounts\nremoveLiquidity: Transfer tokens\nSimpleERC223Token's transfer() function calls the recipient's tokenFallback()\n...\nremoveLiquidity: Balance update\nremoveLiquidity: Balance update\nremoveLiquidity: Balance update\nAs you can see, the balance is actually updated multiple times, but only once the same liquidity has already been withdrawn multiple times via the tokenFallback() hook. Normally the second balance update would revert though since the integer would underflow, but because the update is within an unchecked-block the integer will instead wrap around to the highest possible integer.","challenge-3-borrow-hide-and-seek#Challenge 3: borrow, hide and seek":"Finally, as a conclusion to this not-so-secure ecosystem, the Secureum team built the BorrowSystemInsecureOracle lending platform where one can borrow and loan $ISEC and BoringToken ($BOR). Both tokens can be borrowed by either providing themselves or the other token as collateral.\nThe dex has an initial amount of funds that can be borrowed from.\nUsers can add collateral and take loans from BorrowSystemInsecureOracle.\nUsers may also get liquidated.\nBorrowSystemInsecureOracle uses the InsecureDexLP to compute the $ISEC/$BOR price.\nWill you be able to drain BorrowSystemInsecureOracle's $ISEC/$BOR liquidity?\nHere we have the usual $ISEC and a similarly boring $BOR token, both making use of OpenZeppelin's ERC20 implementation. The DEX and even the Flash Loan lending pool from the previous Challenges are making a comeback - this time though, we're supposed to exploit BorrowSystemInsecureOracle contract. It appears we can take overcollateralized loans from this contract - or at least that's what's supposed to happen...Before getting ahead of ourselves let's look at the challenge setup:\n30.000 $ISEC and 20.000 $BOR are minted\n100 of each token are added as liqudity to InsecureDexLP\n10.000 $ISEC are added to InSecureumLenderPool\n10.000 of each token are added to BorrowSystemInsecureOracle\nThe success conditions are simple: Drain all $ISEC tokens from BorrowSystemInsecureOracle\nTo summarize: We can borrow one token while locking the other as collateral. How much we can borrow depends on the value of the collateral. The value of the collateral is based on the price that the Tokens are currently being traded with in the DEX.The InsecureDexLP is a Constant Function Market Maker, meaning the price of the Tokens is based on their supply in the DEX. So if there's a lot of one Token and very little of the other, the prices are going to be extremely skewed - that means, market participants who have a lot of Tokens can use this to manipulate prices. We don't have many Tokens, but we have an InSecureumLenderPool that provides us with uncollateralized and free flash loans!\nThis challenge was a lesson on why to never trust DEX's current prices as a price feed oracle!"}},"/posts/2023/1/3/race-13-of-the-secureum-bootcamp-epoch":{"title":"RACE #13 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-13, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors.\nThis one was designed by Secureum Mentor Leo Alt who spiced things up with some assembly and function types. I imagine it was very hard to solve all 8 questions within the strict timelimit of 16 minutes, but I recommend you‚Äôll try to see how far you come with using more time before looking at the solutions.As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nJanuary 3, 2023 by patrickd","code#Code":"All 8 questions in this RACE are based on the following contract. You will see them for all the 8 questions in this RACE. The questions are below the shown contract.","question-1-of-8#Question 1 of 8":"In transferFrom(), unchecked is not used in the allowance subtraction:\n A. To save gas \n B. To avoid unauthorized transfers \n C. To avoid reentrancy \n D. None of the above \nCorrect is B.The way this question is asked is quite tricky. First of all, you have to correctly identify the line of code where the allowance subtraction happens (allowed - amount) and notice that unchecked is not being used there to be able to correctly interpret the question.Next, you should ask: What would happen if the unchecked block was actually used? Then Solidity's overflow-checks would be omitted, which would mean less bytecode and therefore gas-savings. Therefore, not using unchecked needs more gas, and answer A cannot be true.Solidity's overflow-checks will make the code revert if the amount is larger than the allowed value. This will ensure that callers are only able to send the amount that they were authorized to handle. Therefore if the unchecked block would have been used and there is no other place that checks this, it would have enabled unauthorized transfers making answer B true.Whether an unchecked block is used or not has nothing to do with reentrancy, therefore answer C is false.","question-2-of-8#Question 2 of 8":"In transfer() and transferFrom(), the use of unchecked would not be desired:\n A. When the token uses large number of decimals \n B. When the token uses small number of decimals \n C. At all times \n D. At no times \nCorrect is D.Originally, the correct answer was intended to be A with the following explanation from Leo: \"Unchecked would be always desired if this operation can never overflow, and never desired if it can easily overflow. Neither case is true. The number of decimals can influence that. The more decimals the token uses, the bigger one's balance is, in terms of tokens. Depending on how large decimals is, it can lead to the case where overflow is possible in a legit use case. In this case, unchecked could lead to bugs.\"This assumed though, that the operation can overflow, which it actually can't in the context of this contract since, as mentioned, by the code-comment \"Cannot overflow because the sum of all user balances can't exceed the max uint256 value.\" thanks to the _mint() function's totalSupply increase not being in an unchecked block.Although the use of large decimals doesn't negatively impact the token's own logic, it should still be mentioned that it might cause issues for other contracts that would like to integrate the tokens. An example for this would be the multiplication of two user's balances where, as per best practice, multiplication would happen before division (to avoid loss of precision) and might cause an overflow.","question-3-of-8#Question 3 of 8":"In name() and symbol(), the returned values are incorrect because:\n A. The string encoding is too short \n B. Inline assembly return does not leave the function \n C. MSTORE does not fill all bytes until 0x5f and function may return junk at the end \n D. The code always reverts \nCorrect is C.The first MSTORE writes a static 0x20 (32 in decimal) to the first memory slot. This is done since the RETURNed value is expected to be ABI-encoded and the encoding of dynamic types such as string is an offset that points to where the actual string in the return data starts. Since there's no other data being returned, it simply points to the next 32-byte slot starting at address 0x20.The second MSTORE actually writes two things at the same time: The string length and the actual string contents. Strings always consist of these two parts and it's important to remember that the length is right-aligned (like all numbers in memory) in the first slot, while the string's content is left-aligned (like all strings and dynamic byte-arrays in memory) in the following slots.Also, remember that MSTORE always writes 32 bytes of data starting at the specified address even when the data you specified is shorter than 32 bytes. In such cases, the data will be left-padded to automatically right-align it like a number. Both functions make use of this fact by not starting at 0x20 but instead increasing the address by the length of the string. That way the first byte of the string in the code will end up at the last byte of the slot while the following bytes are moved to the beginning of the next slot.While this seems like an elegant approach, the problem now is that only the first few bytes (the string contents) of the third slot have been written to. All of the following bytes are simply assumed to be zero-values although they might contain junk which could cause issues for the receiver of the returned data. In this specific case, the code accesses the memory area where Solidity stores the \"free memory pointer\". This value is returned as part of the string, which is basically junk.Finally, the RETURN operation in inline assembly not only leaves the function, but stops the execution of the transaction. It's is told to return 0x60 (3x32) bytes starting at offset 0x0, effectively returning all of the memory slots that had been written to:","question-4-of-8#Question 4 of 8":"To correct name(), one could make the following change(s): A.\n B.\n C.\n D. None of the above\nCorrect is A, B, C.Answer A moves the memory used for the return value by one 32-byte slot \"to the right\". This way the string won't end up sharing its space with where Solidity stores the \"free memory pointer\" and no junk will be returned anymore, at least in this specific case. It still clashes with other reserved memory areas of Solidity, but since they are zero in this case it doesn't matter. The fact that reserved memory areas are used also is no problem as long as control is never returned back to Solidity, as is the case here thanks to the use of RETURN.Answers B and C are basically equivalent and represent the usual ways one would do it in Solidity.","question-5-of-8#Question 5 of 8":"The concern(s) with the check in notify() is/are:\n A. Selector 0x00000000 is the fallback function \n B. Selector 0x00000000 is the receive function \n C. Selector 0x00000000 is possible in which case a valid callback would not be called \n D. Selector can never be 0x00000000, so the check is useless \nCorrect is C.A and B are simply not true, as fallback and receive don't have selectors. Rather, they are called when no function with the requested selector was implemented in the called contract.Answer C is regarding the possibility that, although unlikely, a normal external function could end up having 0x00000000 as function selector. In such a case, Solidity will revert (thinking that the variable was not properly initialized) when attempting to call said callback. Therefore a valid callback would not be called.","question-6-of-8#Question 6 of 8":"The concern(s) with the call in notify() is/are:\n A. The call always reverts \n B. The passed function pointer is internal and therefore not accessible via an external call \n C. One should always use try/catch in external calls \n D. The called contract may not have the called function selector thus falling through to fallback or reverting the transfer \nCorrect is D.The call should not revert in the success case.The function pointer is external, therefore it has an address and a selector, which are used to make an external call.One doesn't need to always use try/catch, which is true especially when they want a failure to bubble up.There are no checks that the selector exists in the given address when a callback is registered. Therefore both the address and selector may be wrong when this external call is made.","question-7-of-8#Question 7 of 8":"Potential change(s) to notify() to mitigate further security concern(s) is/are:\n A. Enforce the callback call to use delegatecall \n B. Enforce the callback call to use staticcall \n C. Send Ether to the called contract \n D. Make the call in inline assembly \nCorrect is B.The use of delegatecall would cause immediate exploits here.Enforcing staticcall instead could prevent unseen potential issues, since it wouldn't allow any state changes from that call. Note that this could of course be different from the intended behavior.","question-8-of-8#Question 8 of 8":"How can the contract be exploited for loss-of-funds via notify callback reentrancy?\n A. During the callback, while being the sender of a transfer, repeat the transfer \n B. During the callback, while being the recipient of a transfer, call transfer again in the token contract sending the tokens back to the original sender \n C. During the callback, while being the recipient of a transfer, burn the received tokens \n D. This cannot happen in this contract \nCorrect is D.In order to exploit the contract via callback reentrancy, it would need to have an incompletely updated state when the callbacks are called. But the contract follows the checks-effects-interactions pattern and both the subtraction from the sender as well as the addition to the receiver's balance have already happened when the calls are made."}},"/posts/2023/1/30/race-14-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #14 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-14, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors.\nThis one was designed by Secureum Mentors Joran Honig and Gon√ßalo from ConsenSys Diligence. I imagine it was very hard to solve all 8 questions within the strict timelimit of 16 minutes, but I recommend you‚Äôll try to see how far you come with using more time before looking at the solutions.As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nJanuary 30, 2023 by patrickd","code#Code":"Questions 1 & 2 are based on the below code snippet. You will see the same code snippet for these two questions. The questions are below the code snippet.","question-1-of-8#Question 1 of 8":"Lending platforms have a few options to configure when it comes to adding new tokens as collateral. For example, you‚Äôll have to set up an oracle for the price of the collateral, and you have to configure a margin requirement. The security concern with using the given collateral configuration is:\n A. The periodic fee parameter is static \n B. The collateral ratio of loans is too low \n C. USDC should not be used as collateral for loans \n D. None of the above \nCorrect is B.It's unknown whether the periodicFee paramter is static or not from the given code, but either way it wouldn't be a security issue.While it might seem okay to keep collateralisation of a stable coin at 100% it has a problem: The user can borrow at a price determined by an oracle without slippage. A smart arbitrageur would, after every trade on every other AMM, go in and trade until the price of the AMM is the same as the price reported by the oracle used by this lending platform. It would even be possible to sandwich oracle updates for profit. It's also risky to maintain collateralisation of positions.There's no general reason to not use USDC as collateral.","question-2-of-8#Question 2 of 8":"Assuming payFees() is periodically called by a function, which iteratively calls payFees() of all collateral contracts, the security concern(s) is/are:\n A. Collateral tokens can define their own fee rewards and have the protocol pay too much fees \n B. There could be many lenders \n C. payFees() might re-enter the contract, paying all fees again \n D. You can deposit at any point during the period \nCorrect is B, D.The collateral contracts are deployed via the lending platform which is assumed to be correctly implemented. Under this assumption it wouldn't be possible for one of them to be malicious in terms of reward distribution.There could be so many lenders to iterate that the gas necessary for the function to complete is too large preventing the function to be called, effectively causing a Denial of Service.The token in question is USDC, an ERC20 token that does not have hooks allowing a receiver to reenter the function. A good recommendation would be the use of safeTransferFrom(), but it's not necessarily required.Nothing prevents someone to game the reward system by sandwiching the payFees() call with a large deposit() and withdrawal(). The current reward system does not incentivice that depositors keep their money in the system for a long time.","question-3-of-8#Question 3 of 8":"The developers want to prevent people from accidentally sending ETH instead of WETH and have implemented a noETH modifier, as defined above, and annotated the deposit function with it. They have also not implemented a receive function. Which of the following statements is true?\n A. Developers can either use the modifier or achieve the same effect by omitting the payable keyword on deposit function \n B. Developers should use the modifier because it achieves a different effect from omitting the payable keyword on deposit function \n C. Developers should remove the modifier but achieve the required effect by omitting the payable keyword on deposit function \n D. None of the above \nCorrect is C.Omitting the payable keyword does not have the same effect. The modifier checks the contract's current balance while the payable keyword checks the actual ether that was sent with the message (msg.value).The developers should NOT use the modifier because of this difference since it will lead to Denial of Service once the contract's balance becomes non-zero (it's possible forcefully injecting ether into a contract, eg. via SELFDESTRUCT).","question-4-of-8#Question 4 of 8":"Developers have used assembly to make their code a bit less repetitive. They use it to annotate a bunch of functions that have as their last argument a pool address. Unfortunately they made a mistake. Which of the following options fixes the bug?\n A. Replace the modifier with require(isValid(pool)); in every function with the modifier \n B. Make all functions using checkedPool external \n C. Everything is fine; this code has no problems \n D. None of the above \nCorrect is A.Dropping the modifier in favor of a single require() fixes the issue and even improves code readability to what was attempted here with assembly. If the developers insist on the use of modifiers they should pass the pool value directly as an argument to it instead: checkedPool(pool).Making all of the function external (therefore preventing them to be called internally) would certainly increase the likelihood that the assembly snippet will pick up the correct value. But there's still no guarantee that the last 32 bytes of the calldata are always the pool argument. A caller can send more data than defined in the function's signature and the ABI decoding would work just fine while ignoring the extra data. But the modifier would load the extra data instead of the pool value. This would potentially allow to bypass the pool validation and allow for exploitation.As mentioned, the code snippet is certainly not fine. It attempts loading the pool parameter from the calldata and assumes that it would always be located in the last 32 bytes - there's no guarantee for that being the case.\nQuestions 5 & 6 are based on the below code snippet. You will see the same code snippet for these two questions. The questions are below the code snippet.","question-5-of-8#Question 5 of 8":"The lending protocol has also built in a liquidation function to be called in the case of under-collateralization. Anyone can call the function and be rewarded for calling it by taking a small percentage of the liquidation. The liquidation function has a vulnerability which can be exploited because:\n A. The lender can open a position with a low amount of collateral and the fee payout reverts \n B. The lender can make the position ‚Äúunliquidatable‚Äù with reentrancy \n C. The lender can liquidate other positions with his callback and make more money \n D. The liquidator can take the full collateral amount with reentrancy \nCorrect is B.At first the code might look like the checks-effects-interactions pattern is followed (ie. no harmful reentrancy is possible) but there's actually another check happening when the Liquidation event is emitted: If the current balance is suddenly larger than the oldDeposit. Since the lender is called before this check happens, they could reenter via the deposit() function and increase the balance causing the liquidation to fail (ie. potentially making the position ‚Äúunliquidatable‚Äù).Aside from answer B, the other options are mostly nonsensical decoys.","question-6-of-8#Question 6 of 8":"Assume that the vulnerability referenced in the previous question has been fixed by changing the line with the Liquidation event emission to emit Liquidation(lender, oldDeposit). The protocol team has built a bot that monitors and liquidates under-collateralized positions automatically. Even though the bot does not monitor the mempool, it simulates the full transaction and, if successful, sends transactions with the exact amount to be able to execute the function + 100000 gas for minimal execution in the onLiquidation() callback. Which of these attacks can be executed in a harmful way towards the protocol?\n A. An attacker can liquidate positions, reenter the contract and steal tokens \n B. The liquidated lender can monitor the mempool and frontrun the protocol bot with a deposit, griefing it \n C. The lender can make their position ‚Äúunliquidatable‚Äù by consuming all the gas provided in the callback \n D. The liquidated lender can tokenize the extra gas in the callback and make a profit \nCorrect is B.First option is nonsensical.Although the revert via reentrancy in the deposit() function has been fixed, it's still possible that the bot is simply frontrun by a lender who increases their collateral via deposit() just in time to make the bot's liquidation attempt fail.A liquidated lender could, when called, only use up all of the gas it was given via the CALL, which is  of the total gas. With a sufficiently high gas limit it is still possible to liquidate the lender.Gas tokenization is no longer viable since EIP-3529 has reduced refunds.","question-7-of-8#Question 7 of 8":"In the context of Questions 5 and 6, someone built a MEV frontrunner bot that is exploiting liquidations in different protocols. It monitors the mempool for collateral contracts deployed from the lending factory and simulates transactions in a mainnet fork within Foundry to check whether it should attack them. The logic behind the bot is that it checks only the token‚Äôs ‚ÄúTransfer‚Äù events for its success conditions. More precisely, it checks if there is liquidity in an AMM to exchange to ETH and make sure it turns a profit at the end. If so, it sends a Flashbot bundle to make a profit by frontrunning the liquidator. Knowing the factory for this new contract is permissionless, how could you extract assets out of this bot?\n A. Open a position with a low collateral amount to grief the bot \n B. Build a similar bot that frontruns this one \n C. Deploy a collateral contract with your own custom token and seed an AMM pool with some ETH and this token, tricking the bot \n D. There is no way to do it \nCorrect is C.Griefing the bot would not extract assets from it.Option C actually happened on mainnet and was labelled \"Salmonella Attack\"Since it's using Flashbot Bundles, frontrunning it should not be possible since the transaction would be private.","question-8-of-8#Question 8 of 8":"The protocol implemented a function to transfer collateral from lender A to lender B with a signature from A, as shown above. Is there a way you can break it?\n A. Lender B can get more than the intended amount from lender A (assuming there is more than double the amount in A‚Äôs account) \n B. Lender A can pretend to transfer to lender B but then steal amount from him \n C. Lender A can grief lender B by sending a malformed signature (assuming the S parameter is correct) \n D. Lender B can steal from another lender C, by submitting a malformed signature. \nCorrect is A.There's nothing preventing the same signature to be re-used over and over again. So as long as the signer has a balance higher than the signed amount, the receiver can transfer their collateral.Even if the code would keep track of the signatures that were already used once, the code would still be exploitable via signature malleability. A correct way to fix it would be to track a nonce for each signer.A malformed signature would cause ecrecover to return a zero-address, but is prevented since the return value must be equal to the signer who may not be the zero-address."}},"/posts/2023/10/17/cryptocurrency-privacy-technologies-borromean-ring-signatures":{"title":"Cryptocurrency Privacy Technologies: Borromean Ring Signatures","data":{"":"October 18, 2023 by patrickd\nDespite being regularly referred to as \"anonymous Internet money\", the ledgers of the most widely adopted cryptocurrencies are completely public. Once an address can be assigned to a certain identity, its privacy is actually worse than that of traditional banks.Building up to technologies that enable private cryptocurrencies, the previous article focused on the first version of the Ring Signature primitive. This article investigates a more modern variation called Borromean Ring Signatures, which is one of the main building blocks of Bitcoin's Confidential Transactions proposal. Borromean Rings, in case you're unfamiliar, refer to loops that are linked to each other as if they are a knot; But as soon as a single Ring is removed, the entire structure falls apart. You'll see that this metaphor is indeed fitting...\nNote that this blog post assumes you have already read the previous article, or are already familiar with RSA-based Ring Signatures.","the-concept#The Concept":"The paper on Borromean Ring Signatures was published in 2015, but not as a direct offspring from the original method. At this point in time, Elliptic Curve Cryptography had finally established itself, and with Bitcoin using ECDSA, the original paper with its basis in the Integer Factorization Problem was no longer a viable option. Instead, the Borromean Ring Signature is built on the 2002 paper \"1-out-of-n Signatures from a Variety of Keys\" which, as the title implies, allowed building rings with various public key schemes including those based on the Discrete Logarithm.","accommodating-for-ecdlp#Accommodating for ECDLP":"As before, we're still dealing with a list of public keys representing the ring's members, one of which signed the message . Only this time, keys are based on ECDLP, meaning that a public key  is a point calculated by \"multiplying\" a secret scalar  with a generator point  (). Review the previous ECC article in case you need a refresher. In a ring of  members, each point  contributes to the manipulation of the glue value , starting from the initial value . This initial value, like the seemingly random scalar values  for each member, are part of the signature .Although structurally the algorithm can still be represented as a ring, you can see that, what was previously referred to as the \"Combining Function\", has changed quite a bit. There's no longer RSA-based asynchronous encryption, synchronous encryption, and mixing of values with XOR going on; in a way, it looks a lot more simple and elegant.The glue value  is always a hash, being the result of the previous member's contribution. This creates a circular dependency between all members, and with the unpredictability of a hash function's output, constructing a valid ring should be impossible. Naturally, here too there is a trick to it where, when a secret key  is known, it's possible to pick a  that will \"close the loop\". The trick is called: Chameleon Hashes.","ring-of-rings#Ring of Rings":"However, before delving deeper, there's still an important fact to mention about Borromean Ring Signatures: They can encompass multiple Rings, and all rings are connected through the same initial value . Similar to their real-world counterpart, if one of these loops doesn't hold, the entire signature is considered invalid.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThis feature is also the reason for the ring's last hash  not simply being the same as . To connect multiple rings through the same initialization value,  is built from the last points   of each ring.Now, what does having multiple rings mean in practice? A Ring Signature with a single ring can be understood as a \"signature of knowledge\" about at least one secret scalar .\"I know \"Borromean Ring Signatures offer an efficient way to have multiple rings covered by a single signature, turning the logic into:\"I know \n\"Each ring, denoted as , may have a varying number, , of members. A signer can only build a valid signature if he knows at least one secret scalar  in each group of public keys .The usefulness of this feature will become especially apparent when looking at the Confidential Transactions proposal of Bitcoin. There, this is used to prove that the transaction value is within the allowed range without revealing the actual value.","the-math#The Math":"","chameleon-hashes#Chameleon Hashes":"There is various research discussing Chameleon Hashes and Signatures created using them. For simplicity, we'll solely concentrate on their use in the context of Borromean Ring Signatures.Looking back at the verification process of a simple ring, you can see that the hash  of the previous step influences the resulting hash  for the next:Here, a Chameleon Hash will allow us to completely disregard  at the beginning, while still being able to produce the output hash . And most importantly, later, we'll be able to add  back in as a factor without causing any changes to  at all.To do so, we'll first choose a random scalar  as a placeholder:Using the resulting hash  as the algorithm's starting point, we can keep stepping forward until we make a full circle back to where we started. Thanks to this, we know now the  hash we'll be passed.If we're aware of the private scalar  belonging to the point  we can now calculate a fitting  (instead of using a random one):To prove that this is indeed the case is quite trivial. The basic claim is that the point resulting from  can also be reached from  when we can add another point  to it. But this is only possible with knowledge of :Now we substitute  and `P_j`:As shown, this method can be used to go \"back in time\" to produce the  required to close the loop.But without knowing  we'd have to break DLP () in order to solve for But knowing  solving for  is simple:Please note that this explanation has omitted many related facts around Schnorr authentication, random oracles, and their use for the Fiat-Shamir transformation. These are not essential for understanding Chameleon Hashes within Borromean Ring Signatures but they do shed light on the origin of this idea.","hashing-points-and-data-to-a-domain#Hashing Points and Data to a Domain":"As you might remember from the RSA-based Ring Signatures, a special wrapper function  was required around the permutation trapdoor function . This ensured that the entire ring shared a common domain, despite each RSA key having a different cyclic group due to all of them having their own modulo .\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nIn ECC, at least assuming that all members are using the same curve with the same parameters, this is much less of a problem. We can simply hash all of the data and the coordinates of a point while just having to make sure that the output of the hashing function will then be a valid scalar within the cyclic group of the globally chosen generator , ie. divided  with  being the order of the group.","the-code#The Code":"The algorithm for the actual creation of Borromean Ring Signatures is quite complex, so I omitted a detailed description of it and concentrated on explaining the essential concepts that enable it above. I attempted implementing it exactly as described in the paper at first but I was unable to get it working doing so. I \"corrected\" several things that looked like off-by-one and sign (+/-) errors in the paper and these changes appear to have resolved the issues. Enjoy with caution!Since I was considering writing a compatible implementation for validating these Ring Signatures on the EVM, I based the hashing on ABI-encoding and keccak256.\nThe fact that Borromean Ring Signatures can handle multiple rings simultaneously makes the code quite complex. The defaultdict lib ended up being a big help simplifying the setup of multidimensional lists. To gain a better understanding you might want to tinker with the section marked as \"Playground\". Consider changing the sign() function's parameters to specify a single ring with a single member (a signer) at first, and expand from there."}},"/posts/2023/10/21/exploiting-ec-recover-for-efficient-borromean-ring-signatures":{"title":"Exploiting EC-Recover For Efficient Borromean Ring Signatures","data":{"":"October 21, 2023 by patrickd\nAttempting to implement the Borromean Ring Signature algorithm with Schnorr for the EVM results in rather costly transactions. However, the EVM has one pre-compile contract available that executes several point addition and multiplication operations on the classical secp256k1n curve for a very cheap price: . This article shows how it can not only be used to validate an ECDSA signature but also, how one may use it as a gadget for other use cases requiring multiple EC addition and multiplication operations.","the-math#The Math":"Vitalik actually described this technique in 2018 and hinted that it could indeed be used for things like ring signatures.","ecdsa-public-key-recovery#ECDSA Public Key Recovery":"When verifying an ECDSA signature where the public key  (as ) of the signer is known, the following equation must hold for the signature to be valid:But, as you might know, the  pre-compile in the EVM is only passed\nThe  hash of the transaction/message ()\nThe signature components  and  which are used to derive  (as )\nThe signature component \nThe result is the address of the signer - which is a partial hash of public key . The validity is then based on whether the expected signer address is returned. If not, or if the address returned is the zero-address, the signature is considered invalid.So obviously, it must be possible to rearrange the equation and solve for :Therefore  basically takes in two scalars , , and a point  and returns  which is the (partial) hash of a resulting point from handling the input variables.","exploiting-ec-recover#Exploiting EC-Recover":"This doesn't seem too dissimilar from what is happening in a single forward-step in the Borromean Ring Signature algorithm:So can't we then exploit this precompile to implement a more efficient EVM-based Borromean Ring Signature validation by\nUsing the input point  as  (public key of a ring member, with the signer's private key being  for ) by  being the x-coordinate of  and  to derive the correct y-coordinate of \nUsing what is expected to be the hashed transaction/message as  component for the ring\nUsing the ECDSA's  component as input hash  of the ring\nMixing in the message  by hashing it with the resulting address","chameleon-hashes-within-ec-recover#Chameleon Hashes within EC-Recover":"As before, the signer of the Ring Signature must be able to make use of a Chameleon Hash in order to determine  before knowing what  will be.We can follow the original pattern of choosing a random scalar . Thankfully, we already know the signer's public key point , therefore we also know .After using this to step through the entire ring, we will know what the input  will be. Therefore we can replace  by solving for :This resulting  of the signer should then be indistinguishable from the randomly chosen  values of the other ring members.","the-code#The Code":"","off-chain-signing#Off-Chain Signing":"","on-chain-verification#On-Chain Verification":""}},"/posts/2023/10/3/race-22-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #22 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-22, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. It was designed by the legendary Secureum Mentor Tincho, creator of Damn Vulnerable DeFi and founder of The Red Guild.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nOctober 3, 2023 by patrickd","question-1-of-8#Question 1 of 8":"Select the true statement(s) about the above contract:\n A. It's impossible for the hasETH function to ever return true\n B. The contract's ETH balance may increase if the deployer sends ETH during the deployment\n C. The contract's ETH balance can increase if it's the target of a SELFDESTRUCT opcode\n D. The contract's ETH balance can increase if it's the target of a beacon chain withdrawal\nCorrect is C, D.\nB: Solidity contracts require a payable constructor in order to be able to receive ETH during deployment. Attempting to send anything will revert.\nC: It's always possible to inject ETH balance into a deployed contract by specifying it as the receiver address when self-destructing another contract as this won't invoke the receiver's bytecode and doesn't give it a chance to reject.\nD: Beacon chain withdrawals happen at protocol level outside of the EVM. They are gas-free and will not invoke the contract's bytecode.\nA: As it's possible for the contract to receive ETH after all, the hasETH function may return true under the mentioned circumstances.","question-2-of-8#Question 2 of 8":"Select the true statement(s) about the above contract:\n A. The ReceiveExecuted event is emitted when ETH is sent in a call, regardless of the length of the call‚Äôs calldata\n B. The FallbackExecuted event is emitted when ETH is sent in a call and the call‚Äôs calldata is not empty\n C. The FallbackExecuted event is emitted when no ETH is sent in a call and the call‚Äôs calldata is empty\n D. The ReceiveExecuted event is emitted when ETH is sent in a call and the call‚Äôs calldata starts with 0xa3e76c0f (the function signature of receive())\n E. The receive function only has 2300 gas available, regardless of how much the caller has sent\nCorrect is B.\nA: If the calldata isn't empty, receive is not executed.\nB: The fallback is executed as a last resort function when none of the others match. In this case, the receive function is not executed because the calldata is not empty, so execution goes to the fallback.\nC: In this case the receive function is executed.\nD: If the calldata is not empty, the receive is not executed. Also note that both fallback and receive are not like external/public functions. They have no function signature.\nE: Is false because receive doesn't limit the amount of gas available. Gas may be limited when the contract was called via transfer() though, for example.","question-3-of-8#Question 3 of 8":"In the above contract, what are some safety checks automatically included by the Solidity compiler?\n A. Panic if the call has value greater than zero\n B. Panic if the calldata's size is not larger than 4 bytes\n C. Panic if the calldata's size is larger than 68 bytes\n D. Panic if the first parameter cannot be ABI-decoded to a uint8 type\n E. Panic if the second parameter cannot be ABI-decoded to a uint64 type\nCorrect is A, B, D, E.\nA: To functions that are not marked as payable, Solidity adds bytecode for ensuring that the msg.value sent is indeed zero.\nB: If the calldata's size is exactly 4 bytes, it may contain the foo function's correct signature but it would error since the calldata is not of sufficient length to attempt decoding the function's parameters from. If the calldata sent does not map to an existing function, or is shorter than 4 bytes, it would error too since there's no fallback or receive functions to handle such a case.\nC: Solidity's ABI decoder will ignore any additional data sent that it doesn't need. If the foo function was being called, it would not error if the calldata size is larger than the function's signature and its expected parameter data.\nD/E: Solidity will indeed revert if parameter values are sent that do not fit within the mentioned types (ie. overflow while down-casting).\nTo verify these, you can compile the contract (solc --ir Example.sol), read the Yul output, and see the actual checks included by the compiler. Here's the Yul output. Tincho has marked the relevant lines with the comment //QUIZZ so you can find them.","question-4-of-8#Question 4 of 8":"Which of the statement(s) is/are true about the above contract?\n A. Due to the use of unchecked, out-of-bound access protections are disabled. So when the index parameter is greater than 300, execution does not revert\n B. Due to the use of unchecked, when b is 0, a / b does not revert and results in 0\n C. The use of unchecked eliminates the compiler‚Äôs automatic overflow check when casting the result of a / b to uint8\n D. None of the above\nCorrect is D.\nA: Unchecked-blocks do not disable out-of-bound access checks in Solidity. You can verify this by copying the contract in Remix, calling the function with an index of 299 and checking that the tx doesn't revert. Then do the same, but with an index of 300, and the tx reverts.\nB: Unchecked-blocks do not cause division-through-zero occurrences to be ignored. Easy to verify in Remix too.\nC: Unsafe downcasting will indeed not revert, but not because of the use of unchecked, but because the down-cast is made explicitly. If you remove the unchecked block, and the result of  is greater than 255, execution still succeeds (silently overflowing the result). Again, you can check this in Remix.","question-5-of-8#Question 5 of 8":"Which of the statement(s) is/are true about the above contract when trying to compile using solc 0.8.17 without optimizations?\n A. Compilation fails because callvalue() is used but the function is not payable\n B. Compilation fails because the assembly block is marked ‚Äúmemory safe‚Äù but memory can potentially be read and written\n C. Compilation succeeds, although the compiler emits a warning due to the unused return value of call\n D. Compilation succeeds without any warnings\n E. None of the above\nCorrect is E.\nA: A function not being payable simply means that there's no check whether value was sent when the function is called. But checking callvalue/msg.value is still allowed.\nB: The ‚Äúmemory safe‚Äù flag merely tells the compiler that it may rely on the assembly block respecting Solidity's memory layout and therefore being able to apply certain optimizations. Compilation won't fail from the assembly-block reading or writing memory in any way.\nC/D: Doesn't compile because the payload variable is a data element and can't be accessed like that. Instead one has to use its .offset or .length attributes. A second compilation error is that call returns a value which is not used, and it needs to be either assigned or discarded.","question-6-of-8#Question 6 of 8":"A developer does some minor changes on the previous contract, resulting in the above contract. Which of the statement(s) is/are true when calling callAndRevert?\n A. The transaction reverts before the external call if the bytes in the payload parameter are not properly ABI-encoded\n B. The transaction reverts before the final revert operation when target is an account without code\n C. The transaction reverts before the final revert operation when execution in the target account reverts\n D. The transaction reverts before the final revert operation when the callee runs out of gas\n E. None of the above\nCorrect is E.\nA: The payloads contents are not checked for their ABI-compatibility before the call is made as it may be non-ABI encoded contents that the target is expecting. It's simply that bytes could be anything, it may be something ABI-encoded, it may be something completely different, like the binary data of an image.\nB: No, CALLing a contract without any code directly will always succeed. This assembly-block skips the check contract-size check that Solidity would do before attempting to call the target.\nC: If the execution in the target account reverts, this won't revert the entire transaction but only the actions done within the context of that CALL. Whether a revert happened can be known by checking the boolean value returned by executing the CALL opcode. The revert is not bubbled-up to the caller context, so execution continues.\nD: If the callee runs out of gas, the revert is again not bubbled-up. Furthermore, one 64th of gas is always put aside for the caller. Even if the callee uses up all of the gas they had available, the caller should still have some gas left to execute a few more operations and it may be enough to finish the transaction without reverting.","question-7-of-8#Question 7 of 8":"Continuing with the same contract, what are the consequences of annotating the assembly block as ‚Äúmemory-safe‚Äù ?\n A. It‚Äôs a good practice to help auditors, and never affects the compiler‚Äôs behavior\n B. The bytes in payload are checked to be ABI-encoded before storing them in memory\n C. returndatacopy will revert if returndatasize is greater than zero, due to writing to Solidity‚Äôs reserved memory space\n D. Return bomb attacks are prevented due to safety checks introduced by the compiler on the size of the returned data copied to memory\n E. None of the above\nCorrect is E.\nA: No, it's a flag telling communicating to the compiler that the assembly-block respect's Solidity's memory layout and it affects the compiler's use of optimizations.\nB: No, since bytes are allowed to be any sort of value and are not necessarily always ABI-encoded. They are simply copied into memory without checks until an attempt to actually decode them is made.\nC: No, writing into Solidity's reserved memory space will not cause a revert. There's no code checking this and the EVM is unaware of Solidity's expectations around memory. There's nothing preventing this.\nD: Solidity does not add any safety checks to assembly-blocks. The responsibility lies with the author.","question-8-of-8#Question 8 of 8":"Alice and Bob have the exact same Solidity contract. Each one compiles the contract in their machines with the same compiler version and settings (e.g., running solc Example.sol ‚Äîbin) . Then they compare the resulting outputs. Which of the following statement(s) is/are true?\n A. The output is the same, because the contract and compiler version and settings are exactly the same\n B. The output is different, because Mercury and Venus are not aligned at the moment of compilation\n C. The output is different, because by default the bytecode includes extra non-executable bytes that depend on each one‚Äôs compilation environment\n D. The output is different, but they could use a compiler flag that would make solc produce the same outputs everywhere\nCorrect is C, D.By default, solc includes the metadata hash as part of the output bytecode. This hash is dependent on the compilation environment (for example, path and filename). Therefore the output will be different between Alice and Bob. The compiler flag that would solve the issue is --metadata-hash none, which removes the hash from the output.You can verify this with two equal contracts in the same directory with different filenames. Compile them as usual, and compare the bytecode, noting that it's different. Then compile again but with the flag, and you'll see the output now is the same.Lesson: Solidity is weird."}},"/posts/2023/12/17/race-24-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #24 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a mirror of a Write-Up on RACE-24, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. It was designed by Secureum Mentor Vahe (aka k3mmio), from hexens.The original version of this document can be found at https://docs.google.com/document/d/1PswXg-sc4Hy4YCGuTrwgZnXSEsfNZ_-rkiABw2ggOKc/editParticipants of this quiz had to answer 12 questions within the strict time limit of 48 minutes about Solidity Security, Basic Python, Basic Algorithms/Graphs and Basic SQL-like Querying. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!","question-1-of-12#Question 1 of 12":"Is there a possibility to create a contract at some specific deterministically chosen Ethereum address and change its state bytecode after the deployment (hence, have the same address but with different bytecode)?\n A. There is none because contract bytecode is immutable\n B. selfdestruct call can be considered as bytecode change because it zeroes it out\n C. Using a proxy pattern, one can change the bytecode at the deployed address\n D. A special constructor allows re-deploying at the same address after self-destructing\nCorrect Picks: B, D.One can use metamorphic contracts to deploy different bytecodes to the same address.","question-2-of-12#Question 2 of 12":"The code shows an example of a vulnerable implementation of the ERC20 permit() function. What are the possible high/critical severity bugs in the code?\n A. The digest calculation cannot stop some of the replay attack vectors\n B. Due to insufficient checks in the code, it allows anyone to drain zero (0x0000000000000000000000000000000000000000) address balance, e.g. in case the ERC20 token‚Äôs burn function actually sends the tokens to zero address\n C. The way signature nonce is checked is vulnerable to replay attacks\n D. Not all of the crucial security-essential parameters passed to the function are actually checked by the permit() function\nCorrect Picks: A, B, C, D.The code has all of the bugs mentioned, as per ecrecover() return value was not being checked for 0 address, expiry was not being checked, and also the S value of the signature is malleable (check ECDSA malleability).","question-3-of-12#Question 3 of 12":"The code shows a snippet of a Liquidity Provider (LP) token staking functionality. The protocol is designed to stake Uniswap V2 Liquidity Provider tokens (LP tokens). The rest of the code should be treated as black-box and without any bugs or other issues. Which of these are correct observations about the code?\n A. Given the scope of only the stakeLP function, reentrancy is possible which has a critical impact\n B. Given the scope of only the stakeLP function, reentrancy is technically possible, although with no real impact\n C. The safeTransferFrom() function is not implemented consistently allowing to drain LPs that are not yet created, but will be in future\n D. A malicious LP token can be used by the attacker to drain the balance of other conventional LP tokens\nCorrect Picks: B, C.safeTransferFrom has a check missing that contract has any code, calling this on empty accounts will return true with empty data and pass the required check, thus if one frontruns the LP token creation (on uniswap factory) and calls the stakeLP will be able to stake any amount of money","question-4-of-12#Question 4 of 12":"A. After a second \n B. After a day \n C. After 3 days \n D. After 5 days \nCorrect Picks: B.This is a rounding bug,  will return  but will start doing that only after one day has passed, as  will be equal to , and for the first day the calculation will (ironically) work correctly.","question-5-of-12#Question 5 of 12":"The code is a snippet of an example Vault contract. The contract allows Ether deposits and withdrawals, and also implements multicall functionality to aggregate multiple calls into one transaction. Which of these observations are correct about the code?\n A. Using multicall() in conjunction with withdrawal functions, an attacker can conduct reentrancy attacks draining all of the Ether from the contract. \n B. The functions withdraw() and withdrawTo() are prone to profitable reentrancy attacks on their own.  \n C. Using multicall() in conjunction with deposit functions, attackers can inflate their balance in order to further withdraw all of Ether from the contract. \n D. The delegatecall() can be abused by the attacker to execute arbitrary bytecode in the context of the Vault contract. \nCorrect Picks: C.This question had a flaw in the task code (missing ‚Äúpayable‚Äù for the multicall function), but this does not make the other 3 answers to be correct. The designed bug was that calling the deposit function in multicall loop will keep the msg.value the same for all the calls, thus amplifying the deposited amount.","question-6-of-12#Question 6 of 12":"In the code we have taken the Ethernaut‚Äôs King challenge and changed it so that no one should have the ability to lock him as the king by changing the transfer() call with a low-level call(). Which of these observations are correct about the code?\n A. No one can lock himself as the king as the low-level call() will not automatically revert, even if the king is a smart contract with a reverting fallback function.\n B. The transaction can be reverted with OOG in case the king returns a large enough amount of return data.\n C. A nonReentrant modifier is missing and as the low-level call is used, the king can reenter the setKing() call to stay as the king.\n D. By setting the success variable from the return tuple of low-level call(), the compiler automatically adds a check for it to be true, thus still making it possible for the king to lock himself by reverting the fallback function.\nCorrect Picks: B.A trick called return-bomb can be used to prevent anyone from changing ‚Äúthe king‚Äù. By returning a big amount of return data the setKing will revert while trying to copy the returned data. Interestingly, despite the code ignores the second return value of the tuple the compiler still adds a code to copy the returned bytes.","question-7-of-12#Question 7 of 12":"A. This code cannot be compiled because multiple inheritance is not allowed in Solidity. \n B. When calling the foo() method of contract C, the event with the value \"A\" will be emitted. \n C. When calling the foo() method of contract C, the event with the value \"B\" will be emitted. \n D. If contract C inherits first from contract B and then from contract A, then the emit event will be \"A\". \n E. If the override construct in the foo() method of contract C swaps contracts A and B, then the value of the event emitted will also change. \n F. None of the above  \nCorrect Picks: C, D.This task revolves around the multiple inheritance challenge in Solidity. To address it successfully, it's essential to understand that Solidity implements inheritance through the C3-linearization algorithm, where the last class from which the original class is inherited becomes the superclass.","question-8-of-12#Question 8 of 12":"A.  \n B.  \n C.  \n D.  \nCorrect Picks: D.This problem introduces a recursive algorithm for calculating the nth Tribonacci number with the use of memoization.","question-9-of-12#Question 9 of 12":"A. \"z\" in x[2] \n B. \"z\" in x[2][\"bar\"] \n C. 30 in x[2][\"bar\"] \n D. \"baz\" in x[2][\"bar\"] \n E. \"baz\" in x[2] \nCorrect Picks: B, E.This task demands comprehension of nested data structures and heightened attention. Importantly, note that when dealing with dictionaries, the in instruction checks keys, not values.","question-10-of-12#Question 10 of 12":"A. \n B. \n C. \n D. \nCorrect Picks: A, B, D.This problem necessitates an understanding of undirected graphs and familiarity with the Depth-First Search (DFS) algorithm. An undirected graph is a mathematical structure comprising vertices and edges, where the edges lack direction. In other words, the connection between any two vertices is mutual and doesn't imply a one-way relationship.DFS, an approach to graph or tree exploration, initiates from a designated vertex and systematically delves as far as possible along a chosen branch. This algorithm relies on either a stack or recursion to meticulously track vertices and their traversal order. The key objective is to reach the final vertex or uncover a complete connectivity component. Once a final vertex or target is encountered, DFS retraces its steps to explore alternative paths.","question-11-of-12#Question 11 of 12":"A Blockchain database consists of the following tables: Contracts, Functions, Modifiers, FunctionModifiers, and Instructions. Each table has the following schema:\nContracts: ContractID, ContractAddress, ContractName, CreationDate\nFunctions: FunctionID, ContractID, FunctionName\nModifiers: ModifierID, ModifierName, Description\nFunctionModifiers: FunctionID, ModifierID (This is a table to represent the many-to-many relationship between Functions and Modifiers.)\nInstructions: InstructionID, FunctionID, InstructionCode, LineNumber\nWrite an SQL query to list all contract addresses (ContractAddress) where the \"mint\" function is used without any modifiers. A.\n B.\n C.\n D.\nCorrect Picks: B.The query starts by joining the Contracts and Functions tables based on ContractID. It filters functions to those specifically named \"mint.\" The NOT EXISTS clause is used to check if there are no entries in the FunctionModifiers table for the corresponding FunctionID, indicating the absence of modifiers. If the subquery returns true (indicating no existence of modifiers), the contract address is selected.","question-12-of-12#Question 12 of 12":"A Blockchain database consists of the following tables: Contracts, Functions, Modifiers, and Instructions. Each table has the following schema:\nContracts: ContractID, ContractAddress, ContractName, CreationDate\nFunctions: FunctionID, ContractID, FunctionName\nModifiers: ModifierID, ModifierName, Description\nFunctionModifiers: FunctionID, ModifierID (This is a table to represent the many-to-many relationship between Functions and Modifiers.)\nInstructions: InstructionID, FunctionID, InstructionCode, LineNumber\nYou need to write an SQL query to list all contract addresses that use high-risk functions (like withdraw, updateBalance) without critical modifiers (onlyOwner, onlyAdmin, onlyManager) and also contain specific risky instruction patterns (like InstructionCode containing externalCall or directTransfer). A.\n B.\n C.\n D.\nCorrect Picks: B.The query joins Contracts and Functions tables based on ContractID. It filters functions to those named 'withdraw' or 'updateBalance'. The NOT EXISTS clause is used to exclude contracts with critical modifiers. Specifically, it checks if there's no entry in the FunctionModifiers table with critical modifiers for the corresponding FunctionID. If the subquery returns true (indicating no critical modifiers), the contract address is selected."}},"/posts/2023/10/31/cryptocurrency-privacy-technologies-confidential-transaction-values":{"title":"Cryptocurrency Privacy Technologies: Confidential Transaction Values","data":{"":"November 1, 2023 by patrickd\nDespite being regularly referred to as \"anonymous Internet money\", the ledgers of the most widely adopted cryptocurrencies are completely public. Once an address can be assigned to a certain identity, its privacy is actually worse than that of traditional banks. In this series, we explored various technologies that aim to improve this situation, and in this article, it's all coming together for a practical solution.Working through technologies in a historical manner, we started with a primer on why RSA works the way it does and made use of this knowledge to understand the homomorphic properties used in RSA-based Blind Signatures. Later, we introduced the idea of Ring Signatures which too were RSA-based in their original form. After a review on how to apply Elliptic Curve Cryptography we were able to explore a more sophisticated form of them called Borromean Ring Signatures and learned how they could be efficiently implemented in the EVM.In this article, we will discover the homomorphic encryption-like properties of Pedersen Commitments and how Borromean Ring Signatures can be used as zero-knowledge range-proofs for blinded values. Finally, a toy example shows how these technologies could even be used to make token transactions on Ethereum private while keeping the entire validation process on-chain.","the-concept#The Concept":"In 2015, Bitcoin developer Greg Maxwell published a paper titled Confidential Transactions, aiming to hide transaction amounts in a manner that created a manageable overhead to the Bitcoin system compared to other solutions at that time. This came as an improvement to his mixing protocol concept (CoinJoin) that obscured the transaction graph but had limited utility from the fact that it was possible to reveal connections by comparing the transaction amounts of inputs and outputs. He suggested hiding this value using a \"blinded commitment\" which has the special property that it can be homomorphically summed up with and compared to other commitments to prove that no more than the allowed amount of coins were spent.","utxo-traceability#UTXO Traceability":"While Ethereum uses the intuitive Account Model, Bitcoin appears a bit more complicated at first: Each time you \"send someone Bitcoin\", what you are actually doing is using some of your Unspent Transaction Outputs (UTXO) as input of a transaction that produces new UTXOs, one of which the receiver will be able to claim. Each UTXO comes with a \"Locking Script\" (ScriptPubKey) which dictates the condition under which it can be spent. When using a UTXO as a transaction's input, one must specify its \"Unlocking Script\" (ScriptSig), which typically is simply a signature of the UTXO's specified owner. If you have a UTXO that doesn't exactly match the amount you want to send another person, you'll have to split it apart, sending yourself the change in a separate UTXO. On the other hand, you might need to combine multiple inputs to create an output that is sufficiently large to cover the desired amount.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nOnce individuals can be associated with their pseudonymous Bitcoin addresses (eg. by KYC), tracking these user fund movements becomes rather trivial. One popular idea to improve on this has been the concept of \"mixing\", where various users' inputs are mixed, resulting in UTXOs to addresses that aren't associated with anyone yet. This makes it indeed a bit harder to track funds, but due to the amounts being visible, one can still make the connection by continuing to monitor later movements and using the process of elimination.With the introduction of confidential transaction amounts, it's still possible to track the range of values a UTXO might contain. However, this range expands when mixed with other UTXOs over time.","commitments-instead-of-amounts#Commitments instead of Amounts":"To implement this concept, we'll replace the plaintext amount in each UTXO with a \"commitment\". You may have heard about commitments as simple hashes of data to be revealed at a later point. If we were to hash a simple amount , it would be trivial to determine the committed value using brute force. Therefore, a complex random factor  is included, without which it is practically impossible to determine the committed data, making this a \"blinded commitment\".For the Bitcoin system to be able to verify the commitment now, we would have to reveal the committed data by publishing the secret blinding factor  with the amount . But that would defeat the entire point since we normally wouldn't want to reveal the transaction value to anyone but to the receiver. Instead, we need commitments that can be both summed up and compared to each other:Due to the random nature of a typical hashing function's output, we need something different here. We need something that produces commitments that are \"additively homomorphic\", a function .","the-math#The Math":"If you're familiar with Elliptic Curve Cryptography, you might already know that it can provide us with what we are looking for:\nWhen multiplying a scalar value, such as an amount , with a generator point  we will get a point  that reveals no information about the secret scalar as long as the discrete logarithmic problem in ECC (ECDLP) assumption hasn't been broken. In other words, we can easily calculate  but it's practically not possible to determine .\nWhen using points as commitments, we can sum points with each other and we're guaranteed to end up at the same additive point when all of the scalars make the same sum as well. Meaning that we can find the same point  by adding  as we would when adding  because .\nWith this being the case, we can add each point of the input and each point of the output and finally compare them. Doing so, we can check whether the sum of scalars of both sides is equal without knowing the scalars themselves:  with each point  when .","pedersen-commitment#Pedersen Commitment":"While the above indeed provides us with additively homomorphic commitments, they are no longer blinded and it would once again be trivial to guess the amount  by trial and error within a reasonable time. We need to introduce a secret blinding factor, but we need to be careful of how we go about it as we'd otherwise end up with a Chameleon Commitment:Here, a user could create a \"commitment\" for  but at a later point, they could declare that the commitment actually had been for  and there'd be no way to disprove this since both resulting points would be the same. To prevent this, a second generator point  is agreed upon, which has been chosen at random such that nobody knows  in . This is important since otherwise  could simply be substituted for , bringing us back to the chameleon situation.In order to choose a provably random generator point , such that it's impossible for anyone to know the  relationship, the paper suggests hashing the encoded point G and converting the resulting value back to the closest point, .H = to_point(hash(enode(G))){:python}The above construct is referred to as a Pedersen Commitment and it allows us to maintain the homomorphic properties of point addition while also blinding the small amount  with a large, hard to guess, scalar .","comparing-pedersen-commitments#Comparing Pedersen Commitments":"But it's not only the homomorphic addition of the amount  that plays a role now. It's also the sums of the blinding values  that have to add up. Thereforeonly holds if  AND This isn't that much of an issue though: Let's say Bob has UTXOs that he'd like to use to send BTC to Eve. Naturally, in order to make use of these UTXOs he needs to be aware of the amount that they are committed to. When Bob received a UTXO from Alice, the only way for him to verify the amount that he had received would have been by revealing it using the blinding factor which Alice would have needed to send him via a secure channel (eg. by encrypting it using Bob's public key). Therefore, we can assume that Bob knows all of the secret scalars  of the input UTXOs and also their sum. For the outputs of the transaction to Eve, he could now choose all but one blinding factor  at random. The last factor  can be chosen such that the sums of both input and output commitments for both amounts and blinding factors will be equal.","negative-counterparts-due-to-inverse-elements#Negative counterparts due to Inverse Elements":"Elliptic Curve Cryptography works thanks to cyclical groups, but with cyclical groups comes the existence of (additive) inverse elements that can behave like negative numbers. Assuming that both generators  and  are part of the same cyclical group of order , that means that for every amount  there exists a negative counterpart  which can be calculated by subtracting it from the order: .\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThe issue might become clearer by demonstrating it with a cyclical group  where  is an (unrealistically small) prime number:By subtracting  from , we find that the inverse counterpart to  within the cyclical group  is .The same issue exists (although with much larger numbers) in Pedersen Commitments: It's possible to find inverse elements that act like negative amounts resulting in valid UTXO commitments that allow for the creation of new Bitcoins from nothing.Above, Eve was able to spend a 5 BTC UTXO commitment and create two UTXO outputs which, in sum, equal the inputs but individually can be spent for much higher amounts of Bitcoin than she originally owned.In order to prevent this issue from being exploitable the amount has to be restricted to a reasonable range  and the number of outputs  a transaction may have must be restricted such that their sum may never be larger than the cyclic group's order (). Luckily, in Bitcoin's case, the maximum amount in a single UTXO may only ever reach 21 million (in the unrealistic worst case), which is nothing compared to the much larger order  of the cyclic group on the elliptic curve that Bitcoin uses.Restricting the number of outputs  that may be used in a transaction is simple. The question is: How can we prove that the commitment's amount is within the allowed range  without revealing  itself?","signing-with-pedersen-commitments#Signing with Pedersen Commitments":"When using ECC as a public-key cryptosystem, the secret key generally is a scalar value multiplied with the generator point, and the result is then used as the public key.The person with knowledge of the secret key  can prove their knowledge by signing a message .A third party can verify the resulting signature using the public key  and in doing so they won't gain any knowledge about  thanks to ECDLP.Pedersen Commitments can be used as a public key only when the amount  is zero, turning the blinding factor  into a private key. This allows signing a message, proving that the signer knows a commitment's  and that  must in fact be zero.But if we reveal the amount  to the party intending to verify the signature, we can even prove arbitrary amounts without revealing the blinding factor.At this point you're rightfully wondering what the point of any of this is when we would reveal the amount - this is where Ring Signatures finally come to into play.","hiding-signable-commitments-in-rings#Hiding signable Commitments in Rings":"While normal signatures prove that \"I know secret  for public \", Ring Signatures are essentially proofs that show \"I know at least one secret  of multiple public \" without revealing which one. Borromean Ring Signatures are a bit special in that they combine multiple of these proofs (ie. multiple rings) into a single signature:\"For the lists of Public Keys  I know \"In simple terms: For each list of Public Keys, I know at least one secret scalar .When we now want to prove that a commitment's amount is, for example, within the range  we can do so by splitting the boundary into a binary representation:Therefore, a commitment for an arbitrary amount such as , could be represented by multiple commitments of either zero or a power of two:A verifier wouldn't know which of the commitments represents a power of two or just a zero, they simply see that everything sums up correctly for the commitment they want to validate the range for:If we now treat  as public keys and  as private keys, the resulting Borromean Ring Signature  will prove that we either know  for  (in case ) or for  (when ):\"I know  for either  for either  for either...\"A verifier can now validate this Ring Signature and be sure that the maximum amount contained within the commitment  cannot possibly be higher than the highest number representable by 8-bit.With 8-bit amounts, the range is obviously too narrow for it to be of any practical use. Here the paper suggests using a decimal floating point where the digits are multiplied by a base 10 exponent (). As an example, let's say that an amount's mantissa  on its own is in Satoshis (Sats), but the amount should be full Bitcoins, therefore we need an exponent  of 8. A commitment for 78 BTC would accordingly encode an amount .As Pedersen Commitments can only be multiplied with scalars (and only added with other commitments), either  or  needs to be public. Making the exponent public leaks how valuable a commitment is, but making the mantissa public makes commitments very easily identifiable and doesn't improve the coin-mixing situation. Therefore the paper chose publishing  as it will be much easier to mix with other outputs of similar value.This approach sacrifices some privacy but significantly increases the provable range without impacting the verification efficiency in a significant way (compared to using larger range proofs).","the-code#The Code":"To experiment with this technology I wanted to implement a very simple Wrapper-Token for ETH with the gimmick that instead of plaintext balances, commitments are stored for each user. This would mean that during wrapping and unwrapping (deposit/withdrawal) the amounts of the commitments would be revealed, but when making token transfers with multiple output commitments (similar to UTXOs) it would be increasingly difficult to guess the actual amount each commitment represents.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThe biggest challenge in making this somewhat reasonably affordable is the fact that the EVM does not (yet) provide any opcodes or pre-compiles for simple curve operations on SECP256K1. This means that any point additions or scalar-point multiplications have to be calculated within the contract's bytecode, making these operations very expensive. I've previously shown how exploiting ecrecover helps with building Borromean Ring Signatures that can be validated much more efficiently within the EVM. When implementing Confidential Transaction Values we can build on top of this, though a few inefficient curve operations can't be avoided.","exploiting-ec-recover-once-again-for-efficient-commitment-validation#Exploiting EC-Recover once again for efficient Commitment validation":"As mentioned, during deposits and withdrawals we'll reveal a commitment 's amount  and it's blinding factor . In order to validate it, the contract needs to re-create it and then compare it: ?This would require 2 scalar-point multiplications and a final point-addition. Which is not too dissimilar from what is happening when using the EC-Recover precompile to recover the signer's public key  from a signature  and the hashed message.We could replace, what it expects to be  with the blinding factor. Point , which is derived from the signature's parameters  and  can be replaced with our second generator point . And the  value can become the amount . The problem is what happens afterward: The resulting point (which would be our commitment ) is multiplied with the (multiplicative) inverse of  and finally the point is hashed and converted into an address. We could apply those same operations to the user-supplied commitment, and check whether both resulting addresses are the same: ?With that, the commitment validation would be reduced to a single inefficient scalar-point multiplication. A fun way to improve that situation would be choosing an  with an x-coordinate that results in  being . And that would actually work! There's indeed a valid curve point at coordinate  (), which has the same order  as  while the relationship  remains unknown. A seemingly completely valid candidate for the second generator . But in cryptography, one should generally avoid \"things that are too good to be true\" since those often come with unexpected consequences.Instead, we can multiply the amount  and the blinding factor  with  and simply have the inverse canceled out.\nVitalik actually described this technique in 2018 and hinted that it could indeed be used for things like ring signatures.\n ?That only leaves the question of calculating the additive inverse of the blinding factor  which can be trivially done by subtracting it from the order ().","range-proofs-and-precalculations#Range-proofs and Precalculations":"For the Range Proof, we don't actually send a complete Ring Signature: There's no need to specify the message  since it'll be the hash of the commitment being validated - the validator can't trust us to provide the correct hash for the commitment  anyway and has to calculate it on its own.For the binary encoding used, each ring will have two members, one for the possibility that it's a commitment for zero and another for it being a power of two. Here we'll only send the sub-commitments  that, when summed up, result in the commitment  whose range is being validated. If a sub-commitment was used - as provided - to sign, it must have been a zero. For the case of the value being a power of two, the prover has to subtract that amount from the sub-commitment and use it as the second ring member (). Here again, the validator can't trust the user to provide the correctly subtracted ring member and needs to do the subtraction itself. Thankfully, these subtrahends () will always be the same and can be precalculated and stored as constants to avoid expensive scalar-point multiplications.","exploiting-ec-recover-a-final-time-for-exponent-multiplication#Exploiting EC-Recover a final time for Exponent multiplication":"In order to show that the Range Proof provided really proves the value of the commitment , we have to scalar-multiply the sum of its sub-commitments with , which again would be very expensive. And one last time, we'll abuse  for gas savings with the same technique as above:","gas-measurements#Gas measurements":"Cost estimate based on the assumption that the current gas price is 25 gwei and 1 ether is USD 2000.We've managed to avoid all expensive scalar-point multiplications by continuously abusing . That leaves us with simple point addition operations, and yet, validating ring signatures requires significant amounts of gas. The main reason for this isn't the actual point additions themselves, which thanks to highly optimized Jacobi-implementations (which use a 3-dimensional coordinate system ) can actually cost less than 1000 gas. Rather, the issue lies in the conversion back to the affine 2-dimensional point  coordinates that require calculating the multiplicative inverse .Calculating an inverse is a rather expensive operation that requires running algorithms such as Fermat's Little Theorem. But there would even be a way around this: We could pre-calculate the inverses off-chain and provide them together with the range proof. Validating whether these inverses are indeed correct is a very cheap operation in comparison: . This optimization is left as an exercise for the reader (I need to finish writing this article at some point).","a-confidential-transaction-values-weth-contract#A Confidential Transaction Values WETH contract":"The following is a WETH-like implementation using Confidential Transaction Values. Users can deposit() ether into the contract and then use transfer(), well, to transfer these balances. It's difficult to call this a \"token\" since it doesn't follow any conventions such as ERC20, and it certainly can't provide a balanceOf() function.The code is intended to be pasted into Remix for you to play around, but note that it requires the via-IR pipeline for compilation (set in compiler_config.json's settings \"viaIR\": true).","client-script#Client Script":"The following python script is a simple \"Client\" implementation for using such a token. In a sense it's like a wallet.","output#Output":"","caveats#Caveats":"Using Borromean Ring Signatures for Range Proofs is unfortunately very inefficient and only practical when sacrificing some specificity (due to a small mantissa) and some privacy (public exponent leaking a range on how valuable a commitment is). At the time, that seemed to be the best choice under the condition that no new cryptographic assumptions would be introduced into the system. But despite those efforts, the proposal never made it into Bitcoin. Instead, implementations of Confidential Transactions can be found in the Elements Framework and the MimbleWimble Blockchain."}},"/posts/2023/11/8/race-23-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #23 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a mirror of a Write-Up on RACE-23, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. It was designed by Secureum Mentor Jon Stephens, from Veridise.The original version of this document can be found at https://veridise.notion.site/veridise/RACE-23-Answers-d63cb0b5373f43f0ba43612e89596547Participants of this quiz had to answer 8 questions within the strict time limit of 64 minutes on Zero Knowledge, Circom, and Merkle Trees. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!","code#Code":"","question-1-of-8#Question 1 of 8":"A call to PrivateToken.transfer may revert for which, if any, of the following reasons?\n A. The current root does not match either addProof.oldRoot or subProof.oldRoot. \n B. The amounts declared in addProof and subProof do not match. \n C. A nullifier has been used in the past. \n D. Either addProof or subProof does not verify. \n E. None of the above \nCorrect Picks: B, C, D.B, C and D all correspond to requires in burn, but there is no root check so A must be false","question-2-of-8#Question 2 of 8":"Which, if any, of the following statements hold with respect to MerkleTreeAdd?\n A. The MerkleTreeAdd circuit will only verify computations where MerkleTreeAdd.oldVal is in the merkle tree with root MerkleTreeAdd.oldRoot.\n B. The MerkleTreeAdd circuit will only verify computations where the leaf is at the location given by MerkleTreeAdd.index in the merkle tree with root MerkleTreeAdd.oldRoot.\n C. The MerkleTreeAdd circuit will only verify computations where the root calculated by ComputeTreeRoot matches MerkleTreeAdd.oldRoot.\n D. The MerkleTreeAdd circuit will only verify computations where the sum of MerkleTreeAdd.oldVal and MerkleTreeAdd.amount are in the merkle tree with root MerkleTreeAdd.newRoot.\n E. None of the above\nCorrect Picks: C.\nA is not correct because of the unconstrained mux.\nB is not correct for the same reason.\nC is correct because of a check in MerkleTreeAdd, which ensures the root matches.\nD is not correct because of the unconstrained mux.","question-3-of-8#Question 3 of 8":"Which, if any, of the following statements hold with respect to a user's balance?\n A. The balance associated with a user's ID is private and cannot be known after funds have been allocated to it.\n B. A user must know the balance associated with an ID to spend funds from it.\n C. An index in the merkle tree is tied to a specific address so the associated balance corresponds to that address's balance.\n D. If the root of the tree doesn't change, a user can always show that their balance is contained in the tree.\n E. None of the above\nCorrect Picks: D.There are two privacy leaks that allow someone to learn a user's balance, so A is not correct. B is not correct because of the unconstrained mux. C is not correct because there is no logic that ties an address to an index. D is correct because if the root has not changed, then someone's merkle proof must remain valid.","question-4-of-8#Question 4 of 8":"Which, if any, of the following statements hold with respect to how a user's balance may change?\n A. Upon a burn, a user's balance will either decrease or remain the same.\n B. Upon a mint, a user's balance will either increase or remain the same.\n C. Upon a transfer, the sum of user balances stored in the tree is constant.\n D. During a mint, burn and transfer, only the balances at the indicated indices will change.\n E. None of the above\nCorrect Picks: E.None of these hold since the balance values may overflow and the unconstrained mux.","question-5-of-8#Question 5 of 8":"Which, if any, of the following signals are under-constrained (i.e. a signal may be assigned to at least two different values while keeping the circuit inputs constant)?\n A. MerkleTreeSub.newRoot \n B. ComputeTreeRoot.hashes[0] \n C. MerkleTreeAdd.nullifier \n D. MerkleTreeSub.oldRootCalc.root \n E. None of the above \nCorrect Picks: A.A is under-constrained because of the unconstrained mux. B is not under-constrained because of the === constraint at the end of the circuit. C is not underconstrained due to <==. Finally, D is not under-constrained since it is explicitly constrained to an input in MerkleTreeSub.","question-6-of-8#Question 6 of 8":"Which, if any, of the following may occur during a burn?\n A. A user may burn more funds than they own.\n B. The new computed root will not be saved.\n C. The balances stored in the merkle tree may be arbitrarily manipulated.\n D. Every user can burn their entire balance.\n E. None of the above\nCorrect Picks: A, C.\nA holds due to the overflow.\nB does not hold since burn does store the new root.\nC holds due to the under-constrained mux.\nD does not hold since the nullifier is tied to user balances, so only one user may have balance 0.","question-7-of-8#Question 7 of 8":"With respect to the nullifier, which, if any, of the following statements hold?\n A. The nullifier leaks information about a user's balance.\n B. The nullifier will prevent users from submitting the same proof twice.\n C. If a nullifier is used by some user, another user will likely not be affected.\n D. The computed value of the nullifier is under-constrained.\n E. None of the above\nCorrect Picks: A,B.\nA holds because it hashes the balance directly and so if two nullifiers conflict,¬† you have learned something about the balance of the user who used it previously.\nB is correct since the same proof will result in the same balance and therefore the same nullifier.\nC is not correct due to the issue identified in A.\nD is not correct because the nullifier is explicitly constrained to be the hash of two input values.","question-8-of-8#Question 8 of 8":"Which, if any, of the following correspond to potential attacks that can be used against the protocol?\n A. Replay Attack\n B. Denial of Service\n C. Under-constrained Signal Manipulation\n D. Arithmetic Overflow\n E. None of the above\nCorrect Picks: B, C, D.\nAs discussed in Q7, you cannot replay proofs so A is not correct.\nB is correct due to the balance issue from Q6, someone can perform a Dos.\nC is correct as there are under-constrained signals that impact the output of the circuit.\nD is correct since since the balance add and sub may overflow."}},"/posts/2023/2/27/race-15-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #15 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-15, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors.  It was designed by Secureum Mentor Nurit Dor from Certora.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nFebruary 22, 2023 by patrickd","code#Code":"All 8 questions in this RACE are based on the below contracts. This is the same contracts you will see for all the 8 questions in this RACE. The question is below the shown contracts.","question-1-of-8#Question 1 of 8":"What is/are the correct implementation(s) of the nonReentrant() modifier? A.\n B.\n C.\n D.\nCorrect is B, D.The correct implementation of a mutex-modifier must change the value of a storage variable and only reset it once the execution of the function's body (triggered by _;) has been completed. This storage variable must also be checked by the modifier: If the value is in its initial state (which in this case is zero since reentrancy_lock is not initialized with any specific value) then the execution of the function may proceed. But if it's in the non-default state (meaning the modifier is being executed again while the function body has yet to complete) then the check should error.\nAnswer A) cannot be correct since it assumes a starting value of 1 instead of 0.\nAnswer B) does indeed assume the default of 0 as the starting point and uses 1 to signify the \"function body is executing\" state.\nAnswer C) requires the starting value to be 1, which is not the default.\nAnswer D) works just like B) with the only difference that it uses the number 2 instead of 1.","question-2-of-8#Question 2 of 8":"Who can claim fees using claimFees()?\n A. Only the owner, due to onlyOwner modifier \n B. The owner  \n C. Anyone who can trick owner into signing an arbitrary transaction \n D. No one \nCorrect is B, C.The claimFees() function uses the onlyOwner modifier which checks the origin address (the signer of the transaction being executed, not the sender of the message) and requires it to match the address stored within the owner state variable. This state variable is only set once during the creation of the contract and can only be changed, once again, by passing through the onlyOwner modifier.The problem with using the transaction origin for authentication purposes is, that anyone can be the caller of the claimFees() and setOwner() function of this contract. Meaning that one could trick the owner into signing a seemingly unrelated transaction to another contract and that other contract may then call these functions as if it were acting with the approval of the owner. Due to this potential \"phishing vector\" it is generally considered a bad practice to use tx.origin for authentication and one should normally use msg.sender instead.","question-3-of-8#Question 3 of 8":"In buyEth(), we put an unchecked block on current_eth -= amount\n A. Because current_eth is uint \n B. Because the compiler is protecting us from overflows \n C. Only if we add a prior check: require(current_eth > amount); \n D. Only if we add a prior check: require(current_eth >= amount); \nCorrect is D.A so-called \"unchecked-block\" will disable overflow protections provided by the Solidity compiler of versions 0.8.0 and higher. If a unchecked-block were used, a buyer may obtain more ether than the SimpleDEX contract is holding if such value was injected into the contract through means other than a normal transfer (eg. via selfdestruct()).While the type of current_eth is indeed uint, standing for unsigned integer, meaning an integer that can not represent negative values, that doesn't mean that it can't underflow. For example, if the value in a uint8 is currently 0 and 1 would be subtracted from it in an unchecked-block, then it would roll over to the biggest value it can represent: 255.Option D) will prevent current_eth from underflowing below 0.","question-4-of-8#Question 4 of 8":"In buyEth(), are there any reentrancy concerns assuming the nonReentrant modifier is implemented correctly?\n A. No, because it has the nonReentrant modifier\n B. No, and even without the modifier you can't exploit any issue \n C. Yes, there is a cross-contract reentrancy concern via Seller \n D. None of the above \nCorrect is C.While the nonReentrant modifier prevents re-entering the same contract to exploit an \"incomplete state\", the same cannot be said for other contracts that might make use of the SimpleDEX's state before the state is completely updated.Specifically state variables involved in determining the price (token_balance & current_eth) are relevant here: current_eth is updated before the call() to the message sender is made. But token_balance is only updated after.If the msg.sender is actually a contract, it will have a chance to call another protocol that is relying on the SimpleDEX's reported price to be correct (such as the Seller contract). If the malicious contract calls this victim contract while the state of SimpleDEX is incomplete (ie. cross-contract read-only reentrancy) the victim would make use of this incorrect price data which might give the attacker an advantage. (Not in this case though. There's no advantage to exploiting this in Seller since the attacker would actually have to pay a higher price than without exploiting this issue).","question-5-of-8#Question 5 of 8":"What will happen when calling buyEth() via SimpleDexProxy?\n A. buyEth() will be called and successfully executed \n B. You can‚Äôt call a function that way; it must be called directly\n C. buyEth() will be called but ETH won't be transferred\n D. Transaction will be reverted\nCorrect is D.The transaction would be reverted since the SimpleDEX's buyEth() function would attempt transferring the tokens from the msg.sender, which in this case would be a proxy that has no way to give it the appropriate allowance even if the user were to transfer their tokens to the proxy first.","question-6-of-8#Question 6 of 8":"In buyEth():\n A. If amount is less than 100, it will lead to an incorrect calculation of fee\n B. If token_balance is already at its MAX_UINT256, it will result in overflow and won't revert\n C. If token_amount is > MAX_UINT64, it will result in a casting issue\n D. None of the above\nCorrect is A, C.In buyEth() the amount is divided by 100 before being multiplied with fees_percentage. Since we're dealing with integer division there are no rational numbers. Dividing anything lower than 100 will result in 0 causing the fee to be 0 as well. The general best practice is \"multiplication before division\" to prevent such issues involving loss of precision.Starting Solidity 0.8.0, an overflow happening outside of an \"unchecked-block\" will always result in the transaction reverting. Therefore B) is incorrect.When the token_amount is added to the token_balance there's indeed a casting issue when the amount's value does not fit into a uint64 type.","question-7-of-8#Question 7 of 8":"Can getEthPrice() return zero?\n A. Yes, if the owner initializes the contract with more ETH than token_balance\n B. Yes, a carefully crafted buyEth() transaction can result in getEthPrice() returning zero\n C. Yes, once all the ETH are sold\n D. No, there is no issue\nCorrect is A.The getEthPrice() function calculates the price with token_balance / current_eth. Since this is integer division, if the token balance would be smaller than the current ether balance the result would not be a integer but a rational. So the result would end up being a zero.There isn't anything special you can \"craft\" for a call to buyEth() to result in the price function returning zero.Once all the ETH are sold, getEthPrice() won't return zero but revert instead due to a division-by-zero.","question-8-of-8#Question 8 of 8":"Which of the following invariants (written in propositional logic) hold on a correct implementation of the code?\n A. this.balance == current_eth <=> token.balanceOf(this) == token_balance \n B. this.balance >= current_eth && token.balanceOf(this) >= token_balance \n C. this.balance <= token.balanceOf(this) && token.balanceOf(this) <= token_balance \n D. this.balance >= current_eth || token.balanceOf(this) >= token_balance \nCorrect is B, D.The symbol <=> is called the biconditional operator, meaning that the expressions on either side are logically equivalent. But the actual balance of ether being equal to the balance tracked within the state variable current_eth does not imply that the actual and tracked token balances are equal too. So the biconditional is invalid. But even if it were an AND operator it would not be an invariant that could hold: The invariant would be simple to break by sending unsolicited tokens to the contract (eg. via selfdestruct()).\nOption B) includes the fact that the actual token and ether balances may be higher than the tracked balances. In a correct implementation this invariant should always hold.\nOption C)'s second part allows the tracked token_balance to be larger than the actual token balance. This should not be the case in a correct implementation.\nOption D) seems similar to B) but would allow for there to be either too little ether to match the tracked balance or too little tokens to match the tracked token balance. So it doesn't sound like a good invariant to test for since you'd want both things to hold true and not just one of them. But that wasn't the question - would it hold true in a correct implementation? Yes."}},"/posts/2023/3/24/cryptocurrency-privacy-technologies-blind-signatures":{"title":"Cryptocurrency Privacy Technologies: Blind Signatures","data":{"":"March 24, 2023 by patrickd\nDespite being regularly referred to as \"anonymous Internet money\", the ledgers of the most widely adopted cryptocurrencies are completely public. Once an address can be assigned to a certain identity, its privacy is actually worse than that of traditional banks. This article explores one of the first technologies ever proposed to enhance the privacy features of digital currencies.","the-concept#The Concept":"The usage of Blind Signatures for untraceable payments was first proposed in 1982 by David Chaum, with the first practical application in DigiCash's \"eCash\", which, although it was centralized, allowed for an exchange of value without the centralized entity being able to know the parties involved in the exchange.A Blind Signature means, quite literally, that the signer can sign data blindly, without being able to see what he is actually signing. In a way, it is similar to Homomorphic Encryption: The ability to apply an operation on an encrypted value. One could encrypt their message, send it to the signer, and then once they receive the signed message back, they could decrypt it while the message would remain signed.In simplified terms, Chaum's paper envisioned an untraceable payment scheme with the following steps:\nPayer picks a random number  and encrypts it, yielding a \"note\" .\nPayer sends  to the bank which signs it blindly, yielding . The bank debits an amount of money from the payers account which the note will be valued at (all signed notes would be worth the same amount).\nPayer decrypts the signed note, after which it remains signed: \nPayer spends their signed note at Payee. Payee sends  to the bank which, although it has never seen the note in this form before, can verify that it had indeed blindly signed it at some point and can safely accredit the note's value to the Payee's account. To prevent double-spending, the bank keeps track of notes that have already been redeemed.","the-math#The Math":"The scheme can be implemented with various cryptographical methods (eg. Elliptic Curves, Schnorr), but the most common one used in examples is RSA. In these, the bank would have a Public  and Private  keypair. as large random prime numbers(To get a sense of why RSA's math works, you might want to read my previous article first.)The payer would pick a message  and multiply it with an encrypted , a random value that must be relatively prime to  and . It's also important to remember that  limits the size of the message , so it should be ensured that .In normal RSA, exponentiation with  is either used for decrypting a received message or for signing a message by encrypting it. In this case, it's both: The part of the note that is the random value  is reversed to its original \"unencrypted\" state, while the message  becomes signed with the bank's private key. Since the bank does not know , they are unable to extract the message  that was signed.The payer is now able to \"unblind\" the note by removing  from the signed note . This results in , which is simply the original message signed with the bank's private key. It can be considered anonymous because the bank has not seen the message  yet.To validate the signature, one uses the bank's public key which yields the plaintext message .To prevent any random number  resulting from this operation to be valid, the message must follow a verifiable pattern. Usually, it would be a hash of the actual message that can then be checked against it. In this case, it could also be required to result in a certain kind of string such as  \"eCash Note Number: #${large random number}\". It would be the responsibility of the payer to choose a valid and unused message before having it signed by the bank, otherwise, they'd end up with an irredeemable note.","the-code#The Code":"The following is an example of how this scheme might be implemented on a blockchain, specifically Ethereum since that is what I am most familiar with. I hope this is obvious, but please do not actually use this code as it was created to be illustrative and easy to understand, and might be incorrect or even insecure.We'll use a \"Mixing Pool\" contract: The payer becomes an Ethereum user who deposits a static amount of ether together with his blinded note . The bank becomes a \"trustee\", an automated off-chain program that monitors for Deposit events, asynchronously signs the specified notes, and publishes them as . Since nobody except of the depositor knows , publishing signed but still blinded notes would not allow anyone else to make use of them. Whoever receives  from the depositor at a later point can now anonymously withdraw the ether from the pool. As you might have guessed, the anonymity of mixing depends on other users also having made deposits in the meantime.To start, we'll generate the trustee's Public-Private key pair:\nThe public key  can now be published as part of the Mixing Pool contract:\nA user intending to make a deposit can now use this information to prepare and publish their blind signature off-chain:\nThe Trustee off-chain program will monitor the chain for new Deposits and automatically sign these notes.\nAfter publishing their blinded note, the depositor had been waiting for it to be signed. Once that has happened they can unblind the note to make it spendable. Now the depositor only needs to wait some time for others to make deposits and withdrawals as well in order to increase the privacy of their note.\nFinally, the receiver of a signed note () can immediately make a withdrawal to verify and redeem the note that he received from a depositor. Thanks to the fact that the public key required for verification is already part of the smart contract, the withdrawal can be completely handled on-chain:","caveats#Caveats":"It's centralized. Since this scheme requires a trusted party signing with their private key, it comes with a whole bunch of related issues. The trustee could rugpull all of the pool's funds by creating as many notes as it wants. The trustee could censor/skip certain deposits and never sign their blinded notes. The trustee could be hacked or socially engineered to leak the private key. The trustee's off-chain service could become unavailable locking up any new deposits forever. etc. etc.\nRSA is hard to get right. In this specific example, the fact that the trustee will sign anything that he observes from a Deposit event could likely cause issues, especially if what's being signed is a very small message. Little gotchas like that are why it's generally considered a bad practice to \"roll your own crypto\".\nThe withdrawing party needs ether to pay the gas cost for the withdrawal. But in order to obtain that they might already lose some anonymity. To avoid this, one could make use of relays or, more recently Account Abstraction (EIP-4337)."}},"/posts/2023/3/23/why-does-rsa-actually-work":{"title":"Why Does RSA Actually Work?","data":{"":"March 23, 2023 by patrickd\nRSA encryption appears so incredibly elegant and simple, it's as if it were some sort of mathematical magic. This article is an attempt to give the reader some intuition about why it works and make the seemingly magical aspects graspable. Note that it avoids mathematical proves where possible, it does not attempt to give an explanation on how to implement RSA securely or efficiently, and it skips any concepts that have been deemed irrelevant for understanding the \"why\".","the-magic#The Magic":"and  chosen as large random prime numbers, phi being \"Euler's Totient Function\", fullfills requirements of  and , combined with  makes up the public key , combined with  makes up the private key , is the encrypted ciphertext  of the message , is the decrypted message  from the ciphertext","what-is---mod-#What is  ?":"You might already know that the triple line equal sign  stands for \"congruence\" rather than \"equality\" while  is short for modulo which yields the remainder of a division operation.In congruence both sides have the same remainder when divided by a particular modulus.Example and therefore and","what-is-e-1-#What is  ?":"One might expect this to be , but we're only dealing with integers here and not rationals, so what could it be?According to RSA the following should be true:The public exponent  and the private exponent  are cancelling each other out, making them \"inverse\" to each other. So in modular arithmetic  is the \"modular multiplicative inverse\" of .Knowing that, we can also see why the notiation of  was chosen to represent the inverse when we go back to using rational numbers:","why-does-inversion-work#Why does inversion work?":"At first it looks quite simple: We just have to find two integers  and  such that when multiplied with each other they result in 1 (within ) and are therefore inverse to each other:An interesting aspect to this is, that it only has solutions when  and  are \"coprime\" (or \"relatively prime\") to . Meaning that the greatest common divisior, the biggest integer that divides both numbers without a remainder, is 1. and ExampleLet's look for the inverse  of 5 within :With  there should not be a solution for , is that true?\nThe numbers this can yield quickly start to repeat and none of them are 1, so there is indeed no inverse.Okay, but 15 is a multiple of 5 so the result is not that much of a surprise.What about 6?With  there should still not be a solution for , but this time 15 is clearly not a multiple of 6 so what happens now?\nStill no inverse, but something interesting becomes visible: The numbers yielded are always multiples of .Let's try a number which should actually have an inverse: . It may be worth pointing out here that neither numbers are prime, but with this result they are still \"relative prime\" to each other.\nNot only has 8 an inverse but the numbers yielded are also all numbers from 0 to 14. Thanks to the greatest common divisor being 1, the resulting numbers are multiples of 1 as well. This time it is as if there's a 1-to-1 mapping between  and the resulting numbers as long as .","why-does-an-inverse-from-mod-phin-still-seem-to-work-in-mod-n-#Why does an inverse from  still seem to work in  ?":"We've seen that the inverse of an integer depends on the modulo. The inverse for 8 in  is 2. But in  it would be 5.In RSA  is calculated as the inverse of  in : and therefore But during decryption  and  appear to be cancelling each other out even under , why does that work?Unfortunately there doesn't seem to be a good way to explain this without making use of (a little bit of magic called) Euler's Totient Theorem which shows that for an integer  that is coprime with  the following applies:To make use of that we have to look for a  in the message's exponent and we can quite easily find it: with the integer Thanks to Euler's Theorem it seems like we can remove everything except for message  from the formula with .But you might remember I mentioned the Theorem should only apply when  is coprime with , which would restrict the valid messages we would be able to encrypt. If  was a prime that would not be an issue as all integers  would be relatively prime to it.","why-does-eulers-theorem-still-seem-to-apply-for-gcdm-n--1#Why does Euler's Theorem still seem to apply for ?":"First, note that because both  and  are prime their greatest common divisor is always 1. If  is not coprime with  that must mean that it is a multiple of either  or , meaning it can't be coprime with one of them. But it must be coprime with at least one of them, because in order to not be coprime with both of them it must be a multiple of both  and  and that cannot be the case as long as  since  is the first possible number to be a multiple of both: .Therefore even when  either  or  will share a residue of 0 with the message (since  would be a multiple of it without leaving a remainder) and the other one would be either:orWith these, we have either one of the following systems of congruence:, or, Both of which can be simplified using the Chinese Remainder Theorem to:Example with  with Despite  not being coprime with  thanks to: therefore  therefore Therefore","why-does-eulers-theorem-work#Why does Euler's Theorem work?":"First of all, Euler's Totient Theorem is a generalization of Fermat's Little Theorem where for a prime  for any integer  the following is true:Next, we need to look at Euler's function, denoted as . This function counts the number of positive integers up to  that are relatively prime to . In other words, it counts the number of integers that have a greatest common divisor of 1 with . When  is a prime number , all the numbers from  to  are relatively prime to , so .Euler's Theorem generalizes Fermat's Little Theorem and states that for any positive integer  and any integer  relatively prime to , we have:Which is in case for  prime equivalent to:ExampleA very simple way to gain some intution for these Theorems is through, what one might call, a modulo-lookup-table:\nImagine this (modulo 5) lookup-table just continues to infinity and always tells us at what remainder we would end up with if we applied .Now let's check :\nWith the help of the lookup-table we can easily determine the remainder of the division by checking in which column we find the number. The Theorem works.Although it's mathematically equivalent, let's check  too:\nHere we can see that we always end up in the 0-column, just as we should.Finally, let's look at :\nSince 0 can't have a greatest common divisor with anything, it also can't be coprime so it yields 0 instead of 1. But as long as the base is coprime to the modulo we always end up in the 1-column.","what-is-the-chinese-remainder-theorem#What is the Chinese Remainder Theorem?":"The last trick to explain is why the Chinese Remainder Theorem shows that  despite  not being coprime to both  and .The Chinese Remainder Theorem (CRT) allows us to solve a system of linear congruences with pairwise coprime moduli. It states that if you have a system of linear congruences like:\nwhere  are pairwise coprime (i.e., they share no common factors other than 1), then there exists a unique solution for  modulo the product of the moduli, i.e., modulo  ... : ... In the context of RSA, we have  where  and  are distinct prime numbers, and thus, they are coprime.\nExampleBased on the systems of congruence from the previous RSA calculation example:We now use the CRT step by step:"}},"/posts/2023/4/1/race-16-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #16 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-16, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors.   It was designed by recently joined Secureum Mentor Jon Stephens and his company Veridise.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nMarch 25, 2023 by patrickd","code#Code":"All 8 questions in this RACE are based on the below contract. This is the same contract you will see for all the 8 questions in this RACE. The question is below the shown contract.","question-1-of-8#Question 1 of 8":"Which of the following is an explanation of why flashLoan() could revert?\n A. The transaction reverts because a user requested to borrow more than maxFlashLoan()\n B. The transaction reverts because the receiver‚Äôs onFlashLoan() did not return CALLBACK_SUCCESS\n C. The transaction reverts because the user returned more than retAmt funds\n D. The transaction reverts because a user tried to spend more funds than their allowance in onFlashLoan()\nCorrect is A, B, D.\nA. Is implicitly checked in the require with message \"Too many funds requested\"\nB. Is explicitly checked in the require with message \"Callback failed\"\nC. The user may return more but not less than retAmt\nD. An ERC20 transferFrom() would revert in that case","question-2-of-8#Question 2 of 8":"If the FlashLoan contract were safe, which of the following invariants should hold at the end of any given transaction for some ERC20 token t?Note: old(expr) evaluates expr at the beginning of the transaction.\n A. t.balanceOf(address(this)) >= old(t.balanceOf(address(this)))\n B. t.balanceOf(address(this)) == old(t.balanceOf(address(this))) \n C. t.balanceOf(address(this)) > old(t.balanceOf(address(this))) \n D. t.balanceOf(address(this)) == old(t.balanceOf(address(this))) + fee \nCorrect is A.For the flashloan to be safe, the contract's token balance must be maintained no matter which function is called. It must be (A) because flashloan will cause the token balance to either increase or stay the same (depending on fee) and all other functions should maintain token balances","question-3-of-8#Question 3 of 8":"Which of the following tokens would be unsafe for the above contract to loan as doing so could result in theft?\n A. ERC223\n B. ERC677\n C. ERC777\n D. ERC1155\nCorrect is C.This can be attacked by ERC20 contracts with sender-callbacks in transferFrom(). ERC777 and ERC1155 are the only ones with callbacks. But ERC1155 will revert as its APIs doesn't match the ones in the flashloan contract, even even then, it would only have receiver-callbacks.","question-4-of-8#Question 4 of 8":"Which external call made by flashLoan() could result in theft if the token(s) identified in the previous question were to be used?\n A. onFlashLoan()\n B. balanceOf()\n C. transferFrom()\n D. approve()\nCorrect is C.ERC777 tokens have potentially dangerous callbacks on transfer()/transferFrom() that can result in theft.","question-5-of-8#Question 5 of 8":"What is the purpose of the fee in the FlashLoan contract as is?\n A. To increase the size of available flashloans over time\n B. To pay the owner of the flashloan contract\n C. To pay those who staked their funds to be flashloaned\n D. It has no purpose\nCorrect is A.In the current FlashLoan contract, as it is, the sole purpose of the fee is to increase the available funds to loan.","question-6-of-8#Question 6 of 8":"Which of the following describes the behavior of maxFlashLoan for a standard ERC20 token over time?\n A. strictly-increasing\n B. non-decreasing\n C. constant\n D. None of the above\nCorrect is B.Can't be A since fee may be 0. It could still increase from people injecting tokens, but it may never decrease.","question-7-of-8#Question 7 of 8":"For some arbitrary ERC20 token t, which of the following accurately describes the FlashLoan contract‚Äôs balance of t after a successful (i.e. non-reverting) call to flashLoan() (where t is the token requested for the flashloan):\n A. The FlashLoan contract's balance of token t will INCREASE OR STAY THE SAME.\n B. The FlashLoan contract's balance of token t will DECREASE OR STAY THE SAME.\n C. The FlashLoan contract's balance of token t will STAY THE SAME.\n D. None of the above.\nCorrect is D.flashLoan() can hypothetically finish successfully with any token that implements the ERC20 interface, even if it is a bogus implementation. Therefore, there are no guarantees on the output of IERC20(token).balanceOf(user).","question-8-of-8#Question 8 of 8":"Which of the following are guaranteed to hold after a successful (i.e., non-reverting) execution of flashLoan(), assuming the token for which the flashloan is requested uses OpenZeppelin‚Äôs Standard ERC20 implementation?\n A. The receiver‚Äôs balance of ‚Äútoken‚Äù increases\n B. The funds that the FlashLoan contract has approved the receiver to spend has either stayed the same or decreased.\n C. The sum of all flashloans granted by the FlashLoan contract is less than the maxFlashLoan amount.\n D. The token balance of any contract/user other than the FlashLoan contract, the caller of the flashLoan(), and the ‚Äúreceiver‚Äù contract will remain the same as before the call to flashLoan().\nCorrect is B.\nA. The token balance of the receiver could increase, decrease, or remain the same in the call to onFlashLoan()\nB. This is ensured by the last if-statement\nC. maxFlashLoan has no relationship to the sum of all flashloans\nD. Other user token balances could be adjusted in the call to onFlashLoan()"}},"/posts/2023/4/29/fuzzing-vyper-contracts-using-foundry":{"title":"Fuzzing Vyper Contracts Using Foundry","data":{"":"This is a guest post by Darren Chapman (aka Parsely) created as part of patrickd‚Äôs writing mentorship.\nApril 29, 2023 by Parsely\nAs a web3 developer one of the essential tasks is to write tests for your contracts. These tests are usually unit tests that test the functionality and business logic of the code. One step up from that, to really enhance the security, would be to write more complex tests like invariant tests and fuzzing tests. The Foundry framework has made writing these types of tests for Solidity contracts a much easier task.But what about smart contracts written in other languages? How can we use the Foundry framework in these development environments?In this article, we will take a look at how the Foundry framework can be used to write tests that will fuzz contracts written in Vyper.","introduction-to-testing-using-foundry-and-solidity#Introduction to testing using Foundry and Solidity":"","the-basics-of-foundry#The Basics of Foundry":"Let's begin by looking at how a basic Foundry test is written and what happens when we run our tests within the framework.This will give us a better foundational understanding of the basic testing pattern, before exploring fuzzing Vyper contracts in more detail.Foundry's module that runs the tests is called Forge. Forge will check the function names declared in the contract, if any of the function's names starts with test, it is considered to be a valid Forge test. By convention, test contracts are usually created within the /test directory and the file name usually ends with .t.sol.When a test is run in Foundry, the setup function which can be used to set up the environment, will be invoked before running each test method.\nOnce setup has successfully run, the framework will then run any of the tests written in our Solidity code in the test contract.","the-basics-of-fuzzing#The Basics of Fuzzing":"We have looked at how simple tests can be written and run in Foundry. Let's now take a look at how fuzzing can be implemented in the Foundry framework.A fuzz test function is identified by Foundry as being a function with at least one input parameter. The test function will run a configurable number of times, with quasi-random values set for the input parameter on each iteration. Foundry has created a way in which developers can either filter input values, or set ranges that these values should be within. Forge tests will recognise and adhere to any limits set by using the cheatcodes called assume and bound.\nThe above test will run and replace the value of theNewValue with a quasi-random number to test the logic and the assertion.By default the test will run 256 times, but this can be set within the Foundry configuration to be any value of your choosing.","deploying-contracts#Deploying Contracts":"The main assumption in any of the test functions is, that any contract that the test needs to interact with either is deployed, or will be deployed in the setup() function or as part of the test logic. The logic in our Solidity code allows us to declare and instantiate any of the contracts we would need to have deployed, in order to successfully run the logic of our test.Vyper contracts are not as easily deployed as Solidity contracts by the Foundry framework, which necessitates a few more steps, in order to deploy the Vyper code that the test needs to interact with.","testing-using-foundry-with-vyper#Testing using Foundry with Vyper":"","vyper-101#Vyper 101":"It would be good to take a quick look at how Vyper development and deployment work on a local system, before we continue exploring how to use Foundry to fuzz test our Vyper contracts.In order to be able to have a working example by the end of the article, we will build a very basic project together, that will allow us to build step by step and end up with a working fuzz example. The Vyper contract we will use is one I have written, which mimicks the functionality of the Counter contract, which is created by default when a new Forge workspace is initialized using the forge init command.To develop the example project, we first need to set up our environment and workspace.\nmkdir vyper_example. Create a directory for the example.\ncd vyper_example. Move into that directory\nnpm install Vyper. Install the Vyper NPM modules.\nmkdir src. Create a src directory\ncd ./src. Move into the src directory.\ntouch Contract.vy. Create a file called Contract.vy\nCopy and Paste the code below into the file\nOpen a new Terminal in the vyper_example directory\nvyper src\\Contract.vy. Compile the contract.\nThe command line code returns bytecode, which can then be used to deploy the Vyper contract onto the blockchain.","combining-vyper-and-foundry#Combining Vyper and Foundry":"When we test Vyper contracts using Foundry, we will need a way to compile and deploy the contracts within the setup or test functions.Foundry comes with a cheatcode in the framework, which enables us to interact with our local system, called ffi. This Foundry cheatcode allows us to run terminal commands from Solidity code on our local system, and assumes the output of the terminal command will be hex values. ffi runs within the context of the root directory of the project, and not within the context of the test directory.Link to Foundry Book Website -> ffi\nThe example below is very basic, but will allow us to compile and deploy a Vyper contract from Solidity code.","building-on-our-example#Building on our example":"The example project we have built so far is not yet setup to use Foundry. We need to initialize the current directory vyper_example and install all the requisite libraries needed by the framework to be able to run tests. The terminal command to initialize a directory is forge init --no-git --force.\nThe --force option is provided to allow Forge to ignore the fact that our root directory is not empty, as by default Forge requires an empty directory to start a new project.\nForge init should install the required test libraries for Foundry in the lib directory by default, however if you need to install them, you can run the command forge install foundry-rs/forge-std.The configuration file called remappings.txt, tells Foundry where to look for files referred to by import statements in the code. In our example, we would need to create this file in our project's root directory called vyper_example.For convenience I have pasted the values needed in our remappings file below. These values map to the Forge-std libraries which are imported into our test contract to run the fuzz tests.\nWe learned earlier that test contract files are usually created in the test directory. So let's create a file in the test directory called Contract.t.sol.Copy and paste the code below into the test contract and save the file.\nWe are now ready to run our first test, which will deploy our Vyper contract and run our static test.\nforge test --match-contract VyperContractTest -vv --ffi. To run the Forge test\n--ffi tells Foundry to allow the ffi call as it is disabled by default for security reasons.\nIf everything has worked correctly our test should be successful.","adding-the-fuzzing-test-function-to-our-test-file#Adding the fuzzing test function to our test file":"We have progressed now to a point where we can deploy and interact with our Vyper contract.The final step will be to add a fuzzing function to our Foundry test contract.Paste the code below into our test contract and run the test command again.\nReminder of the command to run:forge test --match-contract VyperContractTest -vv --ffi","conclusion#Conclusion":"In this article we covered a brief overview of Vyper and Foundry.\nWe went through the steps of creating and compiling the Vyper contract using ffi. We learned how to allow Foundry to interact with our system to run commands, and finally, we saw how all of these techniques can be combined to enable developers to write unit tests and fuzzing tests for their Vyper development."}},"/posts/2023/5/29/race-18-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #18 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-18, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors.   It was designed by Secureum Mentor Josselin Feist from Trail of Bits.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nMay 25, 2023 by patrickd","code#Code":"All 8 questions in this RACE are based on the below contract. This is the same contract you will see for all the 8 questions in this RACE. The question is below the shown contract.","question-1-of-8#Question 1 of 8":"What risk(s) should be considered when reviewing this contract?\n A. Reentrancy risks \n B. Logic bugs \n C. Front-running risks \n D. Arithmetic risks \nCorrect is A, B, C, D.During an audit all risks should be considered! (Duh)This question wasn't about what issues the contract actually has, but these specific issues will be asked about later in this quiz.","question-2-of-8#Question 2 of 8":"Which of the following statement(s) is/are true?\n A. No overflow can ever occur in a contract compiled with solc version 0.8\n B. IERC20.decimals returns(uint256)  is not a correct ERC20 function according to the  ERC20 specification\n C. The contract does not follow Natspec for all the documentation\n D. None of the above\nCorrect is B, C.\nA: With Solidity version 0.8, overflows (eg. from addition or multiplication) can still happen within unsafe blocks.\nB: According to the ERC20 specification, the decimals() function should return uint8.\nC: It's not followed everywhere, eg.: The mint() function's @param is not following Natspec because it's missing one slash.","question-3-of-8#Question 3 of 8":"Which of the following is an/are invariant(s) that should hold true? (assuming no bug)\n A. The contract's ether balance must be strictly equal to the sum of all the balances (in the balances mapping)\n B. For any user, minted[user] <= balances[user] * 10\n C. For any user, token.balanceOf(user) == balances[user]\n D. None of the above\nCorrect is B.\nA: Strict equality with a contract's ether balance is an invariant that would be broken by an injection of ether value into the contract (eg. via SELFDESTRUCT)\nB: This one will hold. A user is able to mint 10 times as many tokens as their ether balance. But they might have a larger balance than actually minted tokens. And most importantly, they should never have more minted token than what they have locked.\nC: A user could transfer their token balance breaking the invariant.","question-4-of-8#Question 4 of 8":"Which of the following sentence(s) is/are true regarding getBalances?\n A. getBalances(msg.sender) returns the sender's balance\n B. getBalances reverts if the user's balance is zero\n C. getBalances always returns zero\n D. None of the above\nCorrect is C.While the function never reverts, it doesn't actually return the sender's balance. That's because the balance return variable is shadowed within the function's body (ie. a new variable with the same name is declared and used instead of the correct one). As the default value of unused variables is always zero-like the number returned by this function will always be 0 too.","question-5-of-8#Question 5 of 8":"Which of the following sentence(s) is/are true regarding the balances mapping?\n A. An attacker can increase their balances (theft) from  balances[victim] \n B. An attacker can reset balances[victim]\n C. An attacker can increase their balances to any amount\n D. An attacker cannot compromise the balances mapping\nCorrect is B.There are no vectors for A and C.But the depositTo() function is mistakenly using = instead of =+, therefore resetting the destination's balance. Attacker's can use this to set the balances[victim] to arbitrary values, most likely very small ones.","question-6-of-8#Question 6 of 8":"Which of the following sentence(s) is/are true regarding reentrancies in this contract?\n A. nonReentrant protects the contract from reentrancies\n B. A correct reentrancy protection modifier is not needed if withdraw is refactored to follow the CEI pattern\n C. There are no reentrancy risks\n D. None of the above\nCorrect is B.\nA: The nonReentrant modifier is not correctly implemented and won't protect the contract from reentrancies. (Its require should check for strict equality)\nB: That's correct, following the CEI pattern would mean updating the user's balance before calling the msg.sender, making reentering the contract pointless.\nC: The withdraw function does not follow the CEI pattern at this moment and the reentrancy-guard is broken, therefore there is a reentrancy risk.","question-7-of-8#Question 7 of 8":"The mint function has the following risks (assuming there are no bugs in the other functions)\n A. The user can generate tokens without having locked ether\n B. An attacker can front-run a call to mint and make it revert\n C. minted[msg.sender] = amount * decimals_factor; should be replaced by minted[msg.sender] = amount / decimals_factor;\n D. None of the above\nCorrect is A.\nA: Is possible by exploiting a loss of precision error due to division-before-multiplication in _has_enough_balance(). This effectively allows minting up to 9 tokens without locking any ether.\nB: There's nothing that a frontrunner could do to make a call to mint revert.\nC: That change would not make sense.","question-8-of-8#Question 8 of 8":"The burn and _has_enough_balance functions have the following risks (assuming there are no bugs in the other functions)\n A. The user can unlock their balance without burning the underlying tokens\n B. An attacker can front-run a call to burn and make it revert\n C. burn should use tx.origin instead of msg.sender to prevent access control issues\n D. None of the above\nCorrect is B.\nA: No way to do this.\nB: Due to the strict equality requirement in burn, a frontrunner can transfer 1 wei of token to the user to make the burn-call revert.\nC: Making this change would likely make the contract less secure (opens phishing vector) and would also be bad for composability (only EOAs could be users)."}},"/posts/2023/6/28/differential-fuzzing-on-solidity-fixed-point-libraries":{"title":"Fuzzing Vyper Contracts Using Foundry","data":{"":"This is a guest post by Wu Enbang (aka 0xNorman) created as part of patrickd‚Äôs writing mentorship.\nJune 28, 2023 by 0xNorman","introduction#Introduction":"Security has become one of the core concerns when writing smart contracts. While developers may be familiar with traditional unit tests to test their code, they only provide a basic level of coverage, testing the functionality and business logic of the code. However, they operate under the assumption that the smart contract behaves as expected under given conditions. To further enhance security, we need to employ more advanced testing methodologies such as differential fuzzing.Differential fuzzing is a testing technique that involves executing different implementations of the same function or logic and comparing the results. This technique allows us to verify that the different implementations are equivalent and behave consistently, even when provided with unexpected, invalid, or random inputs. This is different from normal fuzzing which typically tests a single implementation by feeding it a wide range of inputs and monitoring for unexpected behavior, crashes, or security vulnerabilities.In this project, we used Foundry to perform differential fuzzing on different fixed-point libraries (OpenZeppelin, Solmate, Solady and prb-math). Notice that among several major releases of Solmate libraries (and they did change a lot), we used the latest release (v6) of Solmate library because we think that most of the new projects which choose to import Solmate library will select this version. Our initial motivation was to check if there were any significant differences in the results returned by these libraries. Despite our exhaustive efforts, we found broad compatibility among these libraries, with some differences in handling edge cases and gas efficiency.","implementation-differences#Implementation differences":"We will illustrate the differences in the implementation of these libraries through the following examples.In the Log2 functions (Return the log in base 2), if the input is 0, the implementations return different results for OpenZeppelin and Solady. OpenZeppelin will directly return 0, while Solady will return 0x5be3aa5c (which is the signature of Log2Undefined()). So we add if (num != 0){:javascript} to the test cases to avoid this difference.A similar difference appears in the mulDiv() functions. We need to make sure that the divisor is not zero and it's not offset. Otherwise, OZ and solady will complain differently. For example, Solady will return 0xad251c27, which is the signature of MulDivFailed() so we add if statements as well.To reproduce the error cases in the test, please comment out the if statements and then run specific test cases with traces. And please refer to the library implementations for why it's being handled like that.","gas-reports#Gas reports":"Since the functions in these libraries are mostly compatible, we also compared their gas consumption.Overall gas snapshot can be found in the .gas-snapshot file. The simplified gas report version is also attached below for better readability ( avg gas consumption for each function call ).\nor you can run this command to get it.\nWe found that the gas consumption of the OpenZeppelin implementation is much higher than others. In particular, functions like\nOzLog2, OzLog2Up, OzMulDivDown, OzMulDivUp, and OzSqrt consume more gas than their counterparts in other libraries.Among the rest, solady is the most gas efficient one. All functions compared in the solady library have relatively low gas consumption, and it provides the most comprehensive functionalities,  which makes it preferred for advanced users. However, it's worth noting that only OpenZeppelin and Solmate have been audited by external security teams. And even Solmate is not designed with user safety in mind, so before shooting your foot, one should thoroughly read the documentation and understand the library.The good news is, Cantina decided to audit Solady library as a public goods project, you can monitor their status here","proper-fuzz-campaigns#Proper fuzz campaigns":"To ensure the correctness of the fuzzing, we ran the proper fuzzing campaigns with 9999999 runs. Typically it's not necessary to run that many times during development, but as security researchers we want to cover as many cases as possible.You can change the number of runs in  foundry.toml. More runs mean there are more random inputs fed into the functions. If you instead want to run quick tests, eg. for CI, adjust the configuration according to your needs.","build-on-our-example#Build on our example":"In the vast landscape of Solidity libraries, fixed-point arithmetic is just one aspect. To expand the scope of this research, we could compare libraries handling different functionalities, with different pragma versions. This would provide a broader understanding of the security landscape across different use cases and Solidity versions.Furthermore, we could leverage scripting to automate the input generation and analysis process for the forge test. This would allow us to identify potential bias in the inputs and ensure a fair and comprehensive evaluation of the libraries.","conclusion#Conclusion":"The primary intent of our differential fuzzing campaign was to uncover any significant discrepancies in the outputs produced by Solidity fixed-point libraries of OpenZeppelin, Solmate, Solady, and PRB-Math. Although the libraries demonstrated broad compatibility, our extensive testing uncovered minor differences in the way they handle edge cases, particularly within the Log2 and mulDiv functions. While these disparities might seem minor, they could lead to unexpected behaviors in smart contracts, hence their importance.Through this work, we hope to provide developers with detailed insights that can guide their decisions when choosing libraries for their smart contract applications. Our study didn't find any significant deviations among the libraries. However, the slight differences we observed serve as a reminder of the importance of thorough testing and understanding of the code we use.In the world of blockchain, where code is law, ensuring the robustness and security of our smart contracts is paramount. The differential fuzzing of Solidity libraries is one step towards achieving this goal, providing a higher level of assurance than unit tests alone can offer. We look forward to seeing how this methodology will be employed and further developed by the web3 development community."}},"/posts/2023/7/3/race-19-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #19 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-19, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors.   It was designed by Secureum Mentor Pashov, an independent smart contract security researcher.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nJuly 1, 2023 by patrickd","code#Code":"All 8 questions in this RACE are based on the below contracts. This is the same contracts you will see for all the 8 questions in this RACE. The question is below the shown contracts.","question-1-of-8#Question 1 of 8":"The deployment concern(s) here for different EVM-compatible chains is/are:\n A. receive method behavior might be undefined \n B. The presence of ecrecover precompile is potentially dangerous\n C. Not all opcodes in the bytecode are guaranteed to be supported\n D. None of the above\nCorrect is C.\nA: The receive method (as well as the fallback method) is a Solidity construct and won't be influenced by different EVM versions or alternative chains.\nB: The mere presence of ecrecover by itself is not potentially dangerous in regards to the deployment of these contracts. It certainly is a critical thing to thoroughly review contracts like these though.\nC: Full EVM-compatibility may not be guaranteed with all chains since they might be slower to adapt newly introduced changes. For example, Ethereum recently added the PUSH0 opcode to the EVM and the Solidity compiler will make use of it starting with version 0.8.20. Arbitrum has not added PUSH0 to their EVM yet causing issues when such compiled contracts are attempted to be deployed there.","question-2-of-8#Question 2 of 8":"The security concern(s) in WalletFactory is/are:\n A. ETH funds may get stuck in it forever\n B. The deploy method is not marked as payable\n C. No access control on wallet deployment\n D. Deployment may silently fail\nCorrect is A, D.\nA: Generally, one could inject funds into WalletFactory using self-destruct and coinbase. But in this specific case it could happen because the success-vaule of the send() method is not checked when the ether is transferred to the Wallet after its deployment with deployAndLoad(). The transfer could fail and then the funds would get stuck in the factory. To fix this, the sendValue() method of the already included Address library should be used instead.\nB: There's no need for the deploy() method to be payable, even if it's internally called by a payable function. That is because the \"must not have value\"-check is part of the function selector and would be skipped during an internal call. When called publicly, there's also no need for it to be payable since it does not deal with any value itself.\nC: Assuming that anyone is free to use the factory to deploy their wallet (as it commonly is), there's no concern with people deploying wallets without access control.\nD: The create2 call would return the zero-address if the deployment of a Wallet via the factory failed. In that case deployAndLoad() would burn any msg.value by sending it to the zero-address.\nParticipants have pointed out that, with the missing send-success-check alone within the context of the code as-is, funds cannot get stuck. That may be correct (until proven otherwise), but it's still a breach of a best practice that would've cost very little and may have prevented a loss of funds. We'd recommend reporting issues like these during an audit, even if there's no obvious way to exploit them.","question-3-of-8#Question 3 of 8":"Design flaw(s) of Wallet is/are:\n A. Missing wallet owner role and appropriate access control\n B. Inability to rescue stuck tokens\n C. Assembly usage is unsafe for the Yul IR pipeline\n D. Calling a payable method in a for-loop\nCorrect is A.\nA: The wallet has no owner roles and basically no access control at all. The signature check may look like such, but it has no real effect at all since it would accept any signatures either signed by the msg.sender or by the specified transaction.from which can be freely chosen.\nB: The execute() function allows making arbitrary external calls which would allow to rescue any stuck tokens. (Or in this case, allows stealing them.)\nC: The usage of assembly cannot be called unsafe, but it is indeed not optimal since the inline assembly-blacks are not marked as memory-safe, preventing the optimizer from doing its best job.\nD: This isn't an issue since msg.value is not relied upon.","question-4-of-8#Question 4 of 8":"The security concern(s) with hashing of transaction parameter in execute is/are:\n A. Cross-contract replay attacks\n B. Cross-chain replay attacks\n C. keccak256 hash collision attacks\n D. Reentrancy attacks\nCorrect is B.\nA: Cross-contract replay would not be possible thanks to the inclusion of address(this) (the Wallet contract's address) within the message hash.\nB: Cross-chain replay attacks are possible due to the missing block.chainid within the message hash. This would enable an attacker to replay a published signed transaction on another chain, potentially stealing tokens.\nC: The way the message-hashing has been implemented, a hash collision attack is very unlikely (assuming the attacker has no supercomputers). If the code used encodePacked() instead and used variable-length values in other places than the end of the message, an attack vector would become more likely.\nD: While it's true that CEI is not followed and that could lead to issues, the question's context only concerns the hashing of transaction parameter in execute ‚Äì therefore the fact that reentrancy attacks would be possible, is of no relevance here. This option was purposefully misleading and Question 7 was intended to check whether the reentrancy issue was noticed.","question-5-of-8#Question 5 of 8":"If the hashed payload in execute were to exclude a nonce, the security concern(s) with ecrecover would be:\n A. Signature malleability by flipping the s or v values\n B. Signature malleability by using compact signatures\n C. Signature malleability by hash collisions\n D. Forcefully reverting transactions\nCorrect is A, (B).\nA: Due to the symmetric nature of Elliptic Curve Cryptography, every signature has another valid value signing the same message (by flipping to the other side of the curve by changing the value of v). Similarly, there's another \"lower s\" value that would be accepted as a valid signature for the same message and can be calculated from the original signature. ECDSA libraries like OpenZeppelin's will prevent this from being exploited.\nB: ERC-2098 introduced so called \"compact signatures\" which are now accepted by ecrecover as well. A known signature can be compacted and still stay valid for the same message. Due to this, malleability is inherent to ECDSA signatures in Ethereum, and that is why one should never rely on them as unique identifiers (and use nonces instead).\nC: Nonsensical option.\nD: Nonsensical option.\nAs reported by one of the participants, this answer wasn't quite correct: Compact signatures wouldn't work in this example, as in those, the v value is taken from the 64th byte, not the 65th. So B should not have been a correct answer.","question-6-of-8#Question 6 of 8":"The security concern(s) with Wallet is/are:\n A. Ether sent to the contract will be stuck forever\n B. Anyone can execute arbitrary calls\n C. Anyone can steal the contract ETH balance\n D. None of the above \nCorrect is B, C.\nA: The execute() function allows making raw calls with value. Therefore it would be possible to transfer ether and wouldn't be stuck forever.\nB: As already mentioned, Wallet is missing proper authentication allowing anyone to execute arbitrary external calls.\nC: See the explanations of A & B.","question-7-of-8#Question 7 of 8":"The nonce best practice(s) not followed correctly is/are:\n A. Nonce is not incremented before the low-level call\n B. Nonce is not guaranteed to be included in the signature\n C. Nonce is not incremented correctly on transaction execution\n D. None of the above\nCorrect is A, C.The if-clause that contains the external call would return; before ever reaching the code that would increment the nonce. But even without that, the Checks-Effects-Interactions pattern is not followed which would allow a called contract to reenter and reuse the same transaction.","question-8-of-8#Question 8 of 8":"The security concern(s) with Wallet contract related to ERC721 tokens is/are:\n A. There is no way to get ERC721 tokens out of the contract\n B. Failure to receive ERC721 tokens depending on the transfer method\n C. Failure to receive any ERC721 tokens \n D. Unauthorized burning of ERC721 tokens\nCorrect is B, D.\nA: Since arbitrary external calls are possible through the execute() function, there's no worry of ERC721 tokens to get stuck.\nB: Because the onERC721Received() method is missing, the contract would not be able to receive ERC721 tokens if the safeTransfer() method would be used.\nC: A normal transfer of ERC721 that does not check for the hook/callback mentioned in B would still allow the contract to receive some tokens.\nD: Anyone can burn tokens, not only through the burn-specific methods that are missing access control, but also through arbitrary calls made through execute()."}},"/posts/2023/7/16/secureum-a-maze-x-ctf-2023-at-defi-security-summit":{"title":"Secureum A-MAZE-X CTF 2023 At DeFi Security Summit","data":{"":"July 17, 2023 by patrickd\nOnce more, Secureum is holding a Capture The Flag event. This time as part of the DeFi Security Summit's 101 event happening shortly before EthCC in Paris.Let's take a look!","challenge-1-operation-magic-redemption#Challenge 1: Operation magic redemption":"A prominent protocol, InsecStar, finds itself under attack. Their token, MagicETH (mETH), has been drained through an exploit in their borrow & loan protocol.InsecStar has urgently summoned you to devise a method to recover the stolen tokens and redeem them for ETH before the situation worsens. This is a critical test of your capabilities. Can you rise to the occasion and secure the tokens, thereby reinforcing the strength and resilience of the Ethereum ecosystem?üìå Recover 1000 mETH from the exploiter wallet.üìå Convert the mETH to ETH to avoid further losses.\nOh, a backhack! Similar to whitehacks these attempts to rescue funds can backfire fatally when executed badly. Especially when executed on Ethereum's mainnet where Frontrunners are just waiting for a juicy opportunity ‚Äì that's unlikely to be the case on our local development environment though.Let's take a look at the setup process and success conditions of this challenge:\nA thousand ether are deposited into the mETH contract and that same amount of mETH tokens are then immediately given to the exploiter. So it seems we won't get insight into how the exploit was actually executed here ‚Äì the setup makes it as if everything has already happened. The success condition is a single assertion requiring the \"whitehat\" account, that we're controlling, to own all of the ether contained in the mETH protocol.Initially, I thought that the hacker might have used an exploitation contract, but it seems like the \"exploiter\" account is a simple EOA (meaning a wallet with a public key-pair). So we won't have to find issues in an exploitation contract to hack-back the funds, we're likely expected to use the same vulnerability that the exploiter supposedly used too to reobtain the mETH ERC20 tokens and then quickly use them to withdraw the ether so that they can't be stolen again.A first look at MagicETH reveals that it is a weird mix of a Vault and Wrapper function. The deposit() function gives you exactly as many mETH in return as you put ether in. And the withdraw() function will return ether to you in exchange for mETH, but since mETH can be burned, it's possible to receive more ether than you have mETH. Anyway, we (the whitehat) have neither ether nor mETH to use either of these methods.\nThings get more interesting in the burnFrom() method that is supposed to enable users to burn mETH tokens from accounts that gave them an allowance to do so. The allowance() function, inherited from OpenZeppelin's standard ERC20 contract, has the following parameters:\nOn the other hand, the _approve() method, looks like this:\nSo instead of loading how much the account has approved to the sender, MagicETH checks how much the sender has approved to the account ‚Äì it's the wrong way around for the purposes of this function. And worse, later it overwrites the correct approval amount with the incorrect amount that it had loaded. Therefore the sender can give the account an allowance, call burnFrom() and turn this allowance around so that the account has suddenly given it to the sender as well.\nAnd with that, we have solved the first challenge!","challenge-2-mission-modern-weth-rescue-the-ether#Challenge 2: Mission Modern WETH: Rescue the Ether":"In the ever-evolving world of decentralized finance, an ambitious developer took it upon himself to update the well-established WETH9. The result was ModernWETH, a modernized version in Solidity, that rapidly attracted deposits of over 1000 Ether.However, we've encountered a challenge. Hidden within the code, a potential vulnerability threatens the security of the funds locked within the contract. This situation calls for the dedication and expertise of blockchain security auditors. Are you ready to step up, solve this issue, and play a crucial role in preserving the sanctity of the Ethereum ecosystem? This is the test of our resolve and adaptability, something I've seen in this community time and again.üìå Starting with 10 ETH, recover 1000 ETH from the ModernWETH contract.üìå Recover all ETH to avoid further losses from ModernWETH contract. Whitehat hacker should end up with 1010 ETH.\nThe WETH9 contract, commonly simply referred to as WETH, is a simple wrapper contract that allows you to change your native ether into ERC20-compatible tokens. Many protocols prefer dealing with WETH instead of the native token as it reduces complexity: The project only needs one code path for any ERC20 token and no special native handling is necessary.\nOne of the first things to notice is that the Checks-Effects-Interactions pattern is not followed: External calls to another address (the msg.sender) are made and the effects (burn()/burnAll()) are only applied afterward. Unsafe external calls like these can allow for reentrancy, in this case, the msg.sender could reenter the ModernWETH contract and have the same or other functions of the contract operate on an outdated state.But wait! The nonReentrant modifier is present, which is a mutex that only allows to reenter once the first function execution has been completed. So with that in place a reentrancy shouldn't be possible! ‚Äì And this is where this challenge tricks you: The mutex is only applied to the methods that have specifically been annotated with it ‚Äì which isn't the case with inherited functions here! So while we can't re-enter through the functions with the modifier, nothing is stopping us from re-entering the contract through another function which does not have it.But which one? Looking at OpenZeppelin's _burn() method, it's clear that it would revert with ERC20InsufficientBalance if someone attempts to burn more tokens than the specified account has. So if we were to reenter through transfer() during the call into withdraw() and attempt transferring the tokens to somewhere else, the _burn() method would revert as the amount (wad) is suddenly insufficient.But that isn't true for withdrawAll() which uses _burnAll() and burns the current amount of tokens after the ether amount has already been transferred. So the exploit is simple: We write a contract that calls withdrawAll() for our 10 mWETH and when ModernWETH makes the external call to send the ETH this exploit contract can transfer the 10 mWETH back to us so that they don't get burned. Do that 100 times and we're done!","challenge-3-lendex-pool-hack#Challenge 3: LendEx pool hack":"In the realm of decentralized finance, where trust is often bestowed upon code, a groundbreaking borrowing and lending platform known as LendEx was created.Unbeknownst to the LendEx team, a hacker hid a bug in the LendingPool smart contract with a intention to exploit the bug later. LendEx team reviewed smart contract source code, approved it for the usage and deposited the funds from the LendExGovernor contract to the LendingPool contract.Do you have what it takes to spot how hacker is planning to exploit the LendEx?üìå You have to fill the shoes of the hacker and execute the exploit by stealing stablecoins from a lending pool.üìå Note: Foundry has a bug. If a selfdestruct() is triggered in a test script then it has to be done in the setUp() function and the rest of the code should be in a different function otherwise foundry test script does not see that selfdestruct happened to a contract.\nFrom the description I assume there's some very tricky way how the code is exploitable that isn't obvious at all, maybe it's some sort of Solidity weirdness ‚Äì maybe it's one of the previous submissions to the Underhanded Solidity Contest? Let's start by checking the setup and winning conditions again since they weren't really clear from the intro:\nThe setup is quite complicated this time around ‚Äì two \"deployer\" contracts? What are those for? Later it seems that all USDC created is deposited into the pool. The solve-conditions might actually be giving something away here: Apparently, the \"governor\" contract needs to have a \"LendingPool hack\" contract at the same address where the original pool was.From the code and the description it now sounds like we need to do the following: Somehow trigger the original pool to self-destruct, then use create2 to deploy a malicious pool at that address, and finally use the malicious pool to steal all the stablecoin tokens.The LendExGovernor contract appears to be simple: There are a few getters and setters, but at first glance they all seem fine and I doubt we'll have to do anything here.After that, I opened the LendingPool contract and the first thing I searched for was selfdestruct, and indeed:\nSelf-destructing a contract forces you to send all of the contract's ether balance to a specified address, here the zero-address, which is basically the same as burning all of it forever. But this pool isn't supposed to hold any ether in the first place, it'll hold USDC. Additionally, the contract's storage values will be purged as well, but this won't affect the USDC balance as that is stored in the USDC contract, not the pool. So even after self-destructing this address will still hold the USDC balance that we can recover if we're able to deploy a new contract at the same address.Another vector for causing the self-destruction of a contract is a delegate-call. If the pool contract would delegate-call another address, and we'd be able to specify the destination of this call, we could have a malicious contract take over the execution flow and self-destruct the caller. But thinking about it more, there'd be no need to self-destruct since, at that point, we could simply transfer the USDC tokens in the pool's name. Anyway, there's no delegate-call here.The question now is, how can we trigger the emergencyStop() method? It has an onlyOwner modifier which seems fine. The owner is set once during construction and there seems to be no obvious way to overwrite it. It also doesn't appear to be internally called anywhere. And there's no suspicious usage of assembly either. Is there a hidden unicode character sneakily hiding the code's real behavior? Maybe the weird expression evaluation order when emitting Solidity events is used to hide a malicious state update? Maybe there's a storage clash and we can overwrite the contract owner by using the other public methods?Let's approach it from the other side: Assuming we had already destructed the pool, how would we be able to deploy the LendingHack contract at that address?Remember the setup? First, the Create2Deployer was created, which seems to deploy the CreateDeployer via create2, or rather: with a static salt and therefore always at the same address.\nThe CreateDeployer contract will deploy the pool via a normal new, meaning that it uses the contract's nonce instead of a salt to determine the deployment address. That means that every time the deploy() function is called, the pool would be deployed at a different address unless we call cleanup() first in order to reset the nonce.\nBased on this new information, we now have to do the following to build on the previous assumptions: After self-destructing the pool, we also have to self-destruct CreateDeployer by calling cleanUp(). Then we'll be able to deploy LendingHack at its original address by calling Create2Deployer's permissionless deploy() function. But once more, the self-destruct is behind a onlyOwner modifier, this time in a contract with much less complex code ‚Äì so how to do it?If you were paying attention, you should've noticed by now that I made an incorrect assumption right at the beginning: I assumed that the exploiter wouldn't be the owner ‚Äì why would he be, right? If that were the case we could just call self-destruct on everything without anything stopping us. Well, the reason why that is the case, is that this challenge is supposed to reflect what happened during the Tornado Cash hack and how the exploiter gained control via the governance that the contracts were deployed with. Admittedly, that isn't quite well reflected here, it would be too complex to do so.\nNote that more comments were added to the setup now to make it more obvious who the owner is","challenge-4#Challenge 4":"Challenge 4 was removed due to an error: It assumed that the destination address of the CREATE2 operation is based on the msg.sender, but it is actually based on the contract's address that is executing the operation (address(this)). This confusion stemmed from the fact that this address is commonly referred to as the \"sender\" in EVM documentations. Furthermore, this challenge was very sensitive to changes (comments, filenames, folders, compiler version, etc) that impacted the resulting bytecode and therefore the target address, making it appear as if the msg.sender were relevant.","challenge-5-balloon-vault#Challenge 5: Balloon Vault":"A ERC4626 vault known as the \"Balloon Vault\" has been built to gather WETH and invest it on multiple strategies. This vault was thought to be impenetrable, designed meticulously to maintain the security and integrity of the tokens stored within.The process was straightforward: individuals deposited their digital assets into the Balloon Vault, receiving shares in return. These shares represented their holdings and served as a way to track their savings.Two users of the vault, Alice and Bob, have fallen prey to a potential security vulnerability, jeopardizing their significant holdings of 500 WETH each. Protocol try to reach them with no luck...You have been summoned by the custodians of the Balloon Vault, challenged to assess and exploit the lurking vulnerability, and drain the wallets of Alice and Bob before a bad actor do it. By successfully accomplishing this, you rescue 1000 WETH from Alice&Bob.üìå Drain Bob's wallet and Alice's walletüìå End up with more than 1000 ETH in your wallet\nWhenever I hear Vault, especially when it's ERC4626, I think: Inflation Attack. Basically, an attacker can steal funds of such Vaults by frontrunning user deposits. So that was my first thought, but after continuing to read the description I'm not so sure anymore. Let's start by looking at the setup:\nWhat's interesting here: Alice's and Bob's funds have only been approved for usage to the Vault ‚Äì they haven't actually been used yet. So the Vault is still empty and the Inflation Attack could still be possible ‚Äì under the condition that it's possible for the attacker to deposit their approved funds without needing their permission.\nThe BallonVault itself basically just extends OpenZeppelin's ERC4626 implementation by a single function. And this function looks just like another vector: Phantom Functions. This vector allows us to do exactly what we need: Being able to deposit funds that are already approved to the Vault, without actually having gotten the permission from the fund's owners to do so.But first of all, what are Phantom Functions? Basically it's a type of vulnerability where the caller relies on the called contract to have a certain function implemented. This assumed function is supposed to execute some sort of check and revert when these checks failed. Such a function could be the permit() method, which is used in this case as well.What is the permit() method expected to do? In the above code, the depositWithPermit() function passes a signature to it expecting the following: If the signature is valid, the specified from address has signed a message, proving that this address intends to give the BaloonVault contract an allowance to use, in this case, for a deposit. But there's a second assumption here: That the person calling the depositWithPermit() function could only do this (ie. be in possession of this signature) if the owner of the funds (from) has given them the permission to make use of it.And this is where the problem lies: If the signature is invalid, depositWithPermit() expects the permit() method to revert. But in the case of WETH, it won't revert and it'll continue making the deposit under the assumption that a correct signature was passed. If you've looked at this challenge's WETH contract you'd now rightfully protest and say: But wait! The WETH contract does not have a permit() method, so the call to it should fail! ‚Äì And you'd be correct. Except for the case that the call made to WETH's permit() will be picked up by its fallback() handler instead, which will just silently have the call succeed. And that's what makes permit() a Phantom Function: Even though it doesn't exist within the WETH contract, calling it will still appear to have succeeded as if the signature was valid.So thanks to the fact that Alice and Bob have already given approval of their funds to the BallonVault contract, we just have to pass an invalid signature to deposit their funds into the Vault. This still doesn't give us access to their funds, since the receivers of the shares will still be Alice and Bob respectively. But here's where the Inflation Attack comes into play: We can ensure that they don't get any shares at all, while we're the only ones who have a share that can be redeemed for all of their deposited funds.An Inflation Attack basically consists of the following steps:\nThe attacker deposits 1 wei of an asset into an empty vault, getting 1 wei back in the form of the Vaults share token.\nThe attacker then front-runs a user's deposit request (in this special case, the attacker actually initiates the user's deposit themselves through the Phantom Function vulnerability). Before the user's deposit is made, the attacker donates (ie. sending it directly into the Vault without using the deposit function and without getting any shares) the same amount of assets into the Vault.\nWhen the user's deposit finally happens, the user obtains 0 shares in exchange for their assets. The reason for this is that the formula calculating the number of shares was manipulated by the donation of assets, causing the user's deposit to be rounded down to 0.\nThe attacker is the only one in possession of Vault shares (1 wei) and can redeem this to obtain all of the Vaults assets. Due to the fact that the attacker only has 10 ETH to start with while Alice and Bob have 500, we'll have to execute these steps multiple times. That's because for this attack to work the attacker must be able to match the deposit made by the user.\nOpenZeppelin's ERC4626 implementation actually introduced a mitigation against this attack that doesn't fully prevent it, but at least makes it a lot harder to profit from it. Lucky for us this fix was made in version 4.9, and the release used here is 4.8 ‚Äì exactly one minor version before the issue was fixed!In this version, the shares are determined by a simple formula:After the attacker deposited a single wei of assets, the totalAssets become 1 wei and the totalSupply (ie. supply of vault share tokens) becomes 1 wei as well. Let's say that a user wants to deposit 100 wei into the Vault, but before that happens the attacker donates 100 of their own wei, increasing totalAssets to 101 while the totalSupply stays at 1 wei.Rounded down to integers, this will result in 0 shares for the user. The same principle applies to our attack, just with bigger numbers.","challenge-6-safe-yield#Challenge 6: Safe Yield?":"You got your hands on 0.1 ETH, and of course you would like to stack more. Luckily there's a promising DeFi protocol which allows depositors to earn fees on both dex swaps and flash loans from others. But it takes so long to earn any meaningful amount...Can you do faster?üìå Drain at least 100 ETH from the yield pool\nThat sounds like some sort of price manipulation should be possible.. Maybe the price is influenced simply by taking a flash loan? Let's see..\nIt appears that in the initial setup, there'll be a 1-to-1 ratio between the ETH and the Token in the YieldPool.\nA quick glance at YieldPool's swapping functions shows that indeed: The swap amounts are calculated based on the current token and ether balances and there appears to be nothing that prevents swapping while a flash loan has been taken.Going ahead with these assumptions, the exploitation strategy to obtain as much ether as possible would be to flash loan a lot of ST Token from the pool. This would decrease the YieldPool's holdings as if there were a high demand for these tokens making them very expensive while making ETH very cheap.\nSwapping would revert if we flash loaned all of the tokens (\"invalid reserves\"), and anyway, we'll need to buy some of these tokens to make the actual swap and also to pay the borrowing fee. To get a feeling for this, let's borrow all but 1 wei of Tokens and see how the price changes.\nAt first, we'd get around 0.99 ETH for 1 Token. Then after having taken the flashloan we'd get around 9999.99 ETH for 1 Token. Nice! But there are a few problems: The fee for borrowing all this is 99.99 Token, which, if not paid until the callback returns control to the pool, will revert the entire transaction. At the moment we only have 0.1 ETH and no Tokens at all. And even if we swapped all our ETH for Tokens, we'd only be able to obtain a similarly small amount of Tokens which wouldn't allow us to afford any impactful flash loan due to the 1% borrowing fee.To make the attack profitable, there's another important part missing in how flash loans are paid back:\nAfter our flash loan callback finished and the flashLoan() function takes over again, it has no way to get its ETH back and needs to rely on us sending it back. It validates whether we did so by checking whether the Pool's ETH balance is as much as it was before plus the 1% lending fee. So there's no need for us to send it back via a specific function, it just needs to end up in the contracts balance ‚Äì which means: We can pay our debts back as part of a swap!The easiest option to exploit this is the following steps:\nUse the 0.1 ETH to flash loan 10 ETH.\nWhile the price is skewed, use 10.1 ETH to buy Tokens. This also at the same time repays the loan and its fees.\nThese unfairly obtained Tokens can now be exchanged for ETH again.\nAnd we can do this over and over again until we have enough!","challenge-7-crystal-dao#Challenge 7: Crystal DAO":"The Crystal DAO is a transparent and non-profit DAO whose mission is to gather funds to support the development of different public goods in the Ethereum ecosystem. These funds are stored in custom treasury contracts that follow the ERC1176 minimal proxy standard, and are controlled by one DAO admin.One of such treasuries was recently deployed, and has reached the target amount of 100 ETH in its balance. Therefore, the DAO admin has tried to retrieve the funds for their subsequent donations. However, the admin's signature is not being recognized by the treasury clone contract, and the funds are now stuck, putting the DAO's reputation at risk.Can you help the DAO admin to retrieve the funds?üìå Rescue 100 ETH from the DAO treasury\nFrom the description alone, I'm unable to tell what the issue is. The goal is clear and reflected as such in the tests. But what's more interesting is the \"failed attempt\" by the DAO admin it contains:\nMy first suspicion is that the signed message is simply not being built correctly.\nBut the debug output shows that the address recovered from the signature and hash (via ecrecover) is indeed the daoManager. If the message hash signed would be incorrect, this couldn't be the case.\nLooking at the relevant code for this revert message: Could it be that the owner state variable was set incorrectly?\nA quick test shows that indeed, the owner of the vault contract queried is the zero-address, so no wonder!It's not important how it ended up being that way (probably something about the assembly init), I already know enough to solve the challenge. The contract isn't doing any validation on the result of ecrecover except for the comparison with the owner variable. That's bad, but great for us! Because as it turns out: When ecrecover fails to determine a signer's address it won't revert, instead it'll simply return the zero-address as a sign that something went wrong. So in order to \"sign as the owner\" of this Vault, we simply send an invalid signature.","challenge-8-liquidatoooor#Challenge 8: Liquidatoooor":"The favorite lending protocol in town has opened its doors and is allowing anyone to deposit collateral to borrow debt tokens! The Risk analysis department assures the protocol is sound as a Swiss banking system, and the Tokenomic analysis team argues that if a user's position becomes under-collateralized, the liquidator must receive all of the users collateral as a reward for keeping the protocol safe from bad debt, while punishing the borrower for not managing his positions accordingly!As users start opening debt positions, you notice something unusual in the way that the protocol calculates user account health... something is off here... and it seems that the consequences can result in user positions being liquidated by the attacker who will also make a profit out of it!Can you demonstrate the viability of this attack to convince the Risk and Tokenomic departments to urgently update the protocol?üìå Drop the borrower's health accountüìå Liquidate the borrower and get as much of his collateral as possible\nThe set-up steps of this challenge seem quite complex, but they can be broken down into a few points:\nTwo simple ERC20 Token are deployed (TKN, DAI)\nAn Automated Market Maker (AMM) is deployed\nThe Lending Contract is deployed (Oiler)\nWe (the player) get 100 TKN, 100 DAI\nA \"Superman\" gets 200 TKN, 200 DAI\nSuperman adds 100 TKN and 100 DAI liquidity to the AMM\nSuperman deposits 100 TKN into Oiler and borrows 75 dTOKEN\nThe most interesting part while going through this was probably the following comment:\nWhenever you hear that prices are fetched from an AMM, you should get a little worried. In this specific case, we wouldn't even require flash loans to manipulate the price as the market only has as much liquidity as we ourselves own. That means it should be a simple exercise to significantly manipulate the market with a swap, which will likely influence superman's collateral health.Since \"superman\" used TKN as collateral, let's sell all of our TKNs in the AMM to dump the price and check how this impacts the healthFactor():\nAlthough I have no clue yet what exactly the health factor is, it's clear that we succeeded in manipulating it:\nIt went down!\nAccording to the code, a user is \"underwater\" (ie. can be liquidated) once the following is true:With the price manipulation, we managed to bump it down below the threshold, but as a small repayment has to be burned from the liquidator's balance, we first have to make a deposit ourselves.Luckily \"superman\" was already on the threshold towards liquidation before we manipulated anything. So we actually don't have to make use of our entire token balance.\nThat's it! Thanks to everyone who designed these challenges, until next time!"}},"/posts/2023/7/30/race-20-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #20 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-12, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors.   This one was designed by none other than Hari (@hrkrshnn), Secureum Mentor and Co-Founder of Spearbit. With his background in working on the Solidity compiler this one was sure to be a doozy‚Ä¶\nBefore tackling this race, participants were recommended to review EVM basics as well as spend some time understanding inline-assembly. Aside from that, it‚Äôs the usual 8 questions within the strict time limit of 16 minutes.As always, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nJuly 27, 2023 by patrickd","code#Code":"The first four questions are based on the below library:","question-1-of-8#Question 1 of 8":"Select all true statements:\n A. The inline assembly block is memory-safe\n B. The memory after toString(...) call is always 32-byte aligned\n C. Instead of allocating memory from 0x40, the function can allocate from 0x0 to save gas (memory expansion cost) and still be correct\n D. None of the above\nCorrect is A.A: The inline assembly block is indeed memory-safe, as it only writes to allowed memory ranges. In this specific case it always allocates memory starting from the free memory pointer. A \"pointer\" is an address within memory and the free memory pointer is the address pointing to the start of unallocated, free memory. This pointer itself is stored in memory at address 0x40 and, unless the value that was just stored in memory is temporary, it should be updated to skip over any recently allocated memory.B: While the memory itself is allocated in chunks of 32-byte slots, the values within these slots will not be properly aligned to each slot. First, note that each string stored in memory consists of two components: The first 32 bytes of a string are always its length. After the length, each further 32 bytes slot will contain part of the actual string's value. Therefore, if you want a pointer to the beginning of a string, you have to add 32 bytes to the string's pointer.Step #0: Initial memory state before toString(1) is executed\nStep #1: Memory pointer was updated and initial length of string was written\nStep #2: The char for \"1\" is determined to be 0x31 (48+1), its value is then stored to (str + k) 0x80 + 0x4E = 0xCE\nRemember that MSTORE writes chunks of 32 bytes, so when it writes 0x31 to 0xCE the byte carrying the value will end up in 0xE0's slot because of all the leading zeros (). The number is being converted to a string from right to left. If it would be done the other way around, previous numbers to the left would be overwritten by the following character being placed in memory. By doing it right to left, the MSTOREs won't impact the characters already written on the right.But this is also why a short number such as 1 would end up with many leading zero-bytes in front of it. To prevent this from happening, the str reference variable is updated to point to the 32 bytes before the string starts ‚Äì causing the string's start to no longer be aligned with the beginning of a 32 byte slot. This is the place where finally the string's length will be set at:\nC: The 64 bytes starting at memory address 0x0 are called the \"scratch space\". While they're usually used for operations like hashing, you're free to use them to temporarily store things there and leave them without having to clean them up. But there's no guarantee that anything placed in scratch space will remain there for long as any other part of the contract may use it for temporary operations as well. This function's purpose would be harmed if the returned string would be overwritten at random, therefore, while it may safe gas to attempt doing so, it cannot be said that it would still be correct to use the memory at 0x0 instead of allocating fresh dedicated memory. Furthermore, for the largest uint256, this function requires more memory than the 64-byte scratch space can offer.","question-2-of-8#Question 2 of 8":"Select all true statements about the expression mstore(0x40, add(str, 128)):\n A. The expression allocated more memory than required. The value 128 can be replaced by 96.\n B. The expression allocates less memory than required. The value 128 can be replaced by 160.\n C. The expression is redundant and can be removed to save gas\n D. The expression is not ‚Äòmemory-safe‚Äô assembly in this context\nCorrect is B.A/B: While the expression allocates sufficient memory to ensure that later allocations won't overwrite the string, there's still an issue of dirty memory being returned when the actual string contents are read once the memory after the string is in use:\nLoading 32 bytes to obtain the string's length at the str pointer will work correctly:  loading 32 bytes to obtain the actual string's contents at add(str, 32) will partially return data not belonging to the string:  prevent this, the expression should allocate another 32 bytes, therefore a sum of 160 bytes instead of 128.C: Removing the expression would cause later allocations of memory to use the space where the string has been stored, causing it to be overwritten.D: This expression, as well as the rest of the inline assembly block, is memory-safe as already covered by the previous question.","question-3-of-8#Question 3 of 8":"Select all true statements:\n A. The expression mstore(str, k) at the beginning can be removed to save gas\n B. The expression mstore(add(str, k), char) can be replaced by an equivalent mstore8(...) to simplify the code.\n C. The final expression mstore(str, sub(78, k)) can be removed to save gas\n D. The function does not return the correct output for n = 2**256 - 1\nCorrect is A, B.\nA: The expression storing the string's length at the beginning can indeed be removed to save gas as it's supposed to be overwritten by the actual string's length at the function's end.\nB: As explained in one of the previous answers, MSTORE always writes 32 bytes, which is quite unnecessary (as we're only writing one byte each round) and makes the algorithm more complicated to understand. Using MSTORE8 this could be significantly simplified as it would only touch a single byte in memory, just as needed by this algorithm.\nC: The expression storing the final string's length shouldn't be removed as the string would end up having a length of 0 according to the str pointer pointing at the 32 zero-bytes before the actual string value starts.\nD: The function actually does return the correct output for the maximum uint256 integer value.","question-4-of-8#Question 4 of 8":"Select all true statements:\n A. The function correctly cleans all necessary memory regions\n B. Solidity will correctly be able to handle the string returned by the function\n C. The last bits of memory in the string may be dirty\n D. None of the above\nCorrect is B, C.A: The function does not properly clean all the memory regions that it accesses, this allows dirty bits to remain. Remember that using memory that is still marked \"free\" according to the free-memory-pointer doesn't guarantee that it hasn't been used as a temporary memory by previous operations. Based on the previous toString(1) example, here's how the memory would look like if all of the \"free\" bytes have been set to 0xff before the function was called:\nB: Despite all of the function's imperfections, it'll still be able to appear to be working perfectly fine outside of assembly thanks to Solidity's cleanup measures.C: This once again references the issue of insufficient memory being allocated (See Q2) causing later memory allocations to overlap with the end of the string.\nThe last four questions are based on the below abstract contract:","question-5-of-8#Question 5 of 8":"Select all true statements:\n A. The re-entrancy lock is always unnecessary as it‚Äôs never possible to re-enter the contract\n B. Calls to _delegate are correctly protected for re-entrancy.\n C. The re-entrancy lock is correctly unlocked in some cases\n D. The re-entrancy lock is correctly unlocked in all cases\nCorrect is C.\nA: This is an abstract contract, meaning it'll need be inherited by another which may have more functions, functions that may allow reentering. Furthermore, the implementation contract's functions are unknown as well and they too may allow to re-enter. Based on the available information it's not possible to come to the conclusion that it's \"never possible to re-enter\".\nB: The fact that it's \"protecting\" against reentrancy is more of an happy accident of the bug. It'll prevent reentrancy in practice, but it'll also deadlock the contract forever. With that being the case it cannot be said that calls to the delegation function are \"correctly protected\".\nC/D: The function will always either revert or return. Returning within assembly is a bit different than returning in Solidity: The RETURN opcode will end the current EVM execution context immediately and return the specified data to the external caller. This means that the code in nonReentrant responsible for unlocking is practically unreachable, leaving the contract in a deadlock state. It may still seemingly correctly unlock if the implementation contract's storage variables re-use the same storage slot and unlock it from there. In the reverting case the code responsible for unlocking will be skipped, as intended, and instead the contract is unlocked by the fact that all changes done to the contract's state are reverted.","question-6-of-8#Question 6 of 8":"Select all true statements:\n A. The assembly block is correctly marked as ‚Äòmemory-safe‚Äô\n B. The assembly block will always violate the memory requirements needed for memory-safe blocks\n C. In some cases, the assembly block will not violate the requirement needed for memory-safe blocks\n D. None of the above\nCorrect is C.\nA: The assembly block is purposely not memory-safe (it takes \"full control of memory\") and the reason it gives for it in the first inline comment is that this is not an issue as \"it will not return to Solidity code\". Meaning that it can do whatever it wants as this EVM context will end after its execution. But that isn't actually true anymore due to the usage of the nonReentrant modifier. Such an inconsistency would be a big hint that something smells here during a secure code review.\nB/C: It won't violate the memory requirements as long as the calldata and the returndata is empty or fits into the memory's scratch space.","question-7-of-8#Question 7 of 8":"Select all true statements:\n A. The expression calldatacopy(0, 0, calldatasize()) violates memory-safe assembly annotation\n B. The expression returndatacopy(0, 0, returndatasize()) violates memory-safe assembly annotation\n C. The expression delegatecall(gas(), implementation, 0, calldatasize(), 0, 0) violates memory-safe assembly annotation\n D. The expression return(0, returndatasize()) violates memory-safe assembly annotation\n E. The expression revert(0, returndatasize()) violates memory-safe assembly annotation\n F. None of the above.\nCorrect is A, B.\nA: Copies calldata into memory starting in the scratch space. If calldata doesn't fit it'll violate memory-safety.\nB: Does the same, but with the data returned from the external delegate-call.\nC: The delegate-call operation only reads from memory and the actual execution happens within a fresh EVM context. No violations can happen here.\nD/E: Both operations only read from memory and immediately end the current EVM execution context.","question-8-of-8#Question 8 of 8":"Select all true statements:\n A. delegatecall can never re-enter as the state is shared\n B. delegatecall proxies without proper access controls may be prone to selfdestruct\n C. Proxies are typically used to save deploy-time gas costs\n D. Proxies can be used to prevent contract size limit issues\nCorrect is B, C, D.\nA: Nonsensical filler option.\nB: It's true that delegate-call operations are very dangerous, especially if an attacker is able to get control over the destination address which could be a self-destructing contract.\nC: Extremely small proxies, usually referred to as clones, can be used to deploy code only once and re-use it over and over again.\nD: Proxies may delegate to multiple implementations allowing the code size limit to be bypassed."}},"/posts/2023/9/27/cryptocurrency-privacy-technologies-ring-signatures":{"title":"Cryptocurrency Privacy Technologies: Ring Signatures","data":{"":"September 27, 2023 by patrickd\nDespite being regularly referred to as \"anonymous Internet money\", the ledgers of the most widely adopted cryptocurrencies are completely public. Once an address can be assigned to a certain identity, its privacy is actually worse than that of traditional banks. This article explores the cryptographic primitive that many privacy-focused decentralized currency systems incorporate, focusing specifically on its original version.","the-concept#The Concept":"Ring Signatures were introduced in the year 2001 by the paper \"How to Leak a Secret\" which offered a simple but intuitive usage example: If every member of an organization you're part of has a known public key, including yourself, you can sign and leak the organization's data to the public such that it's possible to prove that the data was indeed signed by a member, without revealing that the member who signed it was you. In terms of privacy, this was a vast improvement over existing \"Group Signatures\", which could be used in a similar manner but required a trusted \"Group Manager\", who could identify you as the signer.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nCompared to a normal signing function, taking in a message  and a private key, a Ring Signature's creation requires a list of all members' public keys of which it will be unknown who actually signed it:Similarly, the verification of a Ring Signature requires that same list of public keys. A verifier can now check if a message was signed by one of the public keys in the list, without revealing which of them:","specifics#Specifics":"While a group of members can already be referred to as a \"ring\", the algorithm laid out by the paper can be represented like a ring as well:At a high level, verifying a ring signature begins with a known initialization variable , which may be random and part of the signature or could just be a statically chosen value such as 0. This variable is continuously updated by the operations applied for each member and is therefore also referred to as the \"glue value\". The important aspect to this variable is that, once each member's part has been mixed into it, the resulting value  should be equal to the initial value , proving that the ring signature is indeed valid.In asymmetric encryption schemes, the public key is commonly used to encrypt data such that only the person in possession of the private key may decrypt it. Roughly the same happens here using each member's known  to encrypt values  which appear to be completely randomly chosen and are part of the data provided as the signature. The resulting values  are then each mixed into the ring.And this is where the trick is: It's impossible for all of the  values to have been chosen at random for this to work. Encrypting purely random values and mixing the resulting  with  would never result in returning to the initialization value. We need control over at least one  value to ensure that the mix will result in . For that, we need to determine a specific  which, when encrypted with , will result in exactly the  needed. To find this, we need to use the inverted encryption function , and that requires the private key .A ring signature, as described by the paper, therefore consists of the following components:For the given public keys, the verification function will return  when, at the end of executing the described algorithm, . If that is the case, we can be sure that at least one  value was not chosen randomly, but was determined using a private key belonging to one of the public keys. Therefore, the signature must have been created by a member of the ring, but since each  looks as random as the other, there's no way to tell which member it was.","the-math#The Math":"The method described by the paper requires some form of invertible trapdoor function to work. Given that most of the paper's authors were also the creators of RSA, it should come as no surprise that they chose the hardness of prime number factorization for their implementations.","rsa#RSA":"Being asymmetric, RSA has a public encryption key  and a decryption key  which is kept secret. Additionally, each key-pair has its own public modulus  which is required for both encryption and decryption functions:Assigning \"encryption\" and \"decryption\" to each function's purpose will cause confusion for understanding how they are used in ring signatures. Note that both functions are inverse to each other, no matter in what order you use them. You could, for example, pick a random integer and feed it into . Even though the integer you chose was not the result of using , the resulting number will still be completely valid. And what's more: You can return to the original randomly chosen number by feeding it into , because it is the inverse of  too.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nFinally, it should be mentioned that the paper wraps  within another function . The reason for this stems from the fact that each public key  has its own modulus for the calculations. This makes things awkward since multiple different moduli are combined into a single ring here. The wrapper function  ensures that all the trap-door permutations are extended to share a \"common domain\". This will be omitted since it has little relevance to grasping the concept.","combining-function#Combining Function":"A Combining Function defines how the  values are mixed with the glue value, initialized as , within the ring signature algorithm.The paper suggests a secure mixing method by first calculating the XOR of the current glue value and the trap-door function's result (). Then it uses a symmetric encryption function  on this result to determine the new glue value to be passed on. This sudden use of symmetric encryption might seem strange but it actually prevents an attack on this scheme that would be possible if only XOR was used. But this is actually also very useful: So far the scheme only proved that someone in the list of public keys knows at least one private key - but none of this referenced any specific message being signed. Therefore, as part of this step, the message is now also being mixed in by using its hash as the symmetric encryption key ().For a ring of size 4 the combining function would therefore look like this:","determining-signers-x#Determining signer's x":"Continuing with the previous example, let's assume that we're in the creation process of the signature and have the private key for . Naturally, a static position of the signer within the ring would defeat the purpose of privacy. All the involved public keys, including the signer's, should be randomly shuffled beforehand.Using the known public keys  of other members we can determine the following without requiring their permission or collaboration:Each  has been randomly chosen.Next, we need to solve the Combining Function for , such that it yields  again (ie. determining 's value such that ).And thanks to knowing the private key  for  we are finally able to determine the  that is guaranteed to result in  during verification.Having  values for every member of the ring, the signature creation process is finished.","the-code#The Code":"Here's a python script implementation of the described Ring Signature method.You may change the ring_size to increase or decrease the amount of members involved in the ring and set the location of the signer within the ring with ring_signer_idx. The symmetric encryption function  is implemented using ChaCha20 in E(), and its inversion  in Ei(). ChaCha20 was simply chosen for its simplicity in usage, it may be replaced by other symmetric encryption algorithms. The trap-door function  is contained within the mentioned wrapper g() and may be used as its inversion  simply by passing in  instead of .\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThe forward() and backward() functions respectively contain a single step clockwise or counter-clockwise in the algorithm as described by the diagrams. A \"step\" always contains the calculation of , its mixing with the glue value, and, depending on the step-direction, either a symmetric encryption or decryption.It might seem counter-intuitive that the message  is not being passed as a parameter to the sign() and verify() functions. This has been chosen in favor of code simplicity. Simply remember that the message's hash is used as the symmetric encryption key and therefore implicitly included.\nThis code has not been audited and should serve as an example only. For the creation of a secure implementation, please study the original paper."}},"/posts/2023/8/22/applied-elliptic-curve-cryptography":{"title":"Applied Elliptic Curve Cryptography","data":{"":"August 22, 2023 by patrickd\nIf you've spent any time trying to learn about Elliptic Curve Cryptography, you have probably already heard a lot about the theory but not that much about how it's then used in practice. At the beginning of this article, we'll quickly review those theoretical basics before moving on to some example applications.\nScreenshots of Andrea Corbellini's ECC demonstration tool","review#Review":"The pretty \"elliptical curve\" plots, as seen in the top row, are created over an infinite field (the real numbers, ) which is useful for getting an intuition of what's going on. But the discrete log problem, required for ECC, is only present over a finite field (integers smaller than a selected prime number, ), where this graphical representation collapses due to the points' coordinates being determined within . The exact formulas by which these coordinates are determined differ between curves and are mostly irrelevant unless you're attempting to roll your own crypto. The points appear to be randomly distributed but can actually become elements of a cyclical group.\nFor such a group to be complete, an operation is required under which several properties hold true. In the case of ECC, this operation is usually referred to as \"point addition\", but it doesn't actually have anything to do with any sort of mathematical addition, rather the operation was \"made up\" to fit the rules.\nA point  on the curve \"added\" to itself () is defined as an operation that draws a tangent on the curve through . Thanks to the elliptic curve's nature, this tangent line will cross the curve at exactly one other point, which is mirrored over the x-axis and then declared to be the \"result of the addition\", point .\nAdding two different points together () is defined as drawing a line that goes through both points, resulting in a third point, whose mirrored point is the result . As implied by this process of mirroring, there'll always be two possible Y values for each X value as these curves are horizontally symmetric.\nNote that it's not possible to multiply or divide points unless scalar values are used to do so. For example, , can also be represented as . While points are usually represented by upper-case letters, scalars are lower-case, ie.: .\nScalars are normal integers and not part of the group, which means that between scalars any usual mathematical operation is available. But they're still part of a field  where  is the order of the cyclic group (ie. the total number of elements in the group, not the same as the prime  defining the finite field). That means that modular arithmetic is applied where calculations between scalars happen.\nFrom the \"addition\" operation, one can also find a reverse \"subtracting\" operation. Here too it's important to remember that a \"negative point\"  is simply the x-axis-mirrored point  and does not involve any actual mathematical subtraction.\nThe properties holding true for said operation are the following:\nClosure: The result of the operation applied upon two elements that are part of the group, always yields a result that is an element of the group as well. Meaning that the \"addition\" of any valid points will result in another valid point on the curve.\nCommutativity: \nAssociativity: \nExistence of an \"Identity\" element  within the group for which the following is true: . No such Identity Point naturally occurs on an elliptic curve, therefore this too is \"made up\" as a \"point at infinity\".\nInvertibility: For every element such as  there's an inverse  such that .\nWhen the order  (ie. the number of existing points) of an elliptic curve is prime, almost any non-identity point on the curve can serve as a generator point  (also referred to as \"primitive element\", or \"base point\"). A generator, when \"added\" to itself over and over again, will cycle through all elements of the group ( ). However, not all curves have a prime order. For those that have a composite order, it's common to choose a large prime-order subgroup and a generator that is part of this sub-group is then determined. These generators won't iterate through all points on the curve, only through points in their subgroup. If the subgroup's order is a large prime, it will have a number of elements comparable to the entire curve.The fact that an exponentiation with a modulus () is hard to reverse is referred to as the discrete logarithmic problem (DLP). Similarly, it's easier to determine a point  by multiplying a scalar with a generator () than it is determining the scalar that was used from knowing both  and , which is referred to as the discrete logarithmic problem in ECC (ECDLP). This operation can be efficiently executed forward thanks to methods such as the Double-And-Add algorithm, while the reverse requires a much more brute force approach.Note that most scientific paper will not make use of the ECDLP scalar-point notation. Instead they typically use the conventional discrete logarithm notation where the base is the generator and the field element (scalar) is the exponent:Using ECC instead of relying on the conventional DLP or the prime factorization problem (RSA) comes with benefits: In ECC the numbers used can be much smaller while offering stronger security guarantees. It can also be argued that ECC has much fewer footguns.","ecdh#ECDH":"The Diffie‚ÄìHellman key exchange scheme can be implemented using ECC. It's a simple and elegant example to gain a bit of intuition in how ECDLP can be made use of in practice.\nStep\tAlice\tBob\tEve\t#0\tShares a chosen curve and its parameters\tReceive's Alice's chosen parameters for the exchange\tHas intercepted the same chosen starting parameters\t#1\tGenerates scalar , a random secret integer\tGenerates scalar , a random secret integer\t-\t#2\tShares the public point  by calculating \tCalculates and shares \tIntercepted points \t#3\tCalculates \tCalculates \t-\t#4\tUses  to derive a symmetric key and use it for encryption\tIs able to use  to derive the same symmetric key and use it for decryption\tUnable to decrypt messages without knowing the shared point .","arriving-at-the-shared-point-c#Arriving at the shared Point C":"In this scheme, both Alice and Bob are able to arrive at the same shared key because scalars can be multiplied with each other:","without-leaking-the-shared-key-to-eavesdroppers#Without leaking the shared key to eavesdroppers":"Eve won't be able to determine  as there's no multiplication operation defined between points:And the point you'd arrive at using addition would be a completely different one:Therefore the shared key  can not be derived from observing exchanged public key points.","without-disclosing-private-scalars-to-the-counter-party#Without disclosing private scalars to the counter-party":"It's worth noting that not even Bob will be able to determine  despite knowing all other parameters:Solving for  is not possible as Bob won't be able to calculate  as no such \"point-division\" operation is defined.","code-snippet#Code Snippet":"","ecies#ECIES":"The \"Integrated Encryption Scheme\" is rather similar to what you could achieve by combining ECDH with any symmetric encryption scheme such as AES. The difference is that in a ECIES implementation you won't have to bother with combining them correctly, it's already \"integrated\" and can make use of your usual public key pair.\nStep\tAlice\tBob\tEve\t#0\tAlice shares her public key  () with Bob\tKnows \tKnows \t#1\t-\tBob generates a random ephemeral key pair , a secret point  and discards \t-\t#2\t-\tBob derives a symmetric encryption key from , encrypts message  with it\t-\t#3\tAlice gets \tBob sends Alice ephemeral public key  and the message's ciphertext \tKnows \t#4\tAlice computes the same secret point , uses it to derive the symmetric key and decrypt the message\t-\tEve would need either  or  to determine","arriving-at-the-shared-secret-s#Arriving at the shared secret S":"Bob arrives at the secret  by multiplying the ephemeral secret key  with Alice's public key . Alice's public key is simply her secret scalar  multiplied with the generator point . When Alice receives the public ephemeral key , all she has to do is mix in her private scalar  to arrive at the same point","ephemeral-key-freshness#Ephemeral Key Freshness":"When combining ECDH with symmetric encryption schemes such as AES, it's essential to choose an appropriate \"block cipher mode of operation\". This ensures that, despite the same shared key being reused, the resulting ciphertext would always be unique preventing pattern recognition or message replay attacks via a Man-in-the-Middle.In contrast, ECIES addresses these concerns by always generating a new ephemeral key pair for each encryption operation. Each encrypted message is accompanied by a fresh public part of the ephemeral key , allowing the recipient to compute the decryption key uniquely for each message.","schnorr-signatures#Schnorr Signatures":"Various protocols employ ECC for message signing, among which Schnorr's algorithm stands out as perhaps the most elegant and simplistic, both in conceptual understanding and extensibility. Regrettably, its patent hindered a broad adoption and led many open-source projects to opt for the more intricate ECDSA protocol. However, with the patent's recent expiration, this is a great opportunity to transition to the Schnorr paradigm!As before, we assume that protocol participants have agreed upon a curve, its parameters and the generator point. Furthermore we assume that participants have public key-pairs ready and a message  they'd like to sign and verify.","signing#Signing":"Generate a reliably random, one-time-use, key-pair . To prevent leakage of the user's private key, it's essential that  is never leaked or shared, but rather discarded immediately after the signing process.\nCalculate\n.\nWith the message  and the public one-time key  being concatenated and hashed resulting in a scalar value. Little  is the private-key part of the user's personal key-pair . This process requires no elliptic curve operations and is very efficient thanks to that.\nShare  and  (both values make up a complete Schnorr Signature) with a person that wants to verify that the message was truly signed by the owner of .","verification#Verification":"The receiver merely has to verify that, with the known values , the following holds true:But why does that work? To show that, we take the formula used during the signing process and multiply both sides with :Basically, by multiplying both sides with the known parameter  we are able to substitute all unknown parameters ( and ) with known ones ( and ), while keeping the formula itself equivalent to the one used during signing.","forging#Forging":"But if the formula still works with all parameters being public, doesn't that mean that someone would be able to generate a signature using only known public parameters?Assuming  is generally known, and we want to forge a signature for the public key , making it look like they have signed an attacker-chosen message . The attacker would need to determine  and , but the problem here is that the relationship of  is impossible to resolve. With the hash being a deterministically random one-way-street, how do you determine an  that when hashed with the message () yields a scalar that works for . This is a mathematically unresolvable circular dependency. The only feasible method is guesswork ‚Äì and the number of possible guesses is astronomically high, making it impractical.","nonce-reuse#Nonce Reuse":"One of the well-known vulnerabilities of ECC based signatures happens by reusing the same random factor for signing multiple messages. Meaning that, at some point, someone published two signatures  and  with both having the same , which implies that the same random  was reused for signing two different messages  and . This issue is so detrimental because by subtracting the signatures from each other we can actually determine the private key: minus As you can see,  was cancelled out, leaving only the need for publicly known parameters to determine the private key .This is why it's very important to ensure a reliable source of randomness is used for the secret scalar . Another way to ensure this is by using the message itself as the source of entropy to deterministically obtain a unique random  every time (RFC 6979).","nonce-as-data-proof#Nonce as Data proof":"One of Blockchain's real-world applications is \"Proof of Inclusion\", meaning a proof that someone (signer) knew about something (hashed) at a certain point in time. In Bitcoin this was typically done using the OP_RETURN operation which allowed storing random data, but at the same time, wasted costly blockspace. A neat way to optimize this is by exploiting  to, additionally to being a random factor, also proof some data. This can be done without influencing the normal verification process and without increasing the signature size.We'll still use the same formula for signing () with the difference that a new parameter  will be random within  instead of  itself:The  value will be calculated as usual based on the resulting  and someone who merely wants to check the signature's validity can do so normally. But once the signer wants to proof the inclusion of the specified , they publish it plus  (from ). Those interested in validating the proof of inclusion can then additionally verify the following:This equation is the result of multiplying both sides with : This technique is being made use of by various concepts in Bitcoin, such as P2CH (\"Pay to Contract Hash\"), \"Taproot\" and \"Graftroot\" (never question bitcoin naming, it doesn't make sense most of the time). But most interestingly: It shows how vast the possibilities of ECC still are and the potential of playfully finding new schemes.","code-snippet-1#Code Snippet":"","eddsa#EdDSA":"This is a \"Digital Signature Algorithm\" typically used with the Edwards-curve, which has some efficiency and security benefits. You may notice that it's very similar to Schnorr, with the biggest difference being how the random nonce  has been replaced with a deterministic secret factor  derived from the private key  and the message  being signed.","signing-1#Signing":"A EdDSA signature too consists of two components :As before, all inputs of  functions are concatenated with each other and return a scalar value. Above, the message  is signed by the owner of the key-pair .","verification-1#Verification":"Using the publicly known variables  it's now possible to validate the signature by checking if the following equation holds true:That this indeed works, can be proven by substituting  and showing that both sides of the equation should indeed be equal:","forging-1#Forging":"As we did in Schnorr's signature scheme, here too we have a paradoxical relationship that cannot be resolved: . In order to forge a signature one would have to pick an , that when hashed, returns a result that satisfies the outside  value. This cannot be resolved neither in a mathematical or logical manner nor using brute force - at least not in a reasonable manner.","ecdsa#ECDSA":"You've probably heard about the Elliptic Curve Digital Signature Algorithm before, or at least its abbreviation. It's widely adopted, although arguably much less elegant than Schnorr and also just a bit intimidating due to its use of modular arithmetic inverses.","signing-2#Signing":"Like Schnorr, ECDSA signatures require a reliably random one-use key-pair (). The signature is made up of two components  that proof that a message  was signed by a specific private key .The  value is simply the x-coordinate of the random point :The  value is the scalar returned by hashing the message , which is added to the product of 's x-coordinate and the private key . This is then multiplied with the inverse of , which is a reminder that these equations are using modular arithmetic over , with  being the order of the curve (number of elements in the group).","verification-2#Verification":"If the following equation holds true for known public values , the message  was indeed signed by :That this indeed works can be shown by factoring out  and  and then substituting  with the equation used during signing:You may be wondering how a verifier can know 's y-coordinate, as it was never communicated. The fact that only  was communicated through  means there are two possible  to choose from due to the curve's symmetric nature. Protocols determine the correct one in different manners, some simply declare some y-values as \"canonical\" based on an agreed upon rule. Others, like Ethereum (), use an extra byte as \"recovery-id\" to communicate which one it is.","malleability#Malleability":"The use of such a \"recovery-id\" introduces malleability: This id can be flipped, making the verifier expect to find a signature for the mirror point  and as it turns out, it's rather simple to determine the  parameters for this point:Since  is the same for the point , it only leaves the value of  to temper with. Furthermore, for  to invert, we have to invert both sides, and with  being a constant, we're left with having to invert : .As seen, the verification will still work for the additive inverse of , which is also very easy to calculate: With this, it's possible to find a perfectly valid second signature for each original signature observed. This doesn't mean that the signature's signed message can be modified, but it does open a vector of replay attacks to certain systems keeping track of which signatures have already been used.","nonce-reuse-1#Nonce Reuse":"Like Schnorr, ECDSA is vulnerable to Nonce Reuse. Someone who noticed that two signatures with different  make use of the same  is first able to determine :And then, knowing  they're able to determine the private key used for signing:An important reminder to ensure that  must be uniquely chosen for each signature and that even after that it must never leak. Best dispose of it immediately.","code-snippet-2#Code Snippet":""}},"/posts/2024/1/1/race-25-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #25 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-25, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. It was designed by the Secureum Mentor Zach Obront, an Independent Security Researcher and Senior Watson at Sherlock.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nJanuary 1, 2024 by patrickd","code#Code":"All 8 questions in this RACE are based on the below contracts. This is the same contracts you will see for all the 8 questions in this RACE. The question is below the shown contracts.","question-1-of-8#Question 1 of 8":"Which accounts am I able to set the magic number for?\n A. Any EOA that I have the private key for \n B. Any contracts that I own \n C. Some EOAs that I don‚Äôt have the private keys for \n D. None of the above \nCorrect is A, C.A magic number is set via the set() function which requires a signature (uint8 _v, bytes32 _r, bytes32 _s) to be passed in which signs for the specified user address for whom the magic number is being set.With that in mind, you'll be able to sign with EOA accounts that you have the private key for, but you won't be able to sign with contracts since those don't have a private key.There's one exception here: The zero-address (address(0x0)). The ecrecover() function which is used to recover the signer's address from the passed signature returns the zero-address in case of errors. This code does not handle this case, instead, by passing in an invalid signature that will cause ecrecover() to error, it'll allow anyone to set magic numbers for 0x0 - although nobody has the private key for this address.","question-2-of-8#Question 2 of 8":"In what situations could a signature be replayed?\n A. Someone could use the same signature to set the same number and add to the prize  \n B. Someone could use the same signature to change the magic number  \n C. Someone could use the signature from another chain  \n D. None of the above \nCorrect is A, C.The messageHash currently only contains two things: The magic number passed and the contract's own address (address(this)). It doesn't keep track of which signatures have already been used and it's therefore possible to use the same signature multiple times.Since the number is part of the signed message, it's not possible to change the magic number with the same signature, but there's nothing stopping you from adding more value to the prize by replaying it. It may be argued that this is true since its possible to exploit invalid signatures for the zero-address, though this wouldn't actually impact any real users (see solution of Question 1).Additionally to a nonce, the signature is also missing a chainId within the message it's signing. Due to that, it's possible to replay them across chains. That means one would be able to use a signature from one chain, where the legitimate player has set a prize, on another chain where the player hasn't even played yet.","question-3-of-8#Question 3 of 8":"Which EIP(s) should be used to create a more secure message hash?\n A. EIP 1559 \n B. EIP 712 \n C. EIP 4626 \n D. EIP 4337 \nCorrect is B.\nEIP 1559: Changed Ethereum's fee market mechanism, adding a base-fee.\nEIP 712: Defined a procedure on how data should be structured for hashing and signing.\nEIP 4626: Extends ERC-20 to provide a standard for tokenized Vaults.\nEIP 4337: Added Account Abstracting using an alternative Mempool.","question-4-of-8#Question 4 of 8":"Is there a reentrancy risk in this contract?\n A. No, it‚Äôs safe because a low level call is used to transfer ETH \n B. No, it‚Äôs safe because checks-effects-interactions is used \n C. Somewhat, reentrancy is possible but the noStealing modifier eliminates the risk \n D. Yes, there is a reentrancy risk that can be used to steal all funds \nCorrect is D.First we have to understand what this contract is all about: Players can use set() to set a magic number which can be reached by a certain pre-image (ie. another number can be converted into a magic number, commonly through hashing). These users also set a certain ether prize that is given to another player guessing the correct pre-image. Guesses can be submitted via the solve() function, if the caller guesses right, they will receive the ether prize.But the solve() function makes use of the noStealing() modifier, which wants to ensure that at the end of the solve() function's execution, the winner did not take more funds from the contract than the prize set by the user who came up with the magic number. It does so by comparing the contract's before and after balance. The issue with this is that an attacker merely has to make sure that this condition is satisfied at the end of the solve() function's execution. To do so, the attacker can call the function using a contract with a fallback() which will be triggered when the ether prize is sent to it.For example, the attacker could double the prize by having the fallback() function, when first receiving the prize, call solve() once more. Since the prizes[user] had not been updated to 0 yet, they can claim the same prize again. When receiving this second ether value through the fallback(), they can use set() to put it back into the contract under their own name. This will make the noStealing modifier's checks pass and allows the attacker to later obtain this balance that is now double-accounted for within the contract.","question-5-of-8#Question 5 of 8":"In what situations could _safeETHTransfer revert?\n A. msg.sender is an EOA that is not able to accept funds \n B. msg.sender is a contract without a receive() or fallback() payable function \n C. msg.sender is a contract that runs out of gas \n D. msg.sender is a contract that reverts in its receive() function \nCorrect is B, C, D.There's no such thing as an EOA that is unable to receiver ether funds. Contracts, on the other hand, are more nuanced: When they receive ether funds via .call() their code is executed and may revert for various reasons.","question-6-of-8#Question 6 of 8":"What does the _shuffleBits() function do?\n A. Moves all the bits to the left \n B. Moves all the bits to the right \n C. Reverses all the bits one by one \n D. Reverses all the bytes one by one \nCorrect is C.The _shuffleBits() function takes in and returns a uint, or more precisely, a uint256 which stands for the 256 bits that make up a simple unsigned (all positive) natural number. The function iterates over each of these bits (from 0 to 255) by checking preimage & (1 << i) != 0:The number 1 has a single bit at its zero position, and with << this bit is shifted i times to the left. The result of this shifting operation is then processed by an bitwise-AND with the preimage. This means that for each bit at position i it checks whether the bit's value is unequal zero.If it is unequal zero it writes result |= 1 << (255 - i) into result which is a new, empty unsigned integer of the same size.The 1 << (255 -i) operation is again shifting a single byte to the left, but this time it starts at the last bit and as i increases the bit gets closer to the zero position.The |= assignment means that the current result's value is bitwise-OR operated on with the shifted bit and then updated with the resulting value. Therefore it reverses all the bits one by one.","question-7-of-8#Question 7 of 8":"_convertPreimageToNumber can return:\n A. An even number \n B. A positive number \n C. A negative number \n D. None of the above \nCorrect is A, B, C.The _convertPreimageToNumber() function takes in and returns a signed integer int, or more precisely a int256 that can represent both positive and negative natural numbers - but it needs to do so using the same 256 bits as before. That means that the amount of positive numbers that can be represented needs to be reduced in order to make space for the negative ones.The point where this split happens is  represents the lowest number that the int type can represent (type(int256).min == -).That this happens at this number is no coincidence. With the last byte going from 0x7f to 0x80, the highest bit 10000000 will now be set for all further numbers. Meaning, if you want to check whether the number is negative or positive, you simply have to check the most significant bit. This method of representation is called \"two's complement\".Subtracting 1 from  - 1results  represents -1Before _convertPreimageToNumber() passes this signed preimage number to _shuffleBits(uint(preimage)) it casts it to a signed integer. This doesn't change the actual value contained within the variable but its representation. If the preimage passed in was -1 then, once cast to uint, it instead becomes the largest number that uint256 can represent, but in reality was and still is .The value returned by _shuffleBits() is then cast back to a signed integer. Here again, the value doesn't change, rather, it depends whether the resulting value was above or below the where the split between positive and negative numbers happens. Depending on that the int shuffled variable becomes positive or negative.Finallyreturn shuffled < 0 ? -shuffled : shuffled;would make you expect that this function will only ever return positive numbers, but if we were to pass it the smallest possible signed  (-)it will return that very same number, in negative form.The reason for this is that the positive range does not include that number without a sign. The maximum positive number representable  (+)And as you can see, that is one too low to be able to represent that negative number as a positive one. So when solidity removes the negative sign, it just wraps around and gets back into the negative range again.","question-8-of-8#Question 8 of 8":"How many valid preimages are there if magicNumber == 1?\n A. 0 \n B. 1 \n C. 2 \n D. 3 \nCorrect is C.Working backwards in _convertPreimageToNumber(), for it to return a 1return shuffled < 0 ? -shuffled : shuffled;The shuffled value either needs to be -1 or 1. Which is one of either:\nThe _shuffleBits() function would have no impact on the -1's value, therefore we already know that -1 is one valid preimage for the magic number 1.But for _shuffleBits() to return +1, the input would need to be that single bit shifted all the way to the right, which is: this two valid preimages for the magic number 1."}},"/posts/2024/10/1/race-33-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #33 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-33, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. This month's RACE was designed by Secureum Mentor George from Consensys Diligence.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nOctober 1, 2024 by patrickd","code#Code":"All 8 questions are based on the following contract for a custom token implementation.At first, the contract mints the intended token total supply (totalSupplyCap) to the 0-address. Then, the owner transfers those tokens from the 0-address to the signatory addresses in batches as they see fit. Those addresses create signatures that allow users to claim some amounts of tokens. In order to claim those tokens, users need to deposit some ETH into the contract, which will be subtracted from them based on the amount of tokens they are claiming with the signature from signatories. Finally, users will be able to withdraw their deposited ETH after requesting withdrawals with a locking period.For simplicity, assume that the contract was deployed with totalSupplyCap=10**6, and 5 users have already deposited ETH in anticipation of their claims, to a total of 10 ETH.The ERC20 implementation used (ERC20_modified.sol) is a modified version of OpenZeppelin's implementation where 0-address checks have been removed which didn't allow transfers to/from the 0-address without minting/burning respectively. Assume the rest of the functionality, like appropriate modification of _totalSupply tracking, has been adjusted as well.","question-1-of-8#Question 1 of 8":"The signature used to claim tokens by user:\n A. Is secure because it uses ecrecover directly by itself. \n B. Is secure from cross-chain attacks because it uses the wrapper \"\\x19Ethereum Signed Message:\\n32\". \n C. Can be re-used. \n D. Is using an outdated hashing mechanism keccak256 that is not compatible with ecrecover after the Paris hardfork. \nCorrect is C.A: Arguably the opposite is true, there are some intricacies that should be considered when making use of the ecrecover() precompile. It's recommended to make use of a library such as OpenZeppelin's ECDSA contract which takes care of them. Also consider the SignatureVerification contract of Permit2, which offers a more condensed handling.B: This wrapper alone offers no protection against cross-chain attacks between Ethereum-based chains. Furthermore, the contract does not make use of EIP712 for structuring the signed data, allowing replay attacks due to a lack of chain ID, target address, etc, especially with such a simple (recipient, amount) pair being signed.C: Not only can the signature be re-played in other places, the same signature can also be re-used multiple times within this contract as it lacks any checks of a nonce, or any other type of logic that would mark a signature as having been redeemed.D: The keccak256 function is the generally recommended hashing mechanism within Ethereum, and there's no compatibility issue using it with ecrecover.","question-2-of-8#Question 2 of 8":"The signer address recovery process:\n A. Will always retrieve the correct signer of the message. \n B. Can retrieve a 0-address as the recovered signer address. \n C. Can retrieve a random/uncontrolled address as the recovered signer address. \n D. Is missing a parameter z from the signature to retrieve the signer address. \nCorrect is B, C.B: If the signature is mathematically incorrect preventing any public key, and from that, address to be recovered, the ecrecover() function does not revert. Instead it simply returns the 0-address. Instead of handling that as an error it continues on as if the 0-address itself is the legitimate signer. This 0-address contains all of the initial supply and is also part of the privileged addresses.A/C: It is however possible to construct a signature that can be processed (the math works) but returns arbitrary and unpredictable addresses as the supposed signer. Looking at the Applied Elliptic Curve Cryptography article on ECDSA you can see that nothing is stopping your from choosing random private keys for creating a signature.D: There's no z parameter for ecrecover.","question-3-of-8#Question 3 of 8":"The users that claim the tokens via userClaim:\n A. Are always part of the signature. \n B. Can have their signature transactions front-run. \n C. Need to deposit before claiming 1000 tokens. \n D. Can control how many tokens they claim in one transaction. \nCorrect is B.A: The receiver of the tokens (msg.sender) may be different from the address receiver specified which is part of the signature. Meaning that the user who is making the claim is not necessarily part of the signature.B: An adversary could indeed monitor the transaction memory pool for calls to userClaim and front-run these using that same signature.C: Claiming 1000 tokens does not actually require a deposit as it will be rounded down to 0. This is due to truncation from integer division at requiredDeposit = amount / tokensPerEthPrice with tokensPerEthPrice = 10**4.D: The claim amount cannot be directly controlled as it is part of the signature.","question-4-of-8#Question 4 of 8":"The getUserMintAmount() function:\n A. Takes in ETH amount and recipient address as arguments, returns PMT token amount to be minted. \n B. Takes in PMT token amount and recipient address as arguments, returns ETH amount to be subtracted from the user balance. \n C. Takes in PMT token amount and recipient address as arguments, returns PMT token amount to be minted. \n D. Is correctly performing the required deposit calculation that isn‚Äôt vulnerable to any rounding issues as it uses a pre-calculated price provided via tokensPerEthPrice. \nCorrect is C.A: The required amount of ETH is computed within this function, and the deposited amount of ETH is read from storage. It does not take in the ETH amount as argument.B: It does not return an ETH amount to be subtracted, it returns the amount of PMT token to be transferred, if the deposited amount of ETH is sufficient.C: Yes.D: It is vulnerable to rounding errors when small amounts (amount < tokensPerEthPrice) are passed.","question-5-of-8#Question 5 of 8":"In the deposit and withdrawal process:\n A. Users can only withdraw as much as they deposit. \n B. The withdrawal process has a re-entrancy issue that can exploit the contract as the transfer occurs before the state update. \n C. Users need to perform a deposit, then request withdrawal, and only then can receive any funds from the contract. \n D. None of the above. \nCorrect is D.A/C: Because the unlockedAt timestamp is zero-initialized for users that have never made a deposit, they can simply call withdraw() directly without previously calling prepareWithdraw(), allowing them to obtain arbitrary amounts because it will satisfy unlockedAt[msg.sender] < block.timestamp. This is works because the withdrawals amount is re-set to 0 instead of subtracting the amount being withdrawn from it, which would cause a revert from integer underflow protection if a user attempted to withdraw more than deposited.B: No, the external call (via transfer()) is made at the end of the function when state updates have already happened (Checks-Effects-Interactions pattern followed).","question-6-of-8#Question 6 of 8":"The events are:\n A. Correctly emitted for all Deposit events. \n B. Correctly emitted for all Withdrawal events. \n C. Correctly emitted for all BalanceUpdated events. \n D. Correctly emitted for all Minted events. \nCorrect is A.","question-7-of-8#Question 7 of 8":"Assuming there are no vulnerabilities, the owner:\n A. Cannot mint more tokens than is defined by variable totalSupplyCap. \n B. Receives all spent deposits to their owner address directly. \n C. Can change the price of the token. \n D. Can retrieve any user funds from the contract. \nCorrect is C.A: Owner can mint past _totalSupplyCap by executing mintToSigner to 0-address again.B: The owner address is only used for authentication purposes, never receives deposits directly.C: Yes, via the setTokenPrice() function.D: With the assumption that the vulnerability within withdraw() has been fixed, no.","question-8-of-8#Question 8 of 8":"The contract:\n A. Is ERC-20 compliant. \n B. Is upgradeable. \n C. No longer complies with the ERC-20 standard due to changed function names like mintToSigner and userClaim instead of mint.  \n D. Is not EIP-712 compliant. \nCorrect is A, D.A/C: Despite using a modified OpenZeppelin ERC-20 implementation, the contract is still ERC-20 compliant.B: Does not make use of any patterns allowing to upgrade the contract's logic.D: See solution to Question 1."}},"/posts/2022/6/28/damn-vulnerable-defi-v2-11-backdoor":{"title":"Damn Vulnerable DeFi V2 - #11 Backdoor","data":{"":"June 28, 2022 by patrickd\nThis is part 8 of the write-up series on Damn Vulnerable DeFi V2. Please consider attempting to solve it on your own first since it's a lot less fun after being spoiled!\nChallenge #11 - BackdoorTo incentivize the creation of more secure wallets in their team, someone has deployed a registry of Gnosis Safe wallets. When someone in the team deploys and registers a wallet, they will earn 10 DVT tokens.To make sure everything is safe and sound, the registry tightly integrates with the legitimate Gnosis Safe Proxy Factory, and has some additional safety checks.Currently there are four people registered as beneficiaries: Alice, Bob, Charlie and David. The registry has 40 DVT tokens in balance to be distributed among them.Your goal is to take all funds from the registry. In a single transaction.","code-review#Code Review":"From the challenge description alone I can't come up with what the issue might be yet. So let's take it step by step and start by reviewing the scenario setup and the success conditions found in the tests: backdoor.challenge.js.\nRight away, it deploys 3 contracts: GnosisSafe, GnosisSafeProxyFactory, DamnValuableToken. The GnosisSafe contract instance is stored in a variable called masterCopy while GnosisSafeProxyFactory's instance is put into walletFactory. If you are familiar with the proxy-factory pattern this should make sense: GnosisSafe safe is the Logic Contract that contains the actual business logic of Gnosis Wallets. The GnosisSafeProxyFactory is a contract that can produce cheap clones of GnosisSafe. Cheap because there's no need to redeploy the entire business logic, just deploying a Proxy contract that delegate-calls to GnosisSafe is sufficient since it can execute the master copy's code within the context of its own state.\nNext up, it deploys the WalletRegistry contract, the main target of this challenge, and passes to it the addresses of the previously deployed contracts, plus the four mentioned beneficiaries. After these users are registered on the contract as such, the registry is given the 40 DVT of rewards that belong to each of them.That's it for the setup, it's simpler than expected and, while we might need some basic understanding of Gnosis, we'll probably just have to take a very good look at the registry contract to find the issue.\nAs expected the success conditions require us to move all 40 DamnValuableTokens into the attacker account, but what's more interesting is that they require all of the 4 users to no longer be registered as beneficiaries but have a registered Wallet - that seems like a pretty big hint! During the setup no Wallets were created for each user to be able to claim these rewards. So most likely we'll be able to claim them for ourselves with our own wallets despite not being an eligible user.Let's look at the only contract that is specific to this challenge first, WalletRegistry.sol, and see if it's possible to find the issue without knowing much about Gnosis itself.\nI guess that is called foreshadowing?Most of the code can be skipped, things start to get interesting here:\nThe first thing that comes to mind when reading \"callback function\" is: Does it make sure that it can really only be invoked by the expected caller, in this case, the GnosisSafeProxyFactory?\nYes, it does, guess that would've been too easy. And it's also checking that the used master copy (singleton) was the real GnosisSafe logic contract - so we can't make use of a manipulated wallet to return fake data.\nI assume this is supposed to ensure that, after the Proxy was deployed, the wallet was initialized using the setup function and no other. Not sure whether that'll end up being relevant yet.Anyway, so where does it check whether the owner of the wallet, the person who initiated the wallet creation triggering this callback in the first place, is actually a beneficiary?\nThere it is, it fetches the first owner of the wallet and checks whether they are a registered beneficiary. Note that MAX_THRESHOLD and MAX_OWNERS are both 1, so we can't simply bypass it by being a secondary owner either.\nThe functions wraps up by removing the beneficiary, registering their wallet address, and transferring the reward tokens to it.It seems that there must be some way to have control over a Gnosis wallet as a creator without currently being one of its owners?Let's take a look at GnosisSafeProxyFactory now since that is the only contract being able to call into the registry's callback function.First, I checked whether there's any other function than createProxyWithCallback that would allow making calls to the registry, but I didn't see any.\nNothing unexpected. The proxy contract is deployed and the callback is called afterwards.Maybe there's something interesting happening with the initializer?\nNow this assembly/yul code might look a little bit intimidating but it's rather simple: It's CALLing the just created proxy contract with the calldata passed in the initializer variable.Note that initializer is a bytes array stored in memory, so when the variable is accessed within assembly its actual value is a number, a memory pointer, the address of where the bytes array is stored. Another thing to understand is that the first 32 bytes (or 0x20 in hexadecimal) of a bytes array in Solidity, still isn't the actual value that was passed as initializer, it's basically an unsigned integer of the initializer's length.Therefore mload(initializer) is loading that length. And add(initializer, 0x20) calculates the address where the actual initializer value starts in memory. That means that a call() is being made to the proxy, passing all still available gas(), specifying the full initializer variable as the calldata and dismissing any return data. Finally, eq(call(...), 0) ensures that it revert()s in case the call didn't succeed.This all seems pretty normal. With that, I assume we can pass something to GnosisSafe::setup() via the initializer that allows us to do some kind of unexpected shenanigans.The GnosisSafe's setup function certainly has some parameters that sound quite interesting:\nAn \"optional delegate call\"? A \"handler for fallback calls to this contract\"? Sounds promising.\nIt seems the \"optional delegate call\" using the to and data parameters is executed immediately. Since the registry callback is called after setup has finished this doesn't help us much. At this point, we wouldn't have received those DVT Tokens that we need yet and we can't add ourselves as owner either since then the checks in the callback would then fail. [EDIT: Later on Discord silent_mastodon#1304 pointed out that this can actually be exploited - by using token approvals!]Now it would be really good if the fallback-handler is delegate-called into as well. In that case, we could set our own exploitation contract as fallback handler and, once the callback sent the tokens to the wallet, we could trigger the fallback and execute arbitrary code within the wallet's context - such as moving all tokens to the attacker's EOA.The fallback handling logic can be found in /base/FallbackManager.sol:\nIt doesn't take much assembly knowledge to know that this is a call() and not a delegatecall() as I was hoping for.But wait! This still allows us to make arbitrary calls to a single address that we can freely choose. The GnosisSafe contract does not have a transfer function. So if we'd set the token address as fallback-handler and call transfer() on the wallet, the wallet should call transfer on the token. Since the token contract is being called by the wallet, the msg.sender will be the wallet's address and therefore we can freely transfer tokens that belong to the wallet.","exploit#Exploit":"The challenge description told us \"to take all funds from the registry. In a single transaction.\" ‚Äì therefore we'll need a smartcontract executing the entire exploitation:\nThen we adjust the backdoor.challenge.js to make a single transaction, the deployment of the exploit contract:\nAnd finally...\nWe got it!Did this take me way too long to solve? Well, I'm certain that the solution would've been a lot more obvious if I'd had any prior knowledge of the Gnosis contracts. But I just now obtained this knowledge through poking around, the best way to learn new things (in my opinion)!I think the challenge was a lot of fun. It felt very \"real\" and, although I can't remember one from the top of my head, I imagine something like this must have happened before."}},"/posts/2022/6/27/ethernaut-26-doubleentrypoint":{"title":"Ethernaut: #26 DoubleEntryPoint","data":{"":"June 27, 2022 by patrickd\nSome time ago Ethernaut got the new DoubleEntryPoint level based on a novel compound vulnerability. I've avoided reading anything about it until I finally have to time solve this challenge in an unbiased manner, and today is the day!\nSince some have asked: There's no write-up series of Ethernaut on this Blog, I didn't see the point since there were already so many existing ones on the Internet when I solved it\nThis level features a CryptoVault with special functionality, the sweepToken function. This is a common function to retrieve tokens stuck in a contract. The CryptoVault operates with an underlying token that can't be swept, being it an important core's logic component of the CryptoVault, any other token can be swept.The underlying token is an instance of the DET token implemented in DoubleEntryPoint contract definition and the CryptoVault holds 100 units of it. Additionally the CryptoVault also holds 100 of LegacyToken LGT.In this level you should figure out where the bug is in CryptoVault and protect it from being drained out of tokens.The contract features a Forta contract where any user can register its own detection bot contract. Forta is a decentralized, community-based monitoring network to detect threats and anomalies on DeFi, NFT, governance, bridges and other Web3 systems as quickly as possible. Your job is to implement a detection bot and register it in the Forta contract. The bot's implementation will need to raise correct alerts to prevent potential attacks or bug exploits.\nRight away I was a little confused, so we're not supposed to exploit this to pass the level but write a \"detection bot\" with Forta? Huh, maybe it'll make sense once we understand the bug... From the description alone it sounds like there might be some unintended way to \"sweep\" the underlying token despite the Vault's attempts to prevent that.","code-review#Code Review":"Looking at the source code, the first thing popping out was the definition of a DelegateERC20 interface. Maybe the challenge has something to do with the Vault delegate-calling into a Token whose source code we can control? But then why is there an \"original Sender\" parameter passed? That wouldn't be necessary for a delegate-call since the msg.sender would be preserved...\nI've only heard high level explanations about what Forta is and does so far and had assumed, like it sounds in the description too, that it was an off-chain monitoring network - so seeing detection bots apparently being executed on-chain here is rather confusing to me. Maybe it's just for demonstrative purposes in this challenge? Let's skip over it for now.\nThen we find the CryptoVault contract with that sweepToken function this challenge seems to be centered around:\nIt's clear that this will simply transfer out the full balance of any token specified as long as it's not the underlying token. It should also be said that both the underlying and sweptTokensRecipient appear to be unchangeable once set during construction/initialization of the Vault. But this also means that even if we're able to \"drain\" the Vault of its underlying token, it would just go into a wallet of the Vault creators and not into ours. So, assuming they're not gonna run away with it, that wouldn't mean a loss of funds but at least a disruption of Vault services. Now it makes sense why the goal is to write a monitoring bot here, it's good to be notified when something like this happens!But I don't see any issues with this code by itself, so most likely we'll be able to find something weird in the \"Legacy Token\" contract. And indeed:\nThe \"Legacy Token\" appears to be just that, it's a normal ERC20 Token at the beginning and then over the course of its lifetime the owner may decide to have transfers point to a new Token (presumably the underlying DET Token in this case). To do so it's not using anything fancy like a delegate-call as I had expected at first. It's simply calling a privileged delegateTransfer() function on the new Token to freely transfer funds (which should make sure that only the Legacy Token can call this).As expected, the Vault's underlying \"DoubleEntryPointToken\" makes sure that delegateTransfer() can only be called by the legacyToken by using a modifier. There's also a fortaNotify middleware that sets the whole detection bot thing in motion and can revert the call if it detects evil function calls.","writing-the-detection-bot#Writing the 'Detection Bot'":"So in summary: The vulnerability is that it's possible to bypass the require(token != underlying, \"Can't transfer underlying token\"); check by calling the sweepToken() function with the Legacy Token. The legacy token will tell the underlying token to sweep the entire Vault's balance to the sweptTokensRecipient, likely causing a disturbance for Vault users.Now we need to write a \"DetectionBot\" contract that reacts to funds being moved out of the Vault. This contract should implement a handleTransaction() function which gets passed the Ethernaut player's address and the raw calldata of the delegateTransfer() function call.\nUsing that calldata we can determine if the origSender is the address of the CryptoVault contract and if so we can call raiseAlert() on Forta with the user that was passed to us to make sure the delegateTransfer() function call will revert:\nDon't forget that calldata from msg.data is prefixed by the 4-byte function signature! Luckily byte arrays located in calldata can be easily sliced in Solidity with brackets: [start:length]. In this case we'll skip the first 4 bytes and omit the length, this will default to return the full length. After removing the signature we can use abi.decode() on the leftover ABI encoded part of the byte array.After deploying this, it has to be registered with the Forta contract by calling setDetectionBot() from the player address' wallet. Then we click the big \"Submit Instance\" Button and..\nCongratulations!This is the first experience you have with a Forta bot.Forta comprises a decentralized network of independent node operators who scan all transactions and block-by-block state changes for outlier transactions and threats. When an issue is detected, node operators send alerts to subscribers of potential risks, which enables them to take action.The presented example is just for educational purpose since Forta bot is not modeled into smart contracts. In Forta, a bot is a code script to detect specific conditions or events, but when an alert is emitted it does not trigger automatic actions - at least not yet. In this level, the bot's alert effectively trigger a revert in the transaction, deviating from the intended Forta's bot design.Detection bots heavily depends on contract's final implementations and some might be upgradeable and break bot's integrations, but to mitigate that you can even create a specific bot to look for contract upgrades and react to it. Learn how to do it here.You have also passed through a recent security issue that has been uncovered during OpenZeppelin's latest collaboration with Compound protocol.Having tokens that present a double entry point is a non-trivial pattern that might affect many protocols. This is because it is commonly assumed to have one contract per token. But it was not the case this time :) You can read the entire details of what happened here.\nThankfully this congratulation message clears up my confusion about Forta Bots - they do actually run off-chain and are apparently written in TypeScript, JavaScript or Python. So then, this wasn't a real Forta Bot experience though? And the bug itself wasn't well hidden at all? Wait, was this whole thing just part of Fortas marketing campaign?!Oh well."}},"/posts/2024/11/10/race-34-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #34 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-34, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. This month's RACE was designed by Secureum Mentor and Independent Security Researcher MiloTruck.\nParticipants of this quiz had a single attempt to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nNovember 10, 2024 by patrickd","code#Code":"All 8 questions are based on the following contract.","question-1-of-8#Question 1 of 8":"The protocol facilitates cross-chain messaging by:\n A. Transmitting messages sent from a source chain to the receiver on a destination chain through relayMessage().\n B. Allowing messages that revert during execution in relayMessage() to be re-executed in replayMessage().\n C. Allowing anyone to execute failed messages in replayMessage().\n D. Allowing the protocol to execute failed messages on anyone‚Äôs behalf.\nCorrect is A, B, D.This is a giveaway question meant to test the participant's understanding of the contract. From looking at the code, A, B and D are clearly true. C is incorrect according to the first require statement within replayMessage().","question-2-of-8#Question 2 of 8":"In relayMessage(), the gasleft() check multiplies message.gas by 64 / 63 to:\n A. Avoid rounding errors due to division in Solidity rounding down.\n B. Account for the reduction in gas passed to a callee due to EIP-150.\n C. Reserve sufficient gas for the function to complete execution after safeCall().\n D. Provide extra gas in case the message sender specifies too little gas.\nCorrect is B.The EIP-150 standard indeed defines the \"send all but one 64th\" gas rule: It dictates that only 63/64 of the currently remaining gas is passed to a callee.The question is whether the check truly accounts for the reduction in gas passed to the callee:\nLet's say we specify  as gas amount sent during the CALL. Then the actual gas received by the callee is . But we want this received amount to be exactly message.gas, let's call it , then . By solving for  we will obtain the gas amount we'd have to specify for  to arrive at the callee: . This matches with the require() statement.If you're wondering whether checks like this actually happen in practice, you can search for the require statement on codeslaw and find many projects where it's used.This check indeed also reserves sufficient gas for the function to complete after safeCall(), but it does so using the GAS_BUFFER. This is not related to the multiplication by 64 / 63 as the question suggests.(Answers A and D are fillers)","question-3-of-8#Question 3 of 8":"In relayMessage(), the external call to message.to may receive less gas than message.gas because:\n A. Calling safeCall() performs a delegate call, which reduces the gas remaining by 1/64.\n B. Calling safeCall() performs an internal call, which consumes gas for a few opcodes.\n C. Copying message from calldata to memory when calling safeCall() consumes gas.\n D. None of the above, the gasleft() check ensures sufficient gas is always passed to the callee.\nCorrect is A, C.There's an important difference in how external functions of Solidity libraries are handled compared to internal functions: Internal functions are \"inlined\" into the contract calling it, in other words, the functions bytecode is added to the calling contract and the call is a JUMP. On the other hand, external functions of libraries are deployed within a separate contract. When a contract makes a call to such an external library function, it does so using the DELEGATECALL opcode.\nTherefore safeCall() indeed performs a delegate call while unsafeCall() performs an internal call.\nAnd EIP-150 applies to DELEGATECALLs as well.\nAdditionally, calling safeCall() requires copying message (declared with data location calldata in CrossChainMessenger's functions) from calldata to memory before the call can be executed with the message data as parameter.\nFor large message.data this copy operation could indeed consume more gas than anticipated by GAS_BUFFER.","question-4-of-8#Question 4 of 8":"A failed message may never be executed if:\n A. Trying to execute the message in replayMessage() fails once.\n B. A sender sends multiple messages with the exact same Message struct without checking if execution in relayMessage() was successful.\n C. The message was sent with empty message.data.\n D. None of the above, a sender can always call replayMessage() to retry a failed message.\nCorrect is B.\nThere is no way to uniquely identify two messages with the exact same sender, to, gas and data. If multiple messages with the same Message struct are sent, they will all have the same messageHash when being stored in the failedMessages mapping. Therefore, replayMessage() can only be called once, even though multiple messages failed.\nA and C are filler answers: A message is only removed from the failedMessages mapping if the replay succeeded. The message.data being empty has no impact on the ability to replay messages.","question-5-of-8#Question 5 of 8":"If replayMessage() could only be called by message.sender (i.e. no msg.sender == message.to condition check in require()), it would be problematic because:\n A. There is no way to verify if the caller of sendMessage() is the address that calls replayMessage().\n B. An EOA that calls sendMessage() on a source chain cannot call replayMessage() on the destination chain as EOAs are chain-specific.\n C. Messages sent from contracts that directly call sendMessage() may not be replayable if execution fails.\n D. None of the above, there are no issues with the current implementation.\nCorrect is C.Unless a contract's deployment was specifically prepared for this, it's unlikely for a contract to have the same address across different chains. In such cases, messages sent from contracts that directly call sendMessage() may not be replayable if execution fails when requiring msg.sender == message.sender alone.With the additional msg.sender == message.to condition the receiver is able to replay the message in such cases.A and B are filler answers: You can obviously keep track of the msg.sender values, and EOAs are not chain-specific and can be used across chains.","question-6-of-8#Question 6 of 8":"Is replayMessage() susceptible to reentrancy?\n A. No, the gasleft() check prevents any state changes from occurring in a reentrant call.\n B. No, the failedMessages[messageHash] check prevents any reentrancy attacks.\n C. Yes, reentrancy can be used to block the execution of future messages.\n D. Yes, reentrancy can be used to execute a single failed message multiple times.\nCorrect is D.\nNotice how failedMessages[messageHash] is reset to false only after safeCall(). Executing a failed message multiple times can be achieved by re-entering replayMessage() in safeCall().\nA possible scenario where this could be exploited would be if the protocol receiving the message makes an unsafe external call to an untrusted contract. Such a malicious contract could then re-enter replayMessage() before safeCall() has returned, causing the protocol to be called once again with the same message. A protocol assuming that each message may only arrive once could benefit the attacker by eg. rewarding them twice.C isn't possible and A, B are filler answers that are unrelated to reentrancy.","question-7-of-8#Question 7 of 8":"When forceReplayMessages() is called, it should not revert. However, a receiver contract (i.e. message.to) can force the function to revert by:\n A. Reverting in the function called by unsafeCall().\n B. Returning false in the function called by unsafeCall().\n C. Consuming all the remaining gas in the function called by unsafeCall() without reverting.\n D. Performing a return bomb attack (i.e. return a huge chunk of data, causing forceReplayMessage() to consume more gas than the block gas limit).\nCorrect is A.While safeCall() returns a boolean depending on whether the CALL was successful, the unsafeCall() function will \"bubble up\" the callee's revert causing forceReplayMessages() to revert. If the callee returns false instead of reverting it is simply treated as returned data.\nAssuming that forceReplayMessage() is called with sufficient gas to execute all messages, the gas limit on each message prevents any execution from consuming all remaining gas in forceReplayMessages().\nD is a trick option. Although unsafeCall() reads an unbounded amount of data from the called function, only a maximum of 100,000 gas is passed to the called function. Due to the memory expansion cost, the function will not be able to create a huge chunk of data in memory to be returned, hence a return bomb attack is not possible.","question-8-of-8#Question 8 of 8":"When forceReplayMessages() is called with multiple messages, it will:\n A. Execute all messages in the messages array.\n B. Always revert when trying to execute the first message.\n C. Execute all messages in the messages array and revert after the last call.\n D. Only execute the first message in the messages array.\nCorrect is D.At a first glance, it may appear that it is correctly looping through all messages, but the problem is that unsafeCall() is not making use of a regular (Solidity) return statement, but rather executes the RETURN opcode which will exit the currently executed contract and not simply return to the calling function. Remember that, unsafeCall() being\nan internal library function, it is not externally-called but instead inlined into the CrossChainMessenger contract. Therefore, when forceReplayMessages() calls unsafeCall() it will JUMP to the function's bytecode which will then terminate execution with RETURN instead of jumping back to forceReplayMessages()'s bytecode.\nA PoC demonstrating this can be found here: https://gist.github.com/MiloTruck/837ecb49fe18901d70bf03241548768b\n(Answers B and C are fillers)"}},"/posts/2024/1/9/cryptocurrency-privacy-technologies-zerocoin":{"title":"Cryptocurrency Privacy Technologies: Zerocoin","data":{"":"January 9, 2023 by patrickd\nDespite being regularly referred to as \"anonymous Internet money\", the ledgers of the most widely adopted cryptocurrencies are completely public. Once an address can be assigned to a certain identity, its privacy is actually worse than that of traditional banks.This article explores the Zerocoin Protocol, the first anonymous cryptocurrency proposal supporting large anonymity sets. Initially suggested as an extension to Bitcoin, it was implemented in various alt-coins over the years. You most likely heard about it as Zcoin (XZC).","the-concept#The Concept":"In 2013 the Zerocoin whitepaper suggested extending the Bitcoin Protocol by introducing a distributed e-cash scheme. Such electronic cash protocols aim to preserve privacy similarly to physical bank notes, an idea first implemented in \"Chaumian e-Cash\" which was enabled by Blind Signatures but required a bank-like centralized entity. Zerocoin instead, is a distributed e-cash system that intended to use Bitcoin's blockchain as a public \"bulletin board\", maintaining a list of valid coins for which a coin's membership is proven in zero-knowledge.This proposal would have effectively extended Bitcoin with a native laundry functionality. Practically, users would have been able to make use of new opcodes added to Bitcoin's scripting language in order to lock funds into the mix and redeem them at a later time without any clear connection between deposit and redemption.","scheme#Scheme":"As you may already know from the previous Confidential Transactions article, Bitcoins are not actually transacted \"from one account to another\". Rather than that, there exist Unspent Transaction Outputs (UTXOs) that each have a \"Locking Script\" (ScriptPubKey) associated with them. This script dictates the condition under which a UTXO can be spent. Typically, this condition is that the transaction's signer matches with the address specified in the Locking Script (\"Pay-to-Public-Key-Hash\"). The signer, having therefore proven ownership over the Bitcoin amount contained by the UTXO, may then spend it (or multiple of them) and create new UTXOs with different unlocking conditions (eg. such that only the new owner of the coins may spend them).Zerocoin introduces a new such condition: A user may choose to specify a locking script in their UTXO which \"mints\" a Zerocoin by publishing a commitment to the coin's unique identifier. This commitment is added to an accumulator containing all legitimately minted Zerocoin commitments. The BTC value within the user's UTXO can be redeemed by anyone who too has minted a Zerocoin of the same denomination. Sometime later, the user may decide to reveal their coin's unique identifier to \"spend\" the Zerocoin in exchange for the locked value it represents. The crux is, that the user is able to prove that the identifier's commitment is within the accumulator without revealing the commitment itself using a zero-knowledge proof. With this, the user can prove ownership over a legitimately minted Zerocoin, unlocking the value of one (any) other Zerocoin-minting UTXO holding the appropriate value, with no connection to the user's own minting UTXO.\nTo show how the protocol works, let's imagine Alice is owner over an UTXO holding a value of 2.4 BTC. She'd like to mint two Zerocoins each representing the denomination of 1 BTC. To do so, she locally generates two unique serial numbers  and  which she will keep secret until she decides to spend the coins. She signs a transaction containing 3 output UTXOs: One sending the change of 0.4 BTC back to herself and the other two each locking one BTC into the e-cash system. The Locking Scripts of these two Zerocoin-minting UTXOs contain commitments  and  which each commit to the yet-to-be-revealed serial numbers. Bitcoin's blockchain now acts as a public bulletin board containing a set of commitments  each representing a minted Zerocoin of 1 BTC value. Basically, the protocol is acting as an escrow pool and it's possible to mint different kinds of Zerocoins for various denominations by maintaining multiple sets.After waiting for a while for other users to participate in the e-cash system, Alice may decide to redeem a Zerocoin in exchange for any other UTXO locking 1 BTC for the Zerocoin protocol (): Alice generates a proof  that shows she knows  for a commitment within the set of all unspent commitments , without revealing which of the commitments the associated  is. Alice signs a Transaction where she chooses any Zerocoin-minting UTXO as input, \"proves her ownership\" over it using , and sends the unlocked BTC to a fresh address that has no known association with her. The protocol will keep record of all revealed serial numbers  in order to prevent double-spending. Alice was able to hide the origin of her funds within the anonymity set of all other Zerocoin holders.I've omitted mentioning transaction fees for simplicity, but they'd not require any changes with the introduction of Zerocoin to Bitcoin. It might also be noteworthy that the anonymity set resets once all commitments in the accumulator have been spent, although this seems an unlikely scenario assuming that the system would find continuous use.\n\"Burning Zerocoins for fun and profit\" reveals a fundamental flaw with using plaintext identifiers: An attacker observing transactions may notice a user's intention for spending a legitimate Zerocoin with serial number . The attacker may quickly mint and redeem a Zerocoin with that very same serial number  and, if they succeeded to do this before the user's spend transaction was included, the user would now be rejected since the specified serial number has already been marked as \"spent\". The user's Zerocoin is then effectively unspendable and the user will not be able to redeem it for its value.The paper suggests using a public key as serial number instead and adjusting the protocol to have the spender prove they know the appropriate private key by having them sign the transaction with it.","accumulators#Accumulators":"Proving that an element is part of a list without revealing the element, is a classical membership problem. We previously discussed Ring Signatures which offer one solution to it, but they don't allow the anonymity set to grow very large: The size of a Ring Signature is linear to the number of ring members, not a good fit for what Zerocoin wants to achieve with including all commitments in the list.Instead, Zerocoin makes use of an \"accumulator\": An algorithm that allows one to combine a set of values into one short value. For a value within the accumulator, there exists a witness  that proves the inclusion, while at the same time, it is infeasible to find a witness for a value that was not accumulated.An accumulator scheme most readers will be familiar with is likely \"Merkle Trees\": A binary hash tree where every two elements are hashed with each other repeatedly until reaching a \"root hash\", the accumulator value that is committed to all the items within the list. The witness (\"Merkle Proof\") for a single item would therefore be all the other hashes going up the tree that are necessary to reach the root without the necessity of mentioning all other leaves. In \"Auditable, Anonymous Electronic Cash\" Sander and Ta‚ÄìShma used a zero-knowledge proof to show that an element is indeed contained within such a tree without revealing the element itself.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nZerocoin uses an \"RSA Accumulator\" scheme which, like Merkle Trees allows everyone to reproduce the accumulation of values without the need of knowing any secret trapdoor information. Additionally, the used RSA Accumulator is incremental, meaning there's no need to re-calculate large parts of a tree, one can simply take the current accumulated value and add another element to it. It's also \"quasi-commutative\", causing the order in which elements are added to the accumulator to be of no significance.\nThe incremental nature of RSA Accumulators is used to optimize Zerocoin: Each of Bitcoin's blocks would have had an \"Accumulator Checkpoint\", which is simply the final accumulated value after processing all transactions included within the block. Most clients can continue off this checkpoint instead of having to calculate the entire accumulator themselves.\nCreating an empty accumulator :Incrementally adding a commitment  to the accumulator yields an updated accumulator value:The order in which commitments are added is irrelevant:A witness to the accumulation of a commitment  is an accumulator state that excludes said commitment: \nAs a toy example, imagine the accumulator is a composite number while all its elements are prime numbers. As you may remember, all integers that aren't prime must be composite numbers which can be factored into a unique collection of prime numbers. (eg.  is not prime, therefore, it must be a composite number and can be represented by its prime factors: )After initializing the accumulator with  we can now add a few primes to the set:The accumulator  is the composite of all the prime numbers added to the set: .A witness  can simply be the state of the current accumulator divided through the number whose inclusion we want to prove:","interactive-zero-knowledge-proofs#Interactive Zero-Knowledge proofs":"The zero-knowledge proofs described in the paper can be instantiated using the technique of Schnorr. Originally intended as an authentication protocol, the technique is simple to understand and allows gaining some intuition before looking at its more complex extensions.\nAssume we have an agreed-upon generator  within a multiplicative cyclic group of order  for which the Discrete Logarithm Problem is hard. In simple terms, that means that an operation such as the following will be impractically difficult to reverse with appropriately chosen parameters:Schnorr's interactive authentication protocol allows the possessor of a random secret key  of public key  to prove knowledge of  without revealing anything about it. This makes it a Zero Knowledge Proof of Knowledge which can be represented by the following notation:These protocols usually involve a single Prover who is in possession of secret information (which will be colored red continuously throughout this article), communicating with a Verifier that the Prover wants to prove knowledge of the secret information. The following table shows Schnorr's scheme. A recording of these interactions with actual values would be referred to as a transcript.\nProver\tVerifier\tKnows \tKnows \tChooses a random value \t\t\t\tSends \tKnows \t\tChooses a random challenge value \tKnows \t Sends \t\t\tSends \tKnows \t\t\t\nWith the Discrete Logarithm Problem being hard, the verifier cannot determine  or  from  and  respectively, but is able to check that  was computed honestly becauseA prover would only be able to determine a legitimate value  by knowing the actual values of its components (). A malicious prover attempting to authenticate for a  is, just like the verifier, unable to extract  due to DLP. But knowing the value  is required for coming up with a valid  value.Looking at , you should also be able to notice why it's important that the prover adds randomness with . After all, if it were simply , the verifier would be able to extract  simply by division of known values: \nTo perform modular division, one has to multiply with the multiplicative inverse of the divisor within the modulus: \nLastly, it's essential that the prover is challenged with . If a prover were able to guess  before committing to a specific , the prover could choose to calculate an  (with a randomly chosen ) without knowing . This  would pass verification and allow the prover to fake knowledge about any public key's secret value.\nAssume we have an agreed-upon generator point  within an additive cyclic group of order  for which the Elliptic Curve Discrete Logarithm Problem is hard. In simple terms, that means that an operation such as the following will be impractically difficult to reverse with appropriately chosen parameters:Schnorr's interactive authentication protocol allows the possessor of a random secret scalar  of public key  to prove knowledge of  without revealing anything about it. This makes it a Zero Knowledge Proof of Knowledge which can be represented by the following notation:These protocols usually involve a single Prover who is in possession of secret information (which will be colored red continuously throughout this article), communicating with a Verifier that the Prover wants to prove knowledge of the secret information. The following table shows Schnorr's scheme. A recording of these interactions with actual values would be referred to as a transcript.\nProver\tVerifier\tKnows \tKnows \tChooses a random scalar \t\t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \t\t\tSends \tKnows \t\t\t\nWith the Discrete Logarithm Problem being hard, the verifier cannot determine  or  from  and  respectively, but is able to check that  was computed honestly becauseA prover would only be able to determine a legitimate scalar  by knowing the actual values of its components (). A malicious prover attempting to authenticate for a  is, just like the verifier, unable to extract  due to ECDLP. But knowing the scalar  is required for coming up with a valid  value.Looking at , you should also be able to notice why it's important that the prover adds randomness with . After all, if it were simply , the verifier would be able to extract  simply by division of known values: \nTo perform modular division, one has to multiply with the multiplicative inverse of the divisor within the modulus: \nLastly, it's essential that the prover is challenged with . If a prover were able to guess  before committing to a specific , the prover could choose to calculate the \"difference\"  (with a randomly chosen ) and send it as . This difference would be added during verification and allow the prover to fake knowledge about any public key's secret scalar.\nConventional DLP is a lot more fragile than ECDLP and requires choosing many of the above parameters very carefully.","making-interactive-proofs-non-interactive#Making Interactive Proofs Non-Interactive":"Interactive proofs are not publicly verifiable. The prover proved knowledge of  to the verifier, but not to anyone else. But this can be changed since, according to a transcript of the interaction, the verifier's sole purpose is to produce a random challenge. By replacing the verifier as random oracle with a hash function, for which there is no known way to distinguish a result from random, the prover is able to generate the challenge themselves while having it remain provably random.This makes the Interactive Proof transcript publicly verifiable without requiring a 1-to-1 interaction with any specific verifier. This idea of replacing an actual challenger with a random oracle is known as the Fiat-Shamir transform and turns the interactive Schnorr authentication protocol into the Schnorr signature scheme: With a hash function returning a different challenge  for different inputs, it's possible to add (by concatenation) more data to the input.The resulting transcript containing , for which  cannot be changed without knowledge of , is therefore a \"Zero-Knowledge Signature of Knowledge\" on .\nProver\tVerifier\tKnows \tKnows \tChooses a random value \t\t\t\t\t\t\t\tSends \tKnows \t\t\t\t\t\nOne obvious drawback of non-interactive proofs of knowledge is that they can no longer be used for authentication since they can fundamentally be replayed. (Unless it's turned interactive again by including a challenge in the message , which is a common way of doing authentication with websites in web3).\nInteractive proofs are not publicly verifiable. The prover proved knowledge of  to the verifier, but not to anyone else. But this can be changed since, according to a transcript of the interaction, the verifier's sole purpose is to produce a random challenge. By replacing the verifier as random oracle with a hash function, for which there is no known way to distinguish a result from random, the prover is able to generate the challenge themselves while having it remain provably random.This makes the Interactive Proof transcript publicly verifiable without requiring a 1-to-1 interaction with any specific verifier. This idea of replacing an actual challenger with a random oracle is known as the Fiat-Shamir transform and turns the interactive Schnorr authentication protocol into the Schnorr signature scheme: With a hash function returning a different challenge  for different inputs, it's possible to add (by concatenation) more data to the input.The resulting transcript containing , for which  cannot be changed without knowledge of , is therefore a \"Zero-Knowledge Signature of Knowledge\" on .\nProver\tVerifier\tKnows \tKnows \tChooses a random scalar \t\t\t\t\t\t\t\tSends \tKnows \t\t\t\t\t\nOne obvious drawback of non-interactive proofs of knowledge is that they can no longer be used for authentication since they can fundamentally be replayed. (Unless it's turned interactive again by including a challenge in the message , which is a common way of doing authentication with websites in web3).\nThe ECC Notation may be more intuitive for some readers, but since we're not dealing with points on curves in this article, further descriptions will make use of the Exponential Notation only.","the-math#The Math":"","strong-rsa-accumulator#Strong RSA Accumulator":"Having covered RSA intuition in a previous article, we'll not go into much detail here. The important parts are that RSA relies on prime factorization being hard for two carefully chosen primes , and that going backward from having calculated  is really hard without knowledge of trapdoor information.With RSA's ability to provide unpredictable one-way permutations, like a conventional hashing function, we can accumulate commitments by modular exponentiation:We can even accumulate multiple commitments at once by multiplying them:For this accumulator to be considered \"strong\" it needs to be collision-free. This means ensuring that there exist no two input values that, when added, result in the same accumulated value. Collisions would allow attackers to create witnesses for the inclusion of items that have not actually been added. This issue is avoided by ensuring that all commitments are prime.","trusted-setup#Trusted Setup":"To initialize the accumulator we need two random primes  and . To strengthen against attacks we wouldn't choose these directly, instead we'll randomly generate prime numbers  and  until the following conditions are met:Parameters ,  generated like this are considered \"safe primes\" while  and  are referred to as Sophie Germain primes. This makes  a \"rigid integer\" and very hard to factor.After calculating  within a trusted environment,  and  are no longer needed. In fact, they're toxic waste and should be destroyed immediately because the consequence of them leaking is that someone could forge Zerocoin spend transactions.\nIn other RSA Accumulator use cases with a trusted centralized party, this trapdoor information is actually useful: It allows removing a value  that is accumulated within  by calculating\n where .\nAlternatively, the Zerocoin paper suggested generating so-called RSA-UFOs (\"Un-Forgeable Opaque\") for accumulator parameters without a trapdoor. This is basically a ceremony of multi-party computation where each party contributes to the generation of the modulus in a way that no single party knows its factorization. The resulting modulus  should, with very high probability, have two large factors.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nMost Zerocoin implementations, including Zcoin, opted for neither of these setups. Instead, they utilized the RSA-2048 parameters generated in 1991 from the RSA factoring challenge, which had a USD200,000 prize if someone managed to factor them. The challenge ended in 2007 with nobody claiming the prize, but this still requires trusting the challenge organizers to truly have destroyed trapdoor information after generation.Lastly, we need to choose the accumulator's initial value , with  and . To find such a Quadratic Residue we calculate  where  can be a random value or something specific such as a representation of the current date. Although no further explanation on this was given, I assume it is because squaring does not create a permutation of the entire group modulo  as it only maps to quadratic residues and then won't impact the accumulator.","witness-generation#Witness Generation":"We've learned how to initialize an Accumulator and how to add commitments to it:To generate a witness  that the commitment  was indeed included within , we cannot simply use the accumulator's state  before the commitment was added. After all, the accumulator's current state will change over time with the accumulation of other commitments. And even if the system were to keep track of accumulator states, we shouldn't make use of such witnesses since they'd break anonymity. This is because while the commitment  and witness  are kept secret during the Zero-Knowledge inclusion proof, the resulting accumulator state  from  will be public. This allows drawing a connection between the Zerocoin minting transaction (when  was publicly added resulting in accumulator state ) and the Zerocoin spending transaction where inclusion of  is proven.Assuming that the system requires us to prove inclusion within the most current accumulator  (ie. according to the current block's accumulator checkpoint), we instead generate a witness that is an accumulator state with all of the same commitments contained within  lest our own . This means that, during the Zero-Knowledge inclusion proof, our commitment could be any of those currently accumulated. In practice, we're unable to remove our included commitment  from the current accumulator state  to generate a witness, since we lack the necessary accumulator trapdoor information to do so. Instead, we may store the accumulator state  from before we added our commitment  together with the other secret spending information  of our coin. Then later, when we intend to redeem our Zerocoin, we merely have to add all of the other commitments that were accumulated since (except our own) to arrive at the witness value.","pedersen-commitment#Pedersen Commitment":"When minting a Zerocoin, a commitment needs to be added to the appropriate accumulator. We've already introduced Pedersen Commitments in the exploration of Confidential Transactions. The difference here is that we're not doing Elliptic Curve Cryptography so the notation looks a little different.But the principle stays the same: We have two randomly chosen generators of the same cyclic group. We use the second generator to add a random blinding factor  to ensure that the committed serial number  can not be guessed with brute force. Furthermore, we'll only reveal  to prevent double-spending of Zerocoins, the blinding factor must remain secret as otherwise the commitment can be reconstructed and you'd be able to draw a connection between the minting (reveals commitment ) and the spending (reveals identifier ) transactions, breaking anonymity.Since Strong RSA Accumulators only allow prime numbers to be added, we may need a few attempts to find a pair  for which the resulting pedersen commitment is prime.\nNote that the modulus  used for the Pedersen Commitment is unrelated to the RSA Accumulator's trapdoor information, though similar in its generation:  where both  are prime with security parameter . Generators  are of a subgroup of order  from which the random values for  are taken.","zero-knowledge-proofs#Zero-knowledge Proofs":"So far, we've learned how to initialize an Accumulator , and how to create a prime commitment  to a Zerocoin's serial number , blinded by a random value . We generate a witness , with an accumulator state where  has not been added, that we can use to prove inclusion of the commitment in . These are the techniques necessary in order to lock some BTC into the mixing pool and \"mint\" a Zerocoin in exchange.To redeem the Zerocoin later, we'd have to prove that (1) our coin's commitment  is indeed included within the Accumulator , and (2) that the unspent Serial number  we're revealing was indeed the one that was committed to. But, in order to stay anonymous, we must prove this without revealing , , or  since any of these would allow connecting the redemption to the transaction that minted the Zerocoin.To accomplish this, the paper described the following Zero Knowledge Signature of Knowledge  on transaction data :","proof-of-accumulator-inclusion#Proof of Accumulator Inclusion":"For the first part, proving that a committed value is accumulated, the Zerocoin paper and other publications omit detailed explanations and instead refer the reader to the original protocol presented by Camenisch and Lysyanskaya. They summarize that the described proof is then converted into a Non-Interactive Zero-Knowledge Proof of Knowledge via Fiat-Shamir transform:\nThe authors consider the described Zero-Knowledge Proof of Knowledge efficient; although the large proof sizes and the resulting inefficiencies are arguably one of Zerocoin's biggest drawbacks. But still, compared to an inclusion proof that, like Ring Signatures, grows linearly with each member within the group, the used proof is indeed much more efficient (logarithmic).\nTo construct the proof, we once again need to make use of a pedersen commitment , which commits to the commitment  which was added to the accumulator as a value. The proof then works by showing that the value  is contained in both the commitment  as well as within the accumulator  without revealing . Only the new commitment  has to be revealed as part of this protocol.\nIn addition to being prime, further restrictions on the choice of the  commitment values are necessary to ensure the proof's security: First, commitments must be within a sub-range  as  to guarantee that the product of any two commitments falls outside of the range. Second, for  and the choice of  it's required that  holds, where  are adjustable security parameters.\nNext, we need a few more auxiliary (helper) commitments. While 's generators are from a subgroup of order  within , these auxiliary commitments instead have  which are two elements from  (quadratic residues within the accumulator's modulo ) for which  is unknown (similarly to how in ECC the relationship  must remain unknown in order to prevent the prover from tempering with the committed values). The blinding factors  are chosen randomly from .\nProver\tVerifier\tKnows \tKnows \tChooses random values \t\t\t\t\t\t\t\tSends \tKnows \t\nLike , the auxiliary commitment  commits to , the value accumulated by . Additionally,  commits to witness  and will be used to prove that it corresponds to the -th root value  ().\nProver\tVerifier\tKnows \tKnows \tChooses random values \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tSends \tKnows \t\tChooses a random challenge \tKnows \t Sends \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tSends \tKnows \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\nNote that the random exponents have to be selected as follows: .\nGranted, this Zero-Knowledge Proof protocol appears much more complex than what we initially introduced as the technique of Schnorr. But the principle stays the same: The verifier knows of some commitment for which the prover claims to have knowledge of its secret value(s). Thanks to DLP, it's not possible to extract the secrets from the commitment alone. The prover chooses randomness () and uses it to calculate temporary commitments  which are sent to the verifier. The verifier too generates randomness and sends it to the prover as the challenge . The prover mixes () its own randomness with the challenge and the secrets in a manner that will allow for things to cancel each other out for the verifier, resulting in a value matching that of the temporary commitments . The verifier knows that the prover wouldn't have been able to generate the appropriate exponents () without knowledge of the secret values, for things to match at the end.\nSubstitute \nSubstitute Since we're in an abelian group, the following holds true: \nSubstitute Since we're in an abelian group, the following holds true: \nSubstitute \nSubstitute \nSubstitute \nSubstitute \nIf you compare the above to the actual paper you may notice some differences. Unfortunately, the paper had several errors in its description of the Proof of Knowledge which I had to correct. I confirmed these fixes with the actual implementation in libzerocoin.","proof-of-commitment-opening-ability#Proof of Commitment opening ability":"Having proven that  commits to a value  that is contained within the RSA accumulator, we next have to prove that we know of such  that is committed to a serial number  that we'll be revealing. To do that, you guessed it, we need yet another commitment  that is committed to  as well:The complication here is that  is a secret value and also an exponent of the accumulator's group, for which we need to prove that we're able to open the commitment  (ie. proving knowledge of ) without revealing it. This requires a double-discrete logarithm proof.\nProver\tVerifier\tKnows \tKnows \tChooses random values \t\t\t\tSends \tKnows \t\tChooses random challenge boolean \tKnows \t Sends \t   \t\t   \t\tSends \tKnows \t\t   \t\t   \t\nThe original Zerocoin paper does not provide much information on the construction of this proof, referring the reader to a future \"full version\". I assume that to be Miers' dissertation, which seems to be mostly a copy containing both the Zerocoin and Zerocash papers. Based on other works I assume that the abelian group of this proof is constructed in the same manner as that of the commitment .\nIn this protocol, the verifier responds with a boolean challenge value  which is like a \"coin flip\" used for a technique called \"cut-and-choose\". We mentioned before that correctly guessing the challenge  is fatal to the Zero Knowledge Proof since it would allow the prover to fake knowledge of the secret values. In case of a \"coin flip\"-like challenge, the prover has a 50/50 chance of guessing correctly. This danger is reduced by choosing a security parameter  that determines how many rounds of the proving game are played. Note that each round  can be played in parallel with the other rounds for efficiency.\nSubstitute \nSubstitute","proof-of-commitment-sameness#Proof of Commitment sameness":"If you were under the impression that we'd be done after two proofs, you wouldn't be alone. After all, there was no paper published on Zerocoin that mentioned the existence of a third proof, the only way to know about it was by reading the actual implementation within the libzerocoin library. Most cryptographers aren't exactly coders, and most coders aren't exactly cryptographers. When cryptographers look for cryptographic flaws, they do so less within the actual code. And when coders review the actual code, they're usually not looking for cryptographic mistakes but rather for implementation issues.It happened as it was bound to happen: Someone discovered a cryptographic flaw in the undocumented proof and promptly started exploiting cryptocurrencies using it. Some currencies implemented a hotfix, others turned off their chain's Zerocoin functionality and never turned it on again. Zcoin was in a bit of a better position: They were already working on replacing the Zerocoin Protocol with Sigma and launched it on their mainnet not long after the attack.\nSo far, the first proof shows that commitment  is indeed within the accumulator, representing a validly minted Zerocoin. The second proof shows that  really commits to a serial number , which, since already redeemed commitments aren't being removed from the accumulator, is remembered in order to prevent double-spending. What's missing is a proof showing that both of them are even talking about the same commitment . Without this, one could re-use the same commitment for the first proof over and over again (assuming that it really was added to the accumulator), but use some completely different commitment for the serial number (assuming that serial number had not been used yet, not necessary for the commitment to actually be accumulated).\nProver\tVerifier\tKnows \tKnows \tChooses random values \t\t\t\t\t\tSends \tKnows \t\tChooses random challenge \tKnows \t Sends \t\t\t\t\t\t\tSends \tKnows \t\t\t\t\t\nSubstitute \nSubstitute \nThe complication in this proof stems from the fact that, while both  and  should commit to the same , the commitments are made under different groups (ie. different moduli, orders, and generators). The problem with that is, that the value of  can be relatively large, but the group orders  and  are smaller. Therefore a  can be chosen that satisfies the following conditions:To forge a coin, the attacker creates a proof with  from a legitimately minted Zerocoin commitment . Then the attacker creates a proof with  of a commitment  for a Zerocoin that wasn't actually minted. Finally, the attacker can forge the above proof with a  value that is equal to  within a group of order  and equal to  within a group of order . With that, it will appear that both  and  are referring to the same commitment .\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThe Zcoin team suggested adjusting security parameters in a manner that would avoid this issue, but they called it a \"band-aid\" that may introduce other unforeseen problems. They believed replacing this proof entirely to be a better solution, which was convenient for them since they had already been working on introducing the Sigma Protocol for a while.","the-code#The Code":"Here would normally be the part where I showcase a simple EVM implementation of the protocol presented. This time we'll instead look at Zeth, which is exactly that, an implementation of the libzerocoin code converted to solidity in order to run Zerocoin on Ethereum. Although its development was abandoned during the prototyping phase (due to the enormous gas cost making actual usage impractical), exploring the code will prove useful for connecting the theory of math formulas to some concrete real-world code.\nThis implementation of the Zerocoin Protocol was written by Tadhg Riordan in 2017, but before he was able to do so, he actually needed to create a BigNumber library for Solidity. While an EVM's word can fit a 256-bit integer, an arguably enormous number, the modulus used for the RSA Accumulator is 2048 bits large, which requires 8 words to fit.\nThe library's struct BigNumber{:solidity} holds the number's value within Solidity's bytes type, which can store byte strings of variable length. Additionally, the struct keeps track of the number's sign (+/-) and its bit length (position of the most significant bit). The library also provides functions to make actual calculations (add, sub, mul, ...) between big numbers.","parameters#Parameters":"At the beginning, we find definitions of various parameter values. Noteworthy is the constant zkp_iterations, the security parameter  for the Zero Knowledge Proof for , showing that its commitment  really does commit to the Zerocoin's serial number . The implementation chooses to execute 80 iterations (rounds) of the \"coin flip\" proof, which means the chance for the prover guessing all rounds correctly is . This is in accordance with the paper, which suggested that a small size of 80 bits would be sufficient since breaking it would only allow forgery of a single coin.After that, we find the security parameters  and  respectively, restricting the value of the commitment  to the range  with . While omitted in the excerpt, this implementation too makes use of the RSA-2048 parameter generated in the 1991 factoring challenge.\nNext, we see the declaration of several commitment_group (abelian group) instances:\ncoin_commitment_group for the commitment , has generators  and  from  for the cyclic group of a 256-bit order  with a 1024-bit modulus , both prime. The Zerocoin paper describes the relationship between these as  with , but it seems that the  and  used here have been created in a different way.\naccumulator_pok_commitment_group for the commitment  of the inclusion proof, has generators  and  from the cyclic group of a 257-bit order  with a 556-bit modulus , both prime.\naccumulator_qrn_commitment_group for the auxiliary commitments  of the inclusion proof we have elements  and  from . This group is defined with a modulus and group_order of 0, with calculations instead using the RSA Accumulator's modulus .\nserial_number_sok_commitment_group for the commitment  of the serial number proof, has generators  and  from the cyclic group of a 1024-bit order  with a 1033-bit modulus , both prime.\nInterestingly, the modulus of coin_commitment_group, and the group_order of the serial_number_sok_commitment_group are both the same, and equal to the parameter max_coin_value, representing the maximum value that the commitment  is allowed to have. It's a shame the papers are so sparse on explanations of these parameters, what their impact is, and how to best select them.\nFinally, we come across the base value  that the accumulator is initialized with. The value 0x03c1 is  in decimals, which is indeed a quadratic with . No further comment on how this choice was made though.","business-logic#Business Logic":"Work on this implementation was never completed, and it especially shows in the functions for minting and spending Zerocoins. I assume the intention was for the minting function to be payable{:solidity} so that a caller is able to pass in some amount of ether and mint a Zerocoin of the appropriate denomination in return. Then later, the user would be able to spend the Zerocoin anonymously from a different address, receiving back the ether value they had paid. The Zerocoin contract would act as a mixing pool, similar to how protocols like Tornado Cash work. Unfortunately, the code is in no state to do these things, there are merely a lot of TODO comments hinting at the intentions.\nHere we find the storage variables used by the minting and spending functions. The serial_numbers () are tracked to prevent double spending of the same coin. Commitments  and iteratively computed accumulator states  are tracked in both mappings and arrays.\nIn the minting function itself, we can observe the commitment  validation: Ensuring that  is within the allowed range , that  is prime, and that  has not been added to the accumulator before. After that, it loads the previously calculated accumulator state  and updates it with .\nThe author was unable to make the spending function public as intended due to restrictions on how Solidity allowed data structures to be passed externally at the time. Instead, it seems there are some other publicly exposed functions that allow passing in the Proof of Knowledge parameters in an unserialized manner. Let's ignore those for now and move on with the assumption that this function would've been used.Three Proof/Signature of Knowledge structs are passed into the spending function, one for each of the proofs we've covered:\nstruct Accumulator_pok{:solidity} containing the auxiliary commitments , the parameters , , and the exponents . Calculation of the challenge  from a hash was not yet implemented, but it would likely have been .\nstruct Serial_number_sok{:solidity} containing two arrays, one for each  and . Furthermore, it contains a hash of which each bit represents one coin flip challenge () with the hash created from  with  rounds. This proof is also a Signature of Knowledge since it signs some transaction data .\nstruct Commitment_pok{:solidity} containing  and the challenge  which is a .\nThe function would then verify all of them (assuming the last one wouldn't be commented out), ensuring that the serial number hasn't been spent already, and finally mark it as spent after paying out the appropriate amount of ether.","proof-verification#Proof Verification":"Calculating the multiplicative inverse of a number is rather expensive, especially in a costly environment such as the EVM where there's no native function to do so. The Zerocoin contract, instead of calculating those inverses on-chain, has them passed in as function parameters in inverse_results. This can be done safely since it's very cheap to verify whether the number really is the inverse: .\nAt the beginning of the Accumulator Inclusion Proof verification function, we find a check for whether the passed accumulator  exists in the contract's storage. Remember that the proof works by showing  where the witness is an accumulator state  without . This means that we're allowed to choose a witness that does not necessarily result in the most current accumulator state. This is likely to ensure that Zerocoin spend transactions won't fail when they get frontrun by other coin mints (which would update the current accumulator state).Following that check, we see the calculation of the challenge  (c) from . With the protocol being non-interactive, that hash is the same one that the prover calculated after committing to the  values but before calculating the exponents  using . The validation then consists of reproducing the same  values using the challenge and exponents.Due to the complexity of the proof, the code snippet was reduced to show only the following checks:With the other verifications mostly being more of the same, I'll stop here. Unfortunately, I wasn't able to find the client that was intended to be used with these contracts, so I'm unable to present the code that was intended to build the ZK Proofs that are being verified here. But if you're curious, the original C++ code of libzerocoin is still available on github and not too bad in terms of readability.\nSo far you might have the impression that the accumulator is providing a global anonymity set, but looking at the actual Zcoin source code from that time we find the following constants within zerocoin_params.h:\nAs you can see, the number of commitments that could be added to an accumulator was actually restricted to only 10 in the beginning and later increased to 10k. Once an accumulator was \"full\", it would simply start from the beginning with an empty one. Each accumulator had a unique identifier that needed to be passed together with the serial number of the Zerocoin you wanted to redeem. These restrictions were put into place because forging an inclusion proof becomes easier with more and more commitments accumulated.","conclusion#Conclusion":"The history of the Zerocoin Protocol was a troubled one. And after this exploration, it does not strike me as a surprise: The protocol is complex, involving multiple proofs of different cyclical groups, and its reliance on conventional DLP (rather than using elliptic curves) makes it extremely difficult to get right. These technological choices were no accident though. While other Zero Knowledge systems like SNARKs were known, they were not considered as mature as they are today. Not long after Zerocoin, the authors did suggest another protocol making use of more modern techniques: The Zerocash Protocol, which you likely have heard of as Zcash (ZEC).\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThose newer methods especially promised a drastic reduction in proof size. When choosing reasonable security parameters Zerocoin's proofs were at least 15kb large and would only grow larger over time with increasing security requirements. Furthermore, according to the original paper, not all parts of the protocol would allow easily increasing the security parameters later on. Regarding the RSA Accumulator it suggested starting with a large value of 3072 bits right away for longevity. Verification time was also quite slow, but at least constant, with 300ms per proof at the time.The paper suggested various improvements that could be made to the protocol and there were follow-ups on some of them such as divisible coins. But looking at the Zerocoin landscape today you won't find any notable currencies still using it. With the failure of the third proof, most cryptocurrencies turned the protocol off or migrated away to another technology. Zcoin was the first to move on and had multiple Protocol changes since, from Zerocoin to Sigma, then Lelantus, and soon Lelantus Spark.","appendix#Appendix":"","tech-tree#Tech-Tree":"Note that this Tech-Tree omits detailed dependencies that are not specific to Zerocoin to maintain readability.","history#History":"Zerocoin Whitepaper published in IEEE Symposium on Security and Privacy\nInitial release of libzerocoin\nFirst stand-alone cryptocurrency Moneta with libzerocoin\nMoneta testnet launched\nZcoin mainnet launched (renamed from Moneta)\nPIVX, a Proof of Stake chain, launches with Zerocoin implemented\nZoin (later Noir), a Zcoin fork, launches without founder rewards\nIan Miers' dissertation published, containing missing proofs\nZcoin suffers inflation attack due to implementation issues in libzerocoin\nHexxCoin launches with Zerocoin implemented\nAudit revealed implementation issues in libzerocoin allowing theft and inflation attacks\nAudit also revealed a fundamental cryptographic flaw in \"Burning Zerocoins for fun and profit\"\nZcoin introduced Dandelion++ protocol, hiding sender IP addresses via TOR/VPN\nSmartCash launches with Zerocoin\nVeil launches enforcing all transaction to use Zerocoin\nCryptographic flaw in proving system was exploited for inflation attack across projects\nZcoin departs from Zerocoin Protocol with Sigma Protocol\nZcoin rebrands to Firo\nFiro activated Lelantus Protocol on mainnet\nFiro implemented Receiver Address Privacy (RAP), an adaption of BIP47\nFiro activates Lelantus Spark protocol on mainnet","references#References":"Miers, I., Garman, C., Green, M. and Rubin, A.D., 2013, May. Zerocoin: Anonymous distributed e-cash from bitcoin. In 2013 IEEE Symposium on Security and Privacy (pp. 397-411). IEEE.\nSatoshi Nakamoto, 2008, Bitcoin: A Peer-to-Peer Electronic Cash System, https://bitcoin.org/bitcoin.pdf\nSander, T. and Ta-Shma, A., 1999. Auditable, anonymous electronic cash. In Advances in Cryptology‚ÄîCRYPTO‚Äô99: 19th Annual International Cryptology Conference Santa Barbara, California, USA, August 15‚Äì19, 1999 Proceedings 19 (pp. 555-572). Springer Berlin Heidelberg.\nChaum, D., Fiat, A. and Naor, M., 1990. Untraceable electronic cash. In Advances in Cryptology‚ÄîCRYPTO‚Äô88: Proceedings 8 (pp. 319-327). Springer New York.\nChaum, D., 1983, August. Blind signatures for untraceable payments. In Advances in Cryptology: Proceedings of Crypto 82 (pp. 199-203). Boston, MA: Springer US.\nRivest, R.L., Shamir, A. and Adleman, L., 1978. A method for obtaining digital signatures and public-key cryptosystems. Communications of the ACM, 21(2), pp.120-126.\nDiffie, W. and Hellman, M.E., 1676. New directions in cryptography. In Democratizing Cryptography: The Work of Whitfield Diffie and Martin Hellman (pp. 365-390).\nCamenisch, J. and Lysyanskaya, A., 2002. Dynamic accumulators and application to efficient revocation of anonymous credentials. In Advances in Cryptology‚ÄîCRYPTO 2002: 22nd Annual International Cryptology Conference Santa Barbara, California, USA, August 18‚Äì22, 2002 Proceedings 22 (pp. 61-76). Springer Berlin Heidelberg.\nBariƒá, N. and Pfitzmann, B., 1997, May. Collision-free accumulators and fail-stop signature schemes without trees. In International conference on the theory and applications of cryptographic techniques (pp. 480-494). Berlin, Heidelberg: Springer Berlin Heidelberg.\nBenaloh, J. and De Mare, M., 1993, May. One-way accumulators: A decentralized alternative to digital signatures. In Workshop on the Theory and Application of of Cryptographic Techniques (pp. 274-285). Berlin, Heidelberg: Springer Berlin Heidelberg.\nMaxwell, G. and Poelstra, A., 2015. Borromean ring signatures. https://raw.githubusercontent.com/Blockstream/borromean_paper/master/borromean_draft_0.01_34241bb.pdf\nSchnorr, C.P., 1990. Efficient identification and signatures for smart cards. In Advances in Cryptology‚ÄîCRYPTO‚Äô89 Proceedings 9 (pp. 239-252). Springer New York.\nCramer, R., Damg√•rd, I. and Schoenmakers, B., 1994, August. Proofs of partial knowledge and simplified design of witness hiding protocols. In Annual International Cryptology Conference (pp. 174-187). Berlin, Heidelberg: Springer Berlin Heidelberg.\nCamenisch, J. and Michels, M., 1999. Proving in zero-knowledge that a number is the product of two safe primes. In Advances in Cryptology‚ÄîEUROCRYPT‚Äô99: International Conference on the Theory and Application of Cryptographic Techniques Prague, Czech Republic, May 2‚Äì6, 1999 Proceedings 18 (pp. 107-122). Springer Berlin Heidelberg.\nCamenisch, J., 1998. Group signature schemes and payment systems based on the discrete logarithm problem (Doctoral dissertation, ETH Zurich).\nBrands, S., 1997, May. Rapid demonstration of linear relations connected by boolean operators. In International Conference on the Theory and Applications of Cryptographic Techniques (pp. 318-333). Berlin, Heidelberg: Springer Berlin Heidelberg.\nFiat, A. and Shamir, A., 1986, August. How to prove yourself: Practical solutions to identification and signature problems. In Conference on the theory and application of cryptographic techniques (pp. 186-194). Berlin, Heidelberg: Springer Berlin Heidelberg.\nChase, M. and Lysyanskaya, A., 2006. On signatures of knowledge. In Advances in Cryptology-CRYPTO 2006: 26th Annual International Cryptology Conference, Santa Barbara, California, USA, August 20-24, 2006. Proceedings 26 (pp. 78-96). Springer Berlin Heidelberg.\nRuffing, T., Thyagarajan, S.A., Ronge, V. and Schroder, D., 2018, June. (Short Paper) Burning Zerocoins for Fun and for Profit-A Cryptographic Denial-of-Spending Attack on the Zerocoin Protocol. In 2018 Crypto Valley Conference on Blockchain Technology (CVCBT) (pp. 116-119). IEEE.\nDanezis, G., Fournet, C., Kohlweiss, M. and Parno, B., 2013, November. Pinocchio coin: building zerocoin from a succinct pairing-based proof system. In Proceedings of the First ACM workshop on Language support for privacy-enhancing technologies (pp. 27-30).\nReuben Yap, 2017, April, Zcoin moving beyond trusted setup in Zerocoin, https://firo.org/id/2017/04/21/zcoin-moving-beyond-trusted-setup-in-zerocoin.html\nMiers, I., 2017. Decentralized anonymous payments (Doctoral dissertation, Johns Hopkins University).\nSiri Dahle, 2018, June, Anonymity for Decentralized Electronic Cash Systems, Master of Science thesis at Norwegian University of Science and Technology Department of Mathematical Sciences.\nMatthew Green, 2016, July, Zerocoin: Anonymous Distributed E-Cash from Bitcoin, lecture at Microsoft Research, https://www.youtube.com/watch?v=4uWlqPIb1zw\nReuben Yap, 2019, May, Zerocoin Critical Flaw Explained With Reuben of Zcoin Crypto, Lark Davis, https://www.youtube.com/watch?v=FEoda23otIY\nTadhg Riorden, 2018, June, Tadhg Riorden Explains Zerocoin Protocol at Bitcoin Wednesday, Zeth: Zerocoin on Ethereum, Bitcoin Wednesday, https://www.youtube.com/watch?v=ZZyqkLDpc3A\nReuben Yap, 2019, April, Cryptographic description of Zerocoin attack, https://web.archive.org/web/20190430131937/https://zcoin.io/cryptographic-description-of-zerocoin-attack/\nChristina Garman, Matthew Green, Ian Miers, and Aviel D Rubin. Rational zero: Economic security for Zerocoin with everlasting anonymity, on creating divisible coins and no longer having to have separate Zerocoins per denomination. In International Conference on Financial Cryptography and Data Security, pages\n140‚Äì155. Springer, 2014.\nGroth, J. and Kohlweiss, M., 2015, April. One-out-of-many proofs: Or how to leak a secret and spend a coin, Zcoin's Sigma protocol. In Annual International Conference on the Theory and Applications of Cryptographic Techniques (pp. 253-280). Berlin, Heidelberg: Springer Berlin Heidelberg.\nJivanyan, A., 2019. Lelantus: Towards Confidentiality and Anonymity of Blockchain Transactions from Standard Assumptions. IACR Cryptol. ePrint Arch., 2019, p.373. https://lelantus.io/\nBinance Research (Etienne), 2020, Februrary, An examination of the flaws in the Zerocoin protocol, https://www.binance.com/en/research/analysis/zerocoin-flaws#2.-Historical-flaws-and-incidents-in-Zerocoin-based-cryptos\nReuben Yap, 2019, April, Further Disclosure on Zerocoin vulnerability, https://web.archive.org/web/20190501035025/https://zcoin.io/further-disclosure-on-zerocoin-vulnerability/\nReuben Yap, 2017, April, Zcoin moving beyond trusted setup in Zerocoin, https://firo.org/id/2017/04/21/zcoin-moving-beyond-trusted-setup-in-zerocoin.html\nReuben Yap, 2019, April, Lelantus: Zcoin‚Äôs next gen privacy protocol, https://web.archive.org/web/20190426131338/https://zcoin.io/lelantus-zcoin/\nReuben Yap, 2018, April, A statement on the paper ‚ÄúBurning Zerocoins for fun and profit‚Äù, https://web.archive.org/web/20180909001327/https://zcoin.io/statement-paper-burning-zerocoins-fun-profit/\nPoramin Insom, 2017, November, Zcoin hard fork statement, https://web.archive.org/web/20171113112736/https://zcoin.io/zcoin-hard-fork-statement/\nMerkle, R.C., 1987, August. A digital signature based on a conventional encryption function. In Conference on the theory and application of cryptographic techniques (pp. 369-378). Berlin, Heidelberg: Springer Berlin Heidelberg.\nDobson, S., Galbraith, S. and Smith, B., 2022. Trustless unknown-order groups. arXiv preprint arXiv:2211.16128.\nSchnorr, C.P., 1991. Efficient signature generation by smart cards. Journal of cryptology, 4, pp.161-174.\nGoldwasser, S., Micali, S. and Rackoff, C., 1985. The Knowledge Complexity of Interactive Proof-Systems In ACM Symposium on Theory of Computing.\nPedersen, T.P., 1991, August. Non-interactive and information-theoretic secure verifiable secret sharing. In Annual international cryptology conference (pp. 129-140). Berlin, Heidelberg: Springer Berlin Heidelberg.\nCamenisch, J. and Michels, M., 1999. Proving in zero-knowledge that a number is the product of two safe primes. In Advances in Cryptology‚ÄîEUROCRYPT‚Äô99: International Conference on the Theory and Application of Cryptographic Techniques Prague, Czech Republic, May 2‚Äì6, 1999 Proceedings 18 (pp. 107-122). Springer Berlin Heidelberg.\nBrands, S., 1997, May. Rapid demonstration of linear relations connected by boolean operators. In International Conference on the Theory and Applications of Cryptographic Techniques (pp. 318-333). Berlin, Heidelberg: Springer Berlin Heidelberg."}},"/posts/2024/3/4/race-27-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #27 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-27, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. It was designed by the Secureum Mentor Alex The Entreprenerd, Security Researcher at Spearbit, Judge at Code4rena and former Smart Contract Developer at BadgerDAO.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nMarch 4, 2024 by patrickd","code#Code":"All 8 questions in this RACE are based on the code snippet below, which was shown for all questions.","question-1-of-8#Question 1 of 8":"Functions depositing and withdrawing from the Gauge:\n A. Should use a reentrancy guard \n B. Should have a minOut parameter \n C. Should verify that we have sufficient balance before making the call \n D. Are perfectly safe because it's an official Curve gauge \nCorrect is D.A: The invest() and divest() functions, which are the only ones depositing or withdrawing from the gauge, are already using a reentrancy guard thanks to the nonReentrant modifier. This option implies that such a guard was missing, which is incorrect and makes this option false.B: The divest() function already has such a parameter, for the invest() function its called minMint instead. Again both already have it, while the option implies that its missing. It can furthermore be noted that gauge contracts have no slippage, so these parameters aren't actually related to the fact that the gauge is being called.C: Since the gauge does not want to get scammed, it'll check whether we have sufficient balance for us and there's no need to check this before making the call. If we had insufficient balance it would revert.D: The gauge used is hardcoded and is indeed an official Curve gauge which will behave as expected by this code.","question-2-of-8#Question 2 of 8":"The function shouldDivest:\n A. Can never return true as the investedBalance function will not revert \n B. Can revert because this may be selfdestructed when we delegatecall to a proxy \n C. Can never return true because the catch clause doesn't catch Bytes Error \n D. None of the above \nCorrect is D.A: The investedBalance() function will revert when the uint256 value returned by balanceInPool() is too large to fit into uint128 when multiplied in bal_1 * PRECISION * balanceInPool(). This is because the expression is casted to uint128 because of bal_1, the fact that it is back-cast to uint256 in the end does not matter.B: Random gibberish option.C: The } catch { will catch all errors, even if it is  some \"Bytes Error\", because it's not specifying anything specific that it is trying to catch.","question-3-of-8#Question 3 of 8":"The owner can:\n A. Steal all funds by withdrawing them to a malicious strategy \n B. Lose their private key \n C. Be impersonated due to Multicall and Meta Transactions support \n D. Set only one strategy per reward and want combination \nCorrect is A, B, C.A: The owner is capable of arbitrarily adding new strategies, and using invest() and divest() to move funds between them. A malicious owner could indeed simply add a strategy that allows stealing all of the funds after moving them there.B: Protocol owners can indeed lose their private key, a simple and true statement. Sadly it happens all the time, or is used to cover up a rug pull. Since this is off-chain operational security, it's hard to tell and often not much thought is put into preventing it.C: Account Impersonation is indeed possible through combining ERC-2771 and Multicall with the used version of OpenZeppelin which does not contain the appropriate fix yet. If you're unfamiliar with this bug, take a look at the detailed write-up about it in the Ethereum Smart Contract Auditor's 2023 Rewind.D: While there's code that prevents adding the same swap route in allowedSwap, this check is directional. Even if [reward][want] has already been set, there's always a second combination involving both of these values that is still allowed (basically [want][reward]).","question-4-of-8#Question 4 of 8":"The System is:\n A. Using a swapper to allow custom swaps in the future \n B. A Template system for generating NFTs \n C. Using a Controller and a Strategy where the Controller can work with only one strategy \n D. None of the above \nCorrect is A.A: It indeed appears to be using a \"swapper\".B: Just random gibberish again.C: The controller appears to work with many Strategies.","question-5-of-8#Question 5 of 8":"The Strategy:\n A. Cannot be forced to invest and divest due to ownership checks in the Controller \n B. Can be made to lose all of its rewards by claiming on behalf of the strategy \n C. Can be made to lose all of its rewards because the reward token is Fee-on-Transfer \n D. Can send tokens to the wrong address \nCorrect is A, B.A: These operations are indeed permissioned by an ownership check.B: Is true when exploiting a gauge loss-of-yield attack.C/D: More gibberish.","question-6-of-8#Question 6 of 8":"Harvest for the Strategy:\n A. Will claim reward tokens and send them to the Controller \n B. Doesn't use safeTransfer and so it will revert \n C. transfer will not revert because the reward token is known \n D. Can overflow via a donation during gauge execution that requires no changes in the external contracts \nCorrect is A, C.A: That's indeed what the code appears to do.B/C: The token we're calling transfer() on is a hardcoded Curve token that is known to work as expected by this code. A safeTransfer wrapper would only be necessary if it's unknown how the used token handles errors.D: You guessed it, gibberish it is.","question-7-of-8#Question 7 of 8":"The mapping lastSafeBalanceOfWant[strategy]:\n A. Is always zero\n B. Should be zero if no direct donation has happened\n C. Will track the balance of want of the controller\n D. Will prevent sweep exploits in invest\nCorrect is B.As you can tell from the code comment, the way that lastSafeBalanceOfWant[strategy] is currently used is incorrect and should be swapped.A/B: Although it is most likely to be zero indeed, it can't be said that it's always zero because there's the possibility that a \"direct donation\" (ie. a injection of funds) happened.C: It's not tracking as intended since setting the mapping's value and checking it are swapped.D: Incorrect, this offers no protection.","question-8-of-8#Question 8 of 8":"The function invest:\n A. Allows the owner to deposit want, convert it into Tricrypto LP and stake in the gauge \n B. Allows an attacker to deposit in the wrong strategy \n C. Allows an attacker to cause loss of  value because lastSafeBalanceOfWant[strategy] is ineffective \n D. None of the above \nCorrect is A, B, C.A: That's factually what it does. You can see that it's indeed \"Tricrypto LP\" by checking the gauge's token tracker in arbiscan.B/C: Both are possible thanks to the ERC-2771 + Multicall bug mentioned in Question 3"}},"/posts/2024/4/2/race-28-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #28 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-28, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. It was designed by our recently joined Secureum Mentor Palina, Research and Verification Engineer at Runtime Verification Inc.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nApril 2, 2024 by patrickd","code#Code":"Note: All 8 questions in this quiz are based on the ERC4626Vault contract snippet. This is the same contract snippet you will see for all the 8 questions in this quiz.ERC4626Vault code snippet illustrates an implementation of an ERC4626-based tokenized yield-bearing vault that represents shares of a single underlying ERC-20 token. Assume that all strictly required ERC4626 functions and events that are not shown are implemented correctly. Assume that the underlying (asset) token contract can be any token following an ERC-20 compatible standard.","question-1-of-8#Question 1 of 8":"Is it possible for an attacker to drain this vault contract?\n A. Yes, due to integer underflow in withdrawal\n B. Yes, because mint function is reentrant\n C. Yes, due to the wrong rounding direction\n D. No, it is not possible\nCorrect is B, C.Palina's answer: There‚Äôs no integer overflow in the withdrawal() function since the contract is written in Solidity 0.8. There is reentrancy in mint() if the underlying asset is an ERC777 token as the Check-Effects-Interactions pattern is not followed. previewWithdraw(), which calculates the number of shares a user has to supply to receive a given number of the underlying tokens, rounds down instead of rounding up which might lead to the loss of funds.I'd add that even a contract written with Solidity 0.8.x can have integer overflows, but that would require the use of unchecked or assembly blocks - neither of which are used in the withdrawal() function.As the info-box above the code mentioned, we are to \"assume that the underlying (asset) token contract can be any token following an ERC-20 compatible standard\". ERC777 tokens are an ERC-20 backward-compatible standard that implements \"hooks\" which are functions that get called when tokens are sent (tokensToSend) or received (tokensReceived). That means that the sender (ie. owner of tokens being sent) or recipient can take over the execution flow and exploit contract state that hasn't been fully updated yet (representing an exploitable reentrancy vector).Rounding should always happen in favor of the protocol to prevent profitable attacks. That means that, when withdrawing a specified amount of assets, the shares to redeem in exchange should be rounded up. When redeeming a specific amount of shares, the amount of returned assets should be rounded down. When depositing a certain amount of assets, the number of shares should be rounded down. When minting a specified amount of shares, the number of assets to be deposited should be rounded up.The previewWithdraw() function returns the number of shares that need to be burned in order to return the specified amount of assets. As such, the number of shares should be rounded up instead of down.","question-2-of-8#Question 2 of 8":"Which is true about the rounding directions according to the standard?\n A. withdraw should round up, redeem should round down \n B. withdraw should round down, redeem should round up \n C. withdraw and redeem should round down \n D. None of the above, all computations must be exact \nCorrect is A.Palina's answer: According to the EIP (https://eips.ethereum.org/EIPS/eip-4626), withdraw should round up, while redeem should round down, thereby ensuring that rounding favors the protocol rather than the user.Also see Question 1 solution.","question-3-of-8#Question 3 of 8":"What is the EIP conformant slippage protection for minting shares in ERC4626Vault?\n A. ERC4626Vault is not vulnerable to slippage by design\n B. Make mint internal\n C. mint should have an extra parameter minShares to be compared with the actual amount of minted shares, e.g., require(assets > minShares);\n D. Slippage protection could be added to a router-like contract that is interacting with ERC4626Vault instead\nCorrect is D.Palina's answer: Slippage is one of the major security concerns related to the ERC4626 standard. Making mint internal would prevent users from calling it, thereby restricting them from using basic ERC4626 functionality, instead of addressing the slippage issue. According to the EIP (https://eips.ethereum.org/EIPS/eip-4626), mint function should have an interface that does not include the additional minShares parameter, making option C non-compliant to the standard. The correct answer is, therefore, D, as implemented in the ERC4626Router (https://github.com/ERC4626-Alliance/ERC4626-Contracts/blob/main/README.md#erc4626router-and-base).","question-4-of-8#Question 4 of 8":"According to the standard, deposit(uint256 assets, address receiver) mints -Vault shares to receiver by depositing exactly assets of underlying tokens. Assume that the exchange rate between the underlying ERC20 asset and a Vault share, as returned by previewDeposit and convertToShares, is 2:1. How many Vault shares will the user get when calling deposit(64, msg.sender) in ERC4626Vault?\n A. 32 \n B. 64 \n C. 128 \n D. 160 \nCorrect is B.Palina's answer: Even though the asset and share should be trading 2:1, there is a typo, which causes _mint function in deposit to mint assets instead of shares. The user depositing 64 assets will therefore get 64 shares in return.","question-5-of-8#Question 5 of 8":"Which of the following invariants hold in the ERC4626Vault contract?\n A. The amount of user shares does not decrease unless the user calls withdraw or redeem\n B. Sum of all user shares is equal to the underlying token balance of the vault\n C. Number of shares in circulation equals the total of shares minted minus the total of shares burned\n D. Sum of all user shares is equal to the total supply of shares\nCorrect is C, D.Palina's answer: A is incorrect because the user can transfer shares to another account using the transfer function that is also present in ERC4626 contracts. Sum of all user shares is equal to the total number of shares minted, i.e., total supply of shares, so D is correct and B isn‚Äôt. The number of shares that is in circulation is the difference between the number of shares that got minted minus the number of shares that were redeemed.","question-6-of-8#Question 6 of 8":"Which statements are true about redeem and withdraw functions in the ERC4626Vault contract?\n A. redeem and withdraw differ in rounding direction\n B. withdraw sends the exact amount of assets to receiver\n C. withdraw and redeem only differ in visibility\n D. redeem burns the exact amount of shares from owner\nCorrect is B, D.Palina's answer: redeem and withdraw are supposed to differ in rounding direction, since redeem should round down and withdraw should round up, however, due to a bug in this contract, they both round down. withdraw does send the exact amount of assets to receiver and redeem burns the exact number of shares from owner according to both the standard and this implementation, hence, B and D are correct. withdraw and redeem don‚Äôt differ in visibility, but do differ in functionality, making C incorrect.","question-7-of-8#Question 7 of 8":"When there are no virtual shares and assets, inflation attacks on ERC4626 vaults are possible because of:\n A. Rounding up when converting shares to assets\n B. Rounding down when converting assets to shares\n C. Share tokens of the vault increasing in price\n D. Donation of assets to the vault to manipulate the exchange rate between assets and shares\nCorrect is B, D.Palina's answer: Inflation attack allows an exploiter to profit at the expense of other users. It is possible since the calculation of users' shares being minted are rounded down, so the attacker can dilute other users' shares by manipulating the exchange rate between assets and shares. One of the ways to skew the exchange rate in the attacker's favor is by donating (not depositing) a large amount of assets into the vault: this increases the total assets in the vault without affecting the amount of shares in circulation, leading to a much smaller amount of shares being minted for the next user. Source: https://blog.openzeppelin.com/a-novel-defense-against-erc4626-inflation-attacks","question-8-of-8#Question 8 of 8":"What are the issues with the underlying ERC20 token integration in the ERC4626Vault contract?\n A. If ERC4626Vault uses fewer than or the same number of decimals as the underlying ERC20 asset token, precision loss can occur during conversions between assets and shares\n B. In withdraw, transfer should be used instead of safeTransfer to transfer assets to the receiver\n C. Tokens taking fee on transfer might break accounting\n D. None of the above, there are no issues with the integration\nCorrect is A, C.Palina's answer: in the constructor, ERC4626Vault invokes parent‚Äôs ERC20 constructor providing 18 as the value of decimals, which allows for discrepancy between the underlying asset‚Äôs decimals and those of ERC4626Vault. Precision loss during conversions between assets and shares can occur due to rounding and is more pronounced if the Vault has fewer decimals than the underlying ERC20. safeTransfer is a safer alternative to transfer that checks the possible Boolean return value of a call, so it is used correctly in the contract. deposit and mint functions do not account for the possibility that assets might change a fee on transfer ‚Äî in this case, (assets - fee) would get deposited in the contract, while the shares calculation accounts for assets being deposited. A safer alternative is to use the actual difference between the contract‚Äôs balance before and after the transfer in the calculation of shares the user is entitled to."}},"/posts/2024/2/10/cryptocurrency-privacy-technologies-sigma":{"title":"Cryptocurrency Privacy Technologies: Sigma Protocol(s)","data":{"":"February 10, 2024 by patrickd\nDespite being regularly referred to as \"anonymous Internet money\", the ledgers of the most widely adopted cryptocurrencies are completely public. Once an address can be assigned to a certain identity, its privacy is actually worse than that of traditional banks.In the previous article, we explored the Zerocoin Protocol which made use of Strong RSA Accumulators for a global anonymity set combined with Zero-Knowledge Proofs to anonymously demonstrate membership within the set. Unfortunately, there was a cryptographic flaw within an undocumented part of the protocol, leading to many cryptocurrencies implementing it to be vulnerable to inflation attacks. This time we'll explore its successor on Zcoin (now Firo), the \"Sigma Protocol\" which did not suffer from its predecessor's flaw and also promised significantly reduced proof sizes and the elimination of a trusted setup.\nThe Zerocoin Scheme in review\nA user chooses a random Serial Number and Blinding Factor and creates a commitment that cannot be opened without knowledge of both values.\nThe user publishes a transaction that locks some amount of value (eg. 1 BTC) and contains the commitment. The chain will keep track of such commitments, effectively \"minting\" a Zerocoin.\nOther users too publish their own commitments and lock the same denomination of value.\nSometime later, the user publishes the Serial Number and proves in Zero-Knowledge that they know of a Blinding Factor that will open one of the commitments - without revealing which. They \"redeem\" their Zerocoin in exchange for one of the locked values.\nThe protocol also keeps track of already used Serial Numbers to prevent double-spending since it's not known which of the commitments have already been spent. The Blinding Factor must never leak, since with it anyone could reconstruct the commitment published during the mint-transaction. The scheme is anonymous thanks to the fact that minting and redemption transactions cannot be connected to each other, effectively \"mixing\" with all other protocol participants.","the-concept#The Concept":"In cryptography, the term \"Sigma Protocol\" (-Protocol) commonly refers to Proof-of-Knowledge techniques where a statement is proven by a Prover and Verifier communicating in three moves: Commitment, Challenge, and Response. Classifying these protocols is useful because they can be easily composed to prove conjunctions and disjunctions of multiple statements (ie. logical AND / OR conjectures). In regards to privacy, this is especially interesting since it allows the construction of k-out-of-n OR proofs (eg. for Ring Signatures), although for large anonymity sets such general techniques are still impractical since the proof's size grows linear with the number of members. More on this can be found in the Appendix.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nZcoin's (somewhat confusingly named) \"Sigma Protocol\" upgrade instead makes use of a specialized technique introduced by \"One-out-of-Many Proofs: Or How to Leak a Secret and Spend a Coin\" (OOOMP) which allows proving membership in large sets requiring only the transmission of a logarithmic number of commitments. Zcoin further improved this by implementing optimizations presented in \"Short Accountable Ring Signatures Based on DDH\".","scheme#Scheme":"At a high level, Sigma still follows Zerocoin's scheme: Private coins are minted by locking some denomination of public coins and publishing a Pedersen Commitment  which can only be opened with knowledge of a serial number  and a random blinding factor :\nAs you can tell by the notation, Sigma makes use of Elliptic Curve Cryptography. This required existing commitments to be re-minted by users after the chain upgrade went live.\nThese commitments are no longer aggregated by an RSA Accumulator for which inclusion is proven to redeem the private coin. Instead, we prove that we are able to open one out of all published commitments where the serial number is equal to zero. To have our  commit to zero, we reveal the serial number  to the validators which allows them to determine the inverse element . However, to maintain anonymity we may not reveal which of the commitments belongs to us, so the validators will add it to all published commitments :For our own , this will have the effect of homomorphically subtracting the serial number, turning it into a commitment to 0:To a verifier all  look like random points on the curve. There's no way for them to identify which one no longer commits to a serial number.","inclusion-proof#Inclusion Proof":"Zerocoin required 3 Zero-Knowledge Proofs to function:\nProof of the commitment's inclusion in the anonymity set without revealing the commitment\nProof of the ability to open a commitment without revealing the commitment or its Blinding Factor\nProof that both of the previous proofs are referring to the same undisclosed commitment\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nSigma achieves this within a single proof roughly by relying on both Prover and Verifier having the same ordered list of published commitments. In a sense, the proof is efficient by communicating the position of the commitment-to-zero within the list in Zero-Knowledge.Assume that within the list of  commitments  () the commitment with the Serial Number of 0  is located at the index  (). Let's say that, based on a seed provided by the Prover and the commitment's place within the list, all commitments are summed up with deterministic random factors  on the side of the Validator:The Prover creates a second sum with factors  that were chosen in such a manner that when subtracted from the first, everything will cancel out but for the commitment  that the Prover is able to open (has knowledge of  for).To prevent this from leaking  the Prover adds  to the sum, distorting the Blinding Factor. Knowing both  and  the Prover is still able to prove their ability to open the resulting commitment .The idea is that the Validator will be none the wiser about which of the commitments  the Prover was able to open since he'll only receive the already calculated sum and no information on the chosen  factors. While this was a gross oversimplification of the actual technique behind OOOMP, it should still have transported the concept at a high level.","the-math#The Math":"As you might have guessed, we assume the Elliptic Curve Discrete Log Problem (ECDLP) to be hard for a group of prime order  with generators  and  where the relationship  is unknown. It could be argued that the selection of these parameters may leave an opportunity to introduce trapdoors, similar to how a trusted setup could have allowed the system to be backdoored. However by using well-established elliptic curve parameters and the use of hash functions for selecting generators, this risk should be insignificant.","sigma-protocol-for-commitment-to-0-or-1#Sigma Protocol for commitment to 0 or 1":"The One-Out-Of-Many technique extends a -Protocol where multiple commitments  are proven to commit to messages  with a value of either 0 or 1.\nProver\tVerifier\tKnows \tKnows \tChooses random scalars \t\t\t\t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \t\t\t\t\t\t\tSends \tKnows \t\t\t\t\t\nVerifiers should enforce commitments , , and  to be valid points on the curve. Scalar values , , and  should be . The challenge  should be a binary value  where the length  is a security parameter.\nSubstitute \nSubstitute \nRemember that all statements can be verified in parallel within 3 moves with the same challenge , effectively resulting in a logical AND conjecture. Like all -Protocols, this one too may be transformed to be non-interactive using the Fiat-Shamir heuristic.","kronecker-delta#Kronecker Delta":"In what follows, a function called the \"Kronecker delta\" will be useful for conciseness. The function takes two parameters () and returns either  (when ) or  (when ). The parameters are usually written as indices of the greek letter  (delta):","one-out-of-many-proof-intuition#One Out Of Many Proof Intuition":"Assuming there are  commitments , where  is the index at which our coin-commitment  is located, the anonymity set known by both the Prover and Verifiers is:We want to construct a proof where, when each commitment is multiplied with a Kronecker delta , the sum results in a commitment to zero which we are able to open (thanks to our knowledge of ):As described, the Kronecker delta  will be 1 only when the current index  is equal to the index  of the commitment  for which we want to prove that we are able to open it.We'll then further break the indices down into their binary representations  and  where . Using this, we can substitute  with the product :\nThis technique assumes , which will rarely be exactly the case in practice. It's recommended to simply pad the commitments by reusing the last  until a valid length has been reached.Zcoin's OOOMP implementation erroneously omitted doing this until version v0.13.8.8 which could have allowed an attacker to generate a false proof.\nExample\nAssume , therefore  with  and the commitment  for which we want to prove set membership of at  (ie. ).\n‚Üì  /  ‚Üí\t1 ()\t2 ()\t3 ()\t0\t0\t0\t0\t1\t1\t0\t0\t2\t0\t1\t0\t3\t1\t1\t0\t4\t0\t0\t1\t = 5\t1\t0\t1\t6\t0\t1\t1\t7\t1\t1\t1\t\nAccording to the above (big-endian) binary table the values of  would be , , and :We'll now unroll the introduced equation using the example's assumptions:\nIf we engage in  parallel -protocols (described above) to demonstrate that all values  by making commitments  (with  and randomly chosen ), the Prover would reveal values of  in the formas part of the final move. Based on that we definewhich gives us for each  that the product  is a polynomial of the formwhere the polynomial's low order coefficients (corresponding to ) are  and can be determined before receiving the challenge value .\nExample\nContinuing based on the assumptions made in the previous example, we determine the polynomial  for each commitment :From these we can create a table of coefficients:\n‚Üì \t ()\t ()\t ()\t ()\t0\t0\t0\t\t\t1\t0\t\t\t\t2\t0\t0\t0\t\t3\t0\t0\t\t\t4\t0\t\t\t\t5\t\t\t\t\t6\t0\t0\t\t\t7\t0\t\t\t\t\nThe actual value for each coefficient can be calculated by information already known by the Prover () before a challenge value  was received. Note that these aren't individually transferred as part of the proof transcript, they're only locally computed by the Prover.Determining the coefficients for all commitments of a large anonymity set may seem extremely expensive, but there are ways to do these calculations in a quite efficient manner.\nThe idea is that the Prover would send commitments  that cancel out these low order coefficients while the high order coefficient  for  will guarantee that only our commitment to zero remains:\nExample\nContinuing with the previous example, we now determine commitments  where :We'll use everything so far to show that the lower order coefficients indeed end up cancelling out all commitments but  during the subtraction ofWe begin by expanding  based on the polynomials we already calculated in the previous example:Next we continue by expanding  making use of the commitments  determined above:Finally, we can see that both parts cancel each other out when adding them, except for :\nAs in the conceptual explanation,  is extended to distort the blinding factor using random values :With this intuition in mind, you should now be able to make sense of the proof's actual construction.","one-out-of-many-proof-construction#One Out Of Many Proof Construction":"A OOOMP demonstrates knowledge of an index  and a blinding factor  for a commitment  to zero contained within a known set of commitments  without revealing neither the commitment nor . In other words, we prove in Zero-Knowledge that we are able to open one of the commitments from a public list.\nProver\tVerifier\tKnows \tKnows \tChooses random scalars \t\t\t\t\t\t\t\t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \t\t\t\t\t\t\t\t\tSends \tKnows \t\t\t\t\t\t\t\nIt can be seen that, unless the challenge  leaked before  was committed, the Prover is required to have knowledge of  in order to construct a valid . And even before that, the Prover must have had knowledge of the coin's serial number  so that a valid commitment  exists that can be opened with  alone.\nSubstituting  and :We've seen in the OOOMP Intuition section that  and  cancel each other out, only leaving Substitute \nAssuming the Serial Number  has already been communicated to determine , the information required to be included in the proof transcript are  commitments and  scalars in the response. If we'd have ~ commitments in the current anonymity set, this means that we would have to add padding until we reach an . In this case,  would suffice to hold all commitments while allowing a proper binary representation. This would require the proof to include  commitments and  scalar values.While this may seem like a lot, this still offers a big improvement in comparison to the original Zerocoin protocol where an expensive double-discrete logarithm proof was necessary and very large multiplicative groups had to be chosen to ensure security, causing each individual commitment or field element in the transcript to be much bigger in comparison.","m-ary-optimization#M-Ary Optimization":"The authors of \"Short Accountable Ring Signatures Based on DDH\" introduced a few modifications to the protocol that further reduced the proof size mainly by pointing out that the proof system forms a binary tree of which one leaf is selected. Generalizing based on this observation, they make use of -ary trees (where each node has  children, instead of just ), allowing to fine-tune parameters for better performance. For , their optimization reduces the number of commitments from  to  with little impact on the number of scalar values or computational cost.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nPractically, they no longer use a binary () representation for indices  and  but  and  respectively. Furthermore, the Prover no longer commits to a single sequence of bits, but to  sequences of  bits where each 's sequence is in the form  (note that this is not binary either). Each of these sequences is proven to only contain a single bit of value . Finally, they use a Pedersen Commitment variant where multiple values are committed by introducing more generators .Note that  means accessing the -th bit of the -th sequence. Similar to before, , but for each sequence  we additionally want the sum of bits to be equal to 1 (). A single Pedersen Commitment  is used to commit to all  at once.\nExample\nLet's assume that  and , therefore . If the commitment  we are able to open is at position :Which means that we'd commit to 3 sequences of 4 bits for :\nThe described -Protocol then serves as the new basis for the optimized OOOMP:\nProver\tVerifier\tKnows \tKnows \tChooses random scalars \t\t\t\t\t\t\t\t\t\t\t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \t\t\t\t\t\t\t\t\tSends \tKnows \t\t\t\t\t\t\t\t\t\nFor simplicity we'll assume  and .Substitute :If :Not equal if for any sequence  there are multiple bits \nFor simplicity we'll assume  and .Substitute :If :Not equal if for any sequence  there are multiple bits  and if any .\nThe Sigma Upgrade reportedly reduced Zcoin's proof size down to ~1.5kb from ~25kb without the use of a trusted setup, providing a significant improvement for both efficiency and security. Although the protocol requires a large amount of computations, these can be further optimized using multi-exponentiation, pre-computation, and batching techniques without requiring any changes to the scheme itself.","the-code#The Code":"Let's take a look at the original C++ source code of Zcoin v0.14.0.5, which was the last release before the project rebranded to Firo and shortly before the activation of the Lelantus upgrade.","parameters#Parameters":"The code makes use of a secp256k1 library for its elliptic curve operations. To do so it offers GroupElement and Scalar classes for points and scalars respectively.\nUnless we're on testnet, where it makes use of a hardcoded point,  is created based on the hash of the secp256k1 curve's default generator. But more interesting than that are the M-Ary tree parameters (Note that in our explanation  and  were switched around for consistency).This means that the \"Many\" in One-Out-Of-Many-Proofs is limited to around 16 thousand commitments which matches the  anonymity set size that Sigma reportedly had. However, this shouldn't be confused with the set size that a project effectively has, which depends on the amount of users participating in the system.\nWe also find code generating  generator points . The first being based on the hash of , the following based on the hash of the previous point ().","minting#Minting":"When minting private coins, the user could choose from 7 denominations ().\nBeing based on Bitcoin, Zcoin has 8 decimals with the smallest possible value of one Satoshi () for public coins. But the smallest denomination of private coins is merely  causing users to end up with UTXOs of \"Tainted Change\" that cannot be transferred anonymously. Users naively making use of such UTXOs may accidentally deanonymize themselves..\nIn mintCoin() we can see how our coin's commitment  is created. Like with Zerocoin, the Serial Number  can't be chosen at random like  since this would allow for \"frontrunning\" attacks where an adversary could make our coin irredeemable . To prevent this,  is created from a public key and when spending the coin we'll prove ownership of its private key by signing.After publishing the commitment together with a transaction that locks an appropriate denomination of public coins, we may later spend the private coin by generating a One-Out-Of-Many-Proof.","spend-proof-generation#Spend: Proof Generation":"When intending to redeem a private coin, the spender needs to reveal a Serial Number  such that the inverse  can be calculated to homomorphically end up with a commitment to zero, which is what happens in the following code. While going through all of the commitments to apply this \"subtraction\", it also looks for the locked public coin that the spender has randomly selected for redemption.\nGeneration of the OOOMP starts with the creation of the commitment  which holds the bits representing the index  of the commitment  that we want to prove inclusion of. The code stores these  bits within a sigma table. The variable rB contains 's random blinding factor .\nThe paper introducing the M-Ary optimization referred to the -Protocol, proving that  and that each sequence only contains a single -bit, as relationship-proof . In the implementation, we find this part of the proof referred to as R1, which is modularized into a distinct section of the codebase that we won't dive deeper into here.Aside from an assertion requiring the anonymity set (commits) size to be non-zero, we find the initialization of  random factors used for the  commitments.Next is the computation of the polynomials' () coefficients .\nThe set's current size won't match the hardcoded  parameters, which means that padding it up to that size is necessary. The code does this in an optimized manner by calculating the sum of all padding-commitments and storing it together with the last polynomial's coefficients:\nThe proof generation finishes with the creation of  commitments (here Gk). To be non-interactive, the challenge value  is determined as part of the generation by hashing all group elements (ie. commitments ). Then the response is created with 's  and  (z) which completes the sigma protocol transcript: Commitments, Challenge, Response.","spend-proof-verification#Spend: Proof Verification":"The verification process is in many ways symmetrical to the proof's generation. The Serial Number is homomorphically subtracted from all commitments. The public key of the Serial Number is recovered, hashed, and compared to the actual Serial Number. After a few more sanity checks, the actual proof verification starts.\nFirst, the implementation ensures that all of the proof's commitments are valid field elements, that hashing them results in the same challenge, and that the response contains valid scalar values .\nIn a similar manner as the coefficients  were determined during generation, the values of  are calculated while making use of the same optimized padding technique as before.\nVerification finishes with calculations of t1 and t2 that should result in a left value that is a commitment to zero which we are able to open.\nWhen Zcoin was still using accumulators, the maximum number of commitments that it would allow adding to one was first restricted to only 10 coin commitments and later increased to 10k. This limit was defined within the zerocoin_params.h file and was put in place because forging an inclusion proof becomes easier with an increasing amount of accumulated commitments.\nStarting with the Sigma Upgrade, Zcoin stopped using accumulators but commitments were still separated into pools with different identifiers. The new limit \"per bucket\" was increased to 16k, but this time not for security reasons but because the complexity of verifying a One-Out-Of-Many Proof increases linearly with the number of commitments in the anonymity set. If this anonymity set were global and without limit instead, the effort to verify Zcoin transactions would grow linearly over time and increasingly slow down the entire blockchain.","appendix#Appendix":"","composing-sigma-protocols#Composing Sigma Protocols":"In the Zerocoin article, we introduced a simple -Protocol, the Schnorr technique, to prove knowledge of a verification key's preimage without revealing it. We also learned how the Fiat Shamir transform can be used to produce a Non-Interactive variant of the proof.Proving two statements in conjunction (logical AND) is a simple matter of executing two instances of the protocol in parallel:The Diffie-Hellman couple proof shows how such AND-conjectures can be coupled to not only prove two instances of the statement but also demonstrate that both instances are proving the same subject:Here, a Prover has knowledge of the preimage integer  and wants to demonstrate to a Validator that for two public points  and  that ,  hold true (ie. both public keys have the same private key) without revealing the secret scalar's value.\nProver\tVerifier\tKnows \tKnows \tChooses a random scalar \t\t\t\t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \t\t\tSends \tKnows \t\t\t\t\t\nSubstitute \nSubstitute \nAs long as the challenge value  remains unknown to the prover until he commits to  and , a correct value of  can only be constructed if the prover knows . If  and  were to contain different values for , at least one of the tests at the end would fail since the same  value containing a single  value is used for both.If the prover would somehow obtain knowledge of  early, he'd be able to determine values for the commitments  and  that will make the tests pass despite not having any knowledge about the secret scalar :The prover is able to \"cheat\" by using any random value for . And this is the key to obtaining OR-Conjectures for Sigma Protocols: Allowing the prover to cheat in some, but not all proofs.","1-out-of-2-or-conjecture#1 out of 2 OR-Conjecture":"For a simple 1 out of 2 disjunction (ie. of 2 statements, one must be correctly proven without cheating) we can make use of XOR (bitwise exclusive OR) to split the challenge value  into two sub-challenges  and , one for each of the statements being proven:With that, the prover is able to pick any random value for either  or , therefore having pre-knowledge of a challenge value and being able to use it to cheat in one of the proofs. Later, when the prover has received the actual challenge , he's able to calculate the other sub-challenge's value such that the above XOR statement will be true. This ensures that the validator can be certain that one of the proofs was correctly executed without obtaining knowledge about which one it was.\nProver\tVerifier\tKnows \tKnows \tChooses a random scalars \t\t\t\t\t\t\t\t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \t\t\t\t\tSends \tKnows","k-out-of-n-or-conjecture#k out of n OR-Conjecture":"Proofs of Partial Knowledge and Simplified Design of Witness Hiding Protocols shows how this same principle can be exploited to generally (for any -Protocol) and efficiently (linear complexity) achieve  out of  OR-Conjectures, where the validator can be assured that out of  overall proofs, at least  were indeed correct without the prover cheating.Their technique makes use of Shamir's Secret Sharing Scheme, which allows splitting a secret into  parts of which at least  are necessary to reconstruct it. To achieve this it makes use of the fact that any  points define a unique polynomial of -nth degree.To start, we'd create a polynomial where the coefficient  is the secret to be shared:\nWe're operating within finite fields of some prime size  where  (including the secret) and .\nThe other coefficients  are chosen randomly. From the resulting polynomial, we then choose  random points, which will each represent a part of the secret that can be shared.Using Lagrange interpolation a polynomial of the -nth degree can be efficiently and exactly reconstructed when one has at least  of these random points.Basically, the interpolation works by creating  individual curves where each one crosses through only one of the points. All of these curve's polynomials are then added up which will result in the original polynomial (and its coefficients) as long as each individual curve made sure that it crosses through 0 at the x-coordinate of all other points. Simplifying the resulting equation, we'll find the original secret once again as the  coefficient.For a  out of  OR-Conjecture we use a polynomial with  degrees of freedom, where  is the amount of statements that the Prover wants to cheat. The polynomial will be of degree , meaning that  points will be necessary to reconstruct it.The prover begins by generating  random values  and uses them as challenges to be able to create commitments for the statements he wants to cheat. For the other  statements, the prover generates honest commitments. After sending all of the commitments to the verifier he gets a challenge  as response. The prover then determines the unique polynomial  that passes through all points at coordinates  and , which makes  points. Next, the prover computes more points  where , so he basically calculates the -coordinate for each of the statements he wants to honestly prove and uses  as their challenge.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThe prover finishes the proof as usual with the response, sending all  and  values in some random order. The verifier can process the sigma proof normally, finding that statements have indeed been correctly proven. To ensure that the prover did not cheat more than allowed, the verifier checks that all of these challenge values are on a polynomial of degree .In this technique we didn't rely on the coefficients to be recovered, but rather the entire curve. This works thanks to the fact that we wouldn't have been able to select random points on the curve for the values  as long as we had not yet received the actual challenge value . This means that, if the prover is not able to prove  statements, he'd not be able to select more points for  values because then it would no longer be a polynomial of the degree that the verifier expects.","tech-tree#Tech-Tree":"Note that this Tech-Tree omits detailed dependencies that are not specific to the Sigma upgrade to maintain readability.","references#References":"Miers, I., Garman, C., Green, M. and Rubin, A.D., 2013, May. Zerocoin: Anonymous distributed e-cash from bitcoin. In 2013 IEEE Symposium on Security and Privacy (pp. 397-411). IEEE.\nGroth, J. and Kohlweiss, M., 2015, April. One-out-of-many proofs: Or how to leak a secret and spend a coin. In Annual International Conference on the Theory and Applications of Cryptographic Techniques (pp. 253-280). Berlin, Heidelberg: Springer Berlin Heidelberg.\nBootle, J., Cerulli, A., Chaidos, P., Ghadafi, E., Groth, J. and Petit, C., 2015, September. Short accountable ring signatures based on DDH. In European Symposium on Research in Computer Security (pp. 243-265). Cham: Springer International Publishing.\nPedersen, T.P., 1991, August. Non-interactive and information-theoretic secure verifiable secret sharing. In Annual international cryptology conference (pp. 129-140). Berlin, Heidelberg: Springer Berlin Heidelberg.\nMaxwell, G. and Poelstra, A., 2015. Borromean ring signatures. https://raw.githubusercontent.com/Blockstream/borromean_paper/master/borromean_draft_0.01_34241bb.pdf\nSchnorr, C.P., 1990. Efficient identification and signatures for smart cards. In Advances in Cryptology‚ÄîCRYPTO‚Äô89 Proceedings 9 (pp. 239-252). Springer New York.\nFiat, A. and Shamir, A., 1986, August. How to prove yourself: Practical solutions to identification and signature problems. In Conference on the theory and application of cryptographic techniques (pp. 186-194). Berlin, Heidelberg: Springer Berlin Heidelberg.\nReuben Yap, 2019, May. What is Sigma and why is it replacing Zerocoin in Zcoin?. https://firo.org/2019/03/20/what-is-sigma.html\nBayer, S. and Groth, J., 2013. Zero-knowledge argument for polynomial evaluation with application to blacklists. In Advances in Cryptology‚ÄìEUROCRYPT 2013: 32nd Annual International Conference on the Theory and Applications of Cryptographic Techniques, Athens, Greece, May 26-30, 2013. Proceedings 32 (pp. 646-663). Springer Berlin Heidelberg.\nReuben Yap, 2019, April. Lelantus: Firo's next gen privacy protocol. https://firo.org/2019/04/14/lelantus-firo.html\nRuffing, T., Thyagarajan, S.A., Ronge, V. and Schroder, D., 2018, June. (Short Paper) Burning Zerocoins for Fun and for Profit-A Cryptographic Denial-of-Spending Attack on the Zerocoin Protocol. In 2018 Crypto Valley Conference on Blockchain Technology (CVCBT) (pp. 116-119). IEEE.\nCramer, R., Damg√•rd, I. and Schoenmakers, B., 1994, August. Proofs of partial knowledge and simplified design of witness hiding protocols. In Annual International Cryptology Conference (pp. 174-187). Berlin, Heidelberg: Springer Berlin Heidelberg.\nShamir, A., 1979. How to share a secret. Communications of the ACM, 22(11), pp.612-613.\nBenny Pinkas, 2019, February. Sigma Protocols (part1). https://www.youtube.com/watch?v=XT1Pad0DM24\nThe Art of Code, 2022, January. Useful functions for game designers - Lagrange Interpolation. https://www.youtube.com/watch?v=4S6G-zenbFM"}},"/posts/2024/5/15/race-29-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #29 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a mirror of a Write-Up on RACE-29, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. It was designed by Secureum Mentor Dimitri Kamenski (aka kamensec), from Sigma Prime.The original version of this document can be found at https://twitter.com/kamensec/status/1786970628387389781Participants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!","code#Code":"","question-1-of-8#Question 1 of 8":"Contract complexity can increase the difficulty of line by line analysis. When dealing with specific, repeatable contract types (such as a lending platform), high level strategies help us focus on where low level tactical or technical issues might occur. Without looking at the contract, valid high level strategies for attacking lending contracts might include:\n A. Liquidation Parameter Manipulation: When liquidation configuration parameters are changed suddenly to cause users to be instantly liquidated \n B. Health Check Manipulation: When factors determining account health can be manipulated, to either show better health ratings and avoid liquidation, or worsen health and cause unexpected liquidation \n C. Interest Rate Calculation Errors: Avoid or increase interest payments on users with debt \n D. Insecure Collateral Accounting: Where collateral can be accessed by a user without repaying debt, or collateral representation does not accurately reflect deposited collateral \nCorrect Answers: A, B, C, D.\nThis was a bit of a warm up question... I was basically giving you 4 different strategies that would help you focus on different areas of the code.","question-2-of-8#Question 2 of 8":"What low level ‚Äòtactics or technical issues‚Äô might an attacker lean-on to compromise the borrow() function?\n A. Griefing or Denial-of-Service Attack \n B. Re-entrancy Attack from unsafe external call \n C. Invalid/inadequate or missing invariant checks \n D. None of the above \nCorrect Answers: C.Health checks should always be performed after any state changes, in the code example provided a borrower could pass health check then take debt that places them well beyond recoverable liquidation. This would lead to bad debt accrual for the protocol.","question-3-of-8#Question 3 of 8":"Considering the withdrawCollateral() function, what tactic/technical issue might an attacker use to cause loss of funds?\n A. Unsafe External Call: Return value of external calls in withdraw collateral isn‚Äôt validated, therefore transfer function may fail without triggering any reverts \n B. Incorrect/Inadequate/missing invariant: Protocol team can potentially cause interest calculations on an account with non-zero debt. While the user is attempting to repay all debt, interest is added to the debt. This could maintain debt, preventing users from completing withdrawal of collateral. \n C. Unsafe use of delete: Will cause collateral accounting to remain in the contract whilst collateral has been removed, allowing users to borrow funds on collateral that doesn‚Äôt exist \n D. Gas Grief: Large number of various collateral tokens might exceed gas limits during collateral withdrawal causing loss of user funds \nCorrect Answers: C.\nIt is important to understand how storage works in the EVM. Structs can often be tightly packed into a single word storage slot in the EVM. Mappings like arrays take more than 1 slot in storage. They take 1 slot for their 'marker slot' and all keys get placed separately in different slots depending on a hash functions.Maybe this sounds complex, but what needs to be understood is that the struct values, which could all fit in one word slot potentially, all get zeroed. But nothing prevents the mapping key/values which are stored separately to the struct itself from holding data.If we can re-initialise the struct with the same values inside it, the mapping will still point to key/value pairs that still exist inside of the contract storage.The way this can be weaponised is by ensuring that all collateral tokens are withdrawn, reinitialising the borrowers details, suddenly the mapping of collateralValues still contains values even though we previously deleted it. So you can end up in a situation where all collateral is withdrawn, but after creating another struct with the same borrower you suddenly have all that collateral back again.More details on that here: https://docs.alchemy.com/docs/smart-contract-storage-layout\nHow to fix this issue?\nI think ideally, we would want to avoid using a pattern of a struct with a mapping here, I think there's better data structures that could do the same thing and likely be more gas efficient as well. But assuming we had to maintain the same data structure (ie a struct with a mapping), we want to zero each key of the borrowers[msg.sender].collateralValues(){:solidity}. This is something that can only be done if you know what the keys are, which in our case are stored in the ?approvedCollaterals or the borrowers[msg.sender]{:solidity}.","question-4-of-8#Question 4 of 8":"Collateral deposit functionality and complexity comes in various forms in this contract. Viewing complexity as a sign of weakness may help focus on specifically vulnerable code branches. Which of the below best describes the impact of a security issue present in batchDepositCollateral()?\n A. Collaterals are not added to the approved collaterals for the users. Liquidations and health checks will not be able to access these collaterals added in batch deposits. \n B. Unsafe external calls will increase collateral values without ensuring actual collateral deposits match \n C. Excessive array sizes may cause out-of-gas reverts. This would prevent users from depositing collateral. \n D. Unchecked math will likely cause numeric overflow, which will prevent users from depositing collateral \nCorrect Answers: A.\nCollateral is not checked against an approved list which breaks logic elsewhere in the contract. The main thing I was showing here was that where complexities lay, so do issues, and you should compare against the base non-complex case to find the easy issues.\nBut isn't the collateral check happening in approvedCollaterals(collateralTokens[i])?\nYeah you are right here my wording is confusing and I've jumbled active and approved collaterals. What I meant to say was that collateral was not added to the borrowers[user].activeCollaterals{:solidity}, by not doing that, it breaks behaviour elsewhere for the deposited collateral and wont be included in health checks etc. In this contract its not the case, but maybe in others that could lead to funds not being withdrawable if the withdraw collateral functionality only dispensed what was considered active.","question-5-of-8#Question 5 of 8":"Another high level area to focus on in lending platforms is interest calculation. Which best describes the tactic/technical issue and relevant impact of a security issue present in the applyInterest calculation?\n A. Tactic/Technical Issue: Inadequate Access Control. Impact: Interest can be calculated and applied to accounts by anyone resulting unnecessary or unexpected debt accrual  \n B. Tactic/Technical Issue: Incorrect Constant Values. Impact: Due to incorrect constants used, interest rate accrual is calculated incorrectly. \n C. Tactic/Technical Issue: Using block and transaction properties maliciously. Impact: Due to unexpected inputs in block and transaction properties, interest can be zeroed. \n D. Tactic/Technical Issue: Rounding Issue, Impact: Due to Rounding issues present in the calculation of interest, it is possible to avoid interest on small debt amounts and accrue bad debt on the protocol. \nCorrect Answers: C, D.\nB: Shouldn‚Äôt be an issue, the interest rate is now calculated as 333/1e10 which produces 3.333e-8 = 1.05 / 31536000 which is roughly per second interest calculation.C: Debt calculations are increasing based on previous debt every second. There is no check if delta timestamp == 0. If so, the entire calculation is Zeroed and the user owes no debt at all.D: even if (C) is fixed there are still rounding issues where account.debt * interest * deltaTimestamp < RATE_PRECISION. Meaning small debt amounts could lead to 0 debt accrual. My opinion here is that the the debt calculation should be added separately to the old debt, instead of risking zeroing the whole calculation.","question-6-of-8#Question 6 of 8":"Liquidations are of critical importance to lending platforms, why?\n A. If users cannot be liquidated, they end up creating bad debt for the protocol team/lenders \n B. Users that are liquidated unfairly/unexpectedly are essentially rugged of all collateral \n C. It allows protocols to quickly generate liquidity in the form of stable coins \n D. None \nCorrect Answers: A, B.(No further explanation provided)","question-7-of-8#Question 7 of 8":"Focusing on the liquidate() function, what recommendations for improvement would you provide?\n A. Liquidations should only be executed by the protocol team \n B. Remove use of storage pointers  \n C. Ensure active collateral array size cannot exceed an amount where out-of-gas reverts could occur \n D. Fix invalid conditional statements to ensure users can be properly liquidated \nCorrect Answers: C, D.\nInterestingly, the if statement in this liquidation function isn't properly formated. Since it is missing the curly brackets if the statement is true (ie health check fails) the next code block executes (ie the for loop is executed then the account debt is zeroed). If it evaluates to false (healthy account), the for loop is skipped and account debt is zeroed.Additionally: Active collateral can be added arbitrarily by the user when individually depositing collateral, so it is possible to have gas reverts in this function if enough collateral tokens are added.","question-8-of-8#Question 8 of 8":"If we were to run test coverage of the code and possible tests provided, we would likely find that the healthCheck() function is hit many times. This becomes a ‚Äòcritical code branch‚Äô and if we can break this, we can potentially cause devastating issues. This section is a little bit mathematical, which might make it hard to see the issues unless you have seen them before. Looking at this function, what tactic/technical issues are present?\n A. Rounding Error: The protocol does not make sufficient protection against rounding errors. Rounding occurs against the protocol, leading to health checks passing, when they should be failing. \n B. Decimal Scaling Error: Assuming the oracle aggregates different Chainlink price feeds through latestRoundData() with no additional logic and presents the relevant feed pricing, then quote prices with larger than 18 decimals may end up passing health checks, when they are supposed to fail \n C. Decimal Scaling Error: Quote prices with less than 18 decimals may end up passing health checks, when they are supposed to fail \n D. Rounding Error: Rounding occurs in favour of the protocol, leading to health checks failing, when they should be passing \nCorrect Answers: B, D.\nThe question itself is hinting at critical strategic areas I would normally look at when looking at lending protocols. Specifically I would look at the 'healthCheck' function with huge scrutiny. However a few more additional tactical issues to look out for are rounding issues and decimal scaling. These are two issues I didn't truly appreciate until later in my career and can have  nasty implications on protocol security.The contract snippet assumes 18 decimal places, for collateral with 10^x extra decimals in price aggregator feeds, the scaledPrice will be orders of 10^x more resulting in a larger cumulative collateral score which assists users in passing health checksRounding down occurs when quote prices are returned that are based on less than 18 decimals. When scaled they can potentially round down, in situations where they are healthy.\nMore resources on 'Liquidation Math'?\nI think I'd avoid the idea of 'liquidation math' that seems too specific to be of any real benefit to you as an auditor. Liquidations themselves are going to be different in different situations. Pattern recognition will help you more. The high level strategies I was referring to was focusing on the liquidation / health check functionality, now you can focus line by line in specific areas. Low level tactics which you will look for during a line by line analysis will build up over time, but two really important ones I gave you here to consider were decimal scaling and rounding errors. Those are extremely important and honestly produce crazy complex issues.Slow down when you get to division, consider rounding errors that are possible, then consider how impactful and significant those rounding errors could be. This is a very vague not necessarily related example but.... In some cases you can withdraw less than 1 share in a ERC4626 vault and get a tiny number of assets back, thats kind of useless and not scalable as an attack vector. In other cases you can deposit tiny amounts of assets and receive a whole share. But try to think harder about the impact from here... if the share value is extremely high (as in can be exchanged for some large number of actual valuable assets), that means the rounding is in your favour and profitable.","conclusion#Conclusion":"My method of auditing involves splitting things between contract specific high level strategies and low level tactics. High level strategies allow me to target things like lending, governance, vaults, staking and other types of protocols more efficiently by directing me to parts of the code that will be more likely to have critical issues.In the end of the day almost everything will come down to low level tactical issues. These include things like integer overflows, out-of-gas reverts, re-entrancy, signature malleability, malformed opcode parameters, this list would go on for days....Especially at the beginning of your career as an auditor line by line analysis can easily have you lost when looking at super complex code. Acquiring high level strategies can give you a guide to certain contract types, which will help you discover more lower level tactical issues that lead to better findings.Hope you learned something from the process on my audit workflow, crush ur next lending protocol for me!@kaminsec"}},"/posts/2024/3/18/cryptocurrency-privacy-technologies-bulletproof-range-proofs":{"title":"Cryptocurrency Privacy Technologies: Bulletproof Range Proofs","data":{"":"March 18, 2024 by patrickd\nIn this article, we explore how \"Bulletproofs\" can achieve short and efficient Zero-Knowledge Range Proofs of values within blinded commitments. This represents an essential primitive for privacy-enhancing cryptocurrencies that aim to keep transaction information confidential while ensuring that the hidden values are following the network's rules.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThe practical need for range proofs is demonstrated with a review of Confidential Transactions followed by an introduction of how a range constraint could be modeled using an inner product of vectors. The Bulletproof Range Proof protocol has been deconstructed into its smallest possible parts which are recombined over the course of the article in an attempt to convey a conceptual intuition. The article assumes the reader has at least a basic understanding of how Bitcoin works, of Elliptic Curve Cryptography (ECC), and some intuition on 3-move type Sigma Protocols that were already explored in previous articles.","introduction#Introduction":"","confidential-transactions-in-review#Confidential Transactions in Review":"The Confidential Transactions Scheme aims to hide transaction amounts within Pedersen Commitments which can be homomorphically summed and compared: Unspent Transaction Outputs (UTXOs) normally hold the value they carry as a plain text integer making it a simple task to ensure that a transaction's outputs do not have larger amounts of coins than its inputs. The scheme instead uses ECC to replace the plaintext amount with  as a scalar value for the generator point  and blinds it with a random blinding factor  of another generator point  creating a commitment .\nA blinding factor  is required to ensure that the hidden value can not simply be discovered by brute force. Since coin amounts tend to be simple and small numbers, it would be easy for an attacker to guess  until the  same commitment  of the user is found. To prevent this, a large random number  is used to make such attacks impractical. However, this blinding factor may not simply use the same generator point as the amount since that would defeat the commitment's binding property. To prevent the commitment creator from claiming a different  and  at a later point, a second generator where the logarithmic relationship to the first one is unknown must be used. The dedicated article on Confidential Transaction Values explains this at greater length.\nThanks to the homomorphic properties of such commitments, they can be summed upand their sums can be comparedsuch that when they are equal, the verifier can be assured that the sums of amounts are equal too:\nIndeed, this also requires the blinding factors of both sums to be equal and you might be wondering how that's possible when they're supposed to be random. The trick is that the last factor, in the above case , is chosen specifically to ensure equality, instead of being random. Done appropriately, this does not affect the commitment's security.","the-need-for-range-proofs#The Need for Range Proofs":"The issue with this scheme is that it falls apart once negative amounts are at play: What if Alice has a UTXO with  coins and makes a transaction where she spends this output while creating two new UTXOs. But one of these has a blinded amount of  and the other a positive amount of . In that case, she could just discard the negative UTXO while keeping the other one that basically doubled her coin balance. To a Verifier this would look perfectly fine since, indeed .Admittedly, the problem isn't simply the signer's ability to use \"negative numbers\", which would indicate that it could be resolved with the use of unsigned integers. The real issue is that we are dealing with scalar values that can have an effect just like negative numbers because they're part of a field . The constant  is the group order shared by both generators  and  at which a scalar wraps back to :Most cryptocurrencies make use of the  curve which has a group order ofwhich in case of Bitcoin and most of its forks is an enormous number considering that the maximum amount we could ever deal with is 21 million coins:So in order to prevent the \"negative numbers\" issue we could restrict the amount  to the maximum  and the number  of UTXO outputs per transaction such that  is always true. With that, the sum of output amounts should never be able to wrap.\nBlinding factors  are intentionally large numbers and will likely wrap around multiple times, but this does not matter since all we need them to do is making it hard to unblind the actual amount and for their sums to be equal when necessary.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nWith the number of UTXO outputs being public, restricting   is trivial. Restricting the coin amount  without revealing it, on the other hand, is a surprisingly difficult task. The solution proposed in the original Confidential Transactions scheme made use of Borromean Ring Signatures to achieve it, a form of Zero-Knowledge Proof which comes at the great disadvantage of linear growth in proof size. Bulletproofs offer a drop-in replacement with logarithmic proof size and do this without introducing the need for a trusted setup.","range-constraints-via-inner-products#Range Constraints via Inner Products":"Bulletproofs are a type of \"Inner Product Argument\" which can be used to prove arbitrary \"Arithmetic Circuits\". More specifically, it enables us to make Proofs of Knowledge on statements that we can get into the form of an Inner Product () on two vectors  and  of length :Practically, a Prover can commit to  and  and show that . We can use this to build a Range Proof by converting the transaction amount  into its binary representation where each vector element  is a bit for an exponential  and the vector length  determines the range boundary :The resulting Inner Product, and therefore the transaction value , cannot possibly be larger than  with these two vectors, therefore enforcing the range.\nExample\nWe assume the Verifier demands the transaction amount  to be within the range  with the vectors fixed at length , which means anything from  to  is a valid value.Both Prover and Verifier know that  is static with elements . So the Prover has to determine  for their commitment  to the amount :Let's say that the Prover sends commitments A, B, V for , and  respectively to start an interactive proof demonstrating . Then, if , the Verifier can be assured that the hidden amount is within the required range.\nWhile we now know the inputs with which we'd feed such a protocol, we have yet to see what's going on within the black box of a Bulletproof Range Proof. How are we enforcing that ? How do Inner Product Arguments work in the first place? How do they manage to have logarithmic proof size complexity?","bulletproofs#Bulletproofs":"","zero-knowledge-inner-product-argument#Zero-Knowledge Inner Product Argument":"Starting from the familiar place of 3-move type Zero-Knowledge Protocols, let's take a look at a proof demonstrating that  while keeping , , and  secret. As usual, hiding these values is achieved using blinded commitments:Where  and  are vectors of generators ,  with unknown logarithmic relationship. Assuming a simple case of two-dimensional vectors () we can expand this to:This alone isn't enough to achieve Zero-Knowledge though, we further need random vectors  and , chosen and committed by the prover as:Specifically, the protocol has the following structure:\nProver\tVerifier\tKnows \tKnows \tComputes \tComputes \tChooses random \t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \tComputes \tComputes \tComputes \tSends \tKnows \t\tComputes \t\t\t\t\t\nShowing correctness of the protocol only requires some substitutions, expansions, and rearrangements:\nExpanding :\nBy remembering that  it becomes obvious that both sides are the same. The equation holds.\nAfter substitution only a few more rearrangements are necessary for both sides to be the same.\nTo more fundamentally understand how this proof works, consider vector polynomials of the form  where each coefficient is a vector . When computing the inner product of two such vector polynomials (), we can do this by either first evaluating each polynomial for a specific  (ie. ) and compute the inner product of the results . Or we can determine the resulting vector polynomial of the inner product  and come to the same final result when evaluating  for a specific .\nThe bulletproofs paper actually defined the above equation as  that will result in lacking  which is required for the proof's correctness.\nIn the case of the above proof specifically, we can notice that  and  are the result of evaluating such polynomials with the challenge provided by the Verifier.Based on these, we can determine the inner product polynomial :As you might notice, we committed to  with , and to , with  and  respectively.In other words, the proof works by first having the Prover commit to the vector polynomial  via its coefficients. After receiving the Validator's challenge , we respond with the result of evaluating , . The Validator can then check whether the inner product of these () matches with what is produced by evaluating  using the commitments , , and .","recursive-inner-product-argument#Recursive Inner Product Argument":"While the above proof allows us to demonstrate  in Zero-Knowledge, by proving  instead, it still requires sending the vectors  and  of length  which results in linear communication complexity. The solution to achieving logarithmic proof sizes is the \"Folding Argument\" presented in \"Efficient zero-knowledge arguments for arithmetic circuits in the discrete log setting\" by Bootle et al, which was further improved by the Bulletproofs paper.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nWe once again start with a 3-move type protocol that is both provably correct and sound but is not Zero-Knowledge. While that means that information on the input parameters can be extracted from the transcript, this does no harm thanks to the fact that we are merely using it as a sub-routine in the above Zero-Knowledge proof where the inputs () are already blinded.For simplicity we assume that the two vectors  and  merely have a length of . The goal is to demonstrate that  using the commitment:\nProver\tVerifier\tKnows \tKnows \t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \tComputes \tSends \tKnows \t\t\t\t\t\nWhat you should notice here is the fact that we started from 2-dimensional vectors  and  which the prover then turned into 1-dimensional values ,  respectively. Furthermore, when generalizing this proof to work with vectors of arbitrary size you can observe that the resulting vectors ,  will always be half the size of the starting vectors , :\nProver\tVerifier\tKnows \tKnows \t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \tComputes \tSends \tKnows \t\t\t\t\t\t\t\t\t\nNote that the generalization splits vectors into two halves, a left () and a right () one. For example, in the 4-dimensional case where  and  the following applies:\nHere, you may notice that  was rearranged in such a manner that the generator vectors () too were halved in size. This leaves us with a commitment  that looks very similar to our starting point:In other words, in order to proof  we're sending vectors  and  for which the validator calculates . This is similar to how we could have convinced the verifier of  by simply sending the full vectors  and . And that's where the trick is: As long as the vectors' ,  dimension is  we don't send them but instead execute the same proof recursively. Each time the vectors that would need to be proven in the final validation are halved in size and we simply stop once we've reached the point where they have a single dimension.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nLet's look at how recursion works in action by starting from 4-dimensional vectors for the inputs  and :\nProver\tVerifier\tKnows \tKnows \t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \t\tComputes \t\t\t\tComputes \tComputes \t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \tComputes \tSends \tKnows \t\t\t\t\t\t\t\t\t\nUnder the assumption that we're starting from vectors with exponential size () this means that every time we double , the communication complexity of this protocol only increases by a factor of 1 (ie. logarithmic).","range-constraint-circuit#Range Constraint Circuit":"Combining everything so far, we achieved Zero-Knowledge Proofs for Inner Products with logarithmic efficiency. Unfortunately, that doesn't mean we can simply plug the bit-vector  and exponentials vector  into them and be done with it. After all, there's nothing forcing the Prover to actually use vectors of this format and due to Zero-Knowledge, there's no way for the Verifier to check them as things stand.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nTo fix this, we first need to determine the constraints that need to hold for the Verifier to be assured of the input's validity. Next, we have to combine these conditions back into a form provable by an Inner Product Argument. We can start with a condition that requires our input vector  to be multiplied with a constant vector:To ensure that  we can compute a second vector by subtracting 1 for each dimension ( where ). Multiplying each entry of these two vectors must always result in a vector of zeros, adding the last two conditions we need:\nExample\nContinuing with the assumption that  with , we can calculate :Then :\nWe can join all these into a single statement using the random linear combination technique that you'll likely recognize from the 3-move type protocols we've already seen. In short: It's adding up everything while preventing the Prover from cheating. Let's look at an extremely simple example to get an intuition for it.If the statements we want to combine are , the Prover makes the first move by committing to them. The Verifier sends a random challenge value  to the Prover which he uses to combine everything as . Without the challenge (ie. ), the Prover could simply have committed to values for  and  that result in  when combined and break the condition that both should have been equal to zero. With the challenge, the Prover will be unable to pick  and  in a manner that would have them cancel out since he doesn't know what  will be.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nWhen involving vectors, a condition like  can also be seen as  conditions that we could combine into a single statement with linear combination. Coincidentally, such a statement is a sum, just like an inner product is a sum too. As such, we can exploit linear combinations of vectors to end up with an inner product statement:\n\t\t\t\t\t\t\t\t\t\t\t\t\nThe same principle can be applied after rearranging  to :\n\t\t\t\t\t\t\t\t\t\t\t\t\nThis will still leave us with 3 separate conditions, though each is in the form of an inner product now:Once again, we can apply the random linear combination technique to combine them all into a single statement:Finally, this statement needs to be rearranged back into the form of a single inner product. But we need to do this in a way where each side of the inner product only has one of the secret inputs (). All the other terms () should be non-secret values that the verifier is able to compute on their own. The algebra necessary for this is a bit tedious (click to expand and see it), but the end result is this:\nTo rearrange the statement into a single inner product, we start with taking note of the fact that a complex inner product like  can be split into two simpler ones \nTherefore,  can be rewritten asResulting inof which we can move over  to the other side of the equation:We're able to use the same technique that was applied to split the inner product apart, for recombining simpler inner product terms into a complex one. But for that, we first have to move the  factors into the inner product, which can be done simply by adding it to either side of the inner product.\nWith that, we can rewrite the statement aswhere all except the third inner product can now be combined into a single inner product since they all share  on the left side:To merge the last remaining inner product, we want to have its right side match with the other inner products right side. We'll do this by taking note of the fact that  is the same as .\nResulting inwhich allows us to add the missing terms by adding  to both sides of the equation:For readability, we'll shorten the right side of the equation with . Then we rearrange by combining until we end up with the desired single inner product argument on the left side of the equation:That only leaves us to deal withwhich can be simplified with the same techniques as above:","adjusting-the-zero-knowledge-proof#Adjusting the Zero Knowledge proof":"We've managed to bring the statement into the format of an inner product where the Prover begins by creating a bit-vector  such that . From this, the Prover next determines the second vector  by computing . The question now is: How do we adjust the Zero-Knowledge Protocol to accept these vectors as input under the constraints we've found?\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nLooking back you should notice how the vector polynomials  and  were basically random linear combinations of the input vectors  and  with the random blinding vectors  and  respectively.We'll \"wrap\" the constraints right around these, replacing them as our source for  and :Similar to before,  will be the polynomial resulting from the inner product of both functions:What changes though, is that  won't simply be the value of  but rather , which requires adjusting the equations that the Validator verifies appropriately. The values for ,  can still be computed from the coefficients of  and :Specifically, the Zero-Knowledge protocol proving  for a commitment  has the following structure with all necessary changes applied:\nProver\tVerifier\tKnows \tKnows \tComputes vector \tComputes vector \tChooses random vectors \tChooses random factors \t\t\tSends \tKnows \t\tChooses a random challenge scalars \tKnows \t Sends \tComputes \tComputes \tComputes \tComputes \tChooses random factors \t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \tComputes \tComputes \tComputes \tSends \tKnows \t\tComputes \t\t\t\tComputes \t\t\t\nFor  we can see that the equation's  are indeed correct:\nExpanding :\nLooking at  is a bit more interesting since we're introducing a new generators vector\nIf  the equality would not be able to hold.\nThe reason for introducing  with  instead of applying it in the equation directly is that this generators vector will be used as part of the sub-routine that makes the protocol logarithmic.","bringing-it-all-together#Bringing it all together":"Let's bring everything we've discussed together in a full Zero-Knowledge Range Proof protocol with logarithmic communication efficiency. As before we'll start from a secret value  that we've already committed to with  for which we want to prove that  (ie. ).\nProver\tVerifier\tKnows \tKnows \tComputes vector \tComputes vector \tChooses random vectors \tChooses random factors \t\t\tSends \tKnows \t\tChooses a random challenge scalars \tKnows \t Sends \tComputes \tComputes \tComputes \tComputes \tChooses random factors \t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \tComputes \tComputes \tComputes \tComputes \tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \tComputes \tComputes \tComputes \t\t\t\t\t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \t\tComputes \t\t\t\tComputes \tComputes \t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \tComputes \tSends \tKnows \t\t\t\t\t\t\t\t\t\t\t\nAs shown by the , the vectors  are no longer transferred to prevent the transcript from being linear in size to . With that, the prover can no longer compute the inner product () of them, so instead it's computed by the Prover and committed to with a Verifier chosen factor . While the Prover is able to create  with knowledge of  and , the Verifier can reproduce it while enforcing\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nby computing the left hand side as  from the already available information. Assuming equality, we can then use the right hand side to produce :Therefore  is already implicitly guaranteed by the success of the logarithmic inner product protocol via .","conclusion#Conclusion":"Range Proofs built with Bulletproofs require sending  group elements  and  field elements (). This is really impressive compared to linear solutions, but still quite inefficient compared to schemes such as SNARKs. But those require a trusted setup, a property that is generally undesirable for use in cryptocurrencies.It should also be noted that verifying multiple Bulletproofs at once can be done in a very efficient manner. Not only can multiple separate proofs be efficiently batch-verified, they can also be aggregated (combined into a single proof). An aggregated proof containing  range proofs only requires an additional  group elements over a single range proof.","appendix#Appendix":"","tech-tree#Tech-Tree":"Note that this Tech-Tree omits detailed dependencies to maintain readability.","references#References":"B√ºnz, B., Bootle, J., Boneh, D., Poelstra, A., Wuille, P. and Maxwell, G., 2018, May. Bulletproofs: Short proofs for confidential transactions and more. In 2018 IEEE symposium on security and privacy (SP) (pp. 315-334). IEEE.\nGreg Maxwell, 2016. Confidential transactions. https://people.xiph.org/~greg/confidential_values.txt\nMaxwell, G. and Poelstra, A., 2015. Borromean ring signatures. Accessed: Jun, 8, p.2019.\nBootle, J., Cerulli, A., Chaidos, P., Groth, J. and Petit, C., 2016. Efficient zero-knowledge arguments for arithmetic circuits in the discrete log setting. In Advances in Cryptology‚ÄìEUROCRYPT 2016: 35th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Vienna, Austria, May 8-12, 2016, Proceedings, Part II 35 (pp. 327-357). Springer Berlin Heidelberg.\ndalek cryptography, Cathie Yun, 2020. https://doc-internal.dalek.rs/bulletproofs/\nBenedikt B√ºnz (Stanford University), May, 2018. Bulletproofs: Short Proofs for Confidential Transactions and More. IEEE Symposium on Security and Privacy YouTube channel. https://www.youtube.com/watch?v=Adrh6BCc_Ao\nCathie Yun, June, 2018. Cathie Yun on Bulletproofs: Short Proofs for Confidential Transactions and More [PWL SF] 3/2018. PapersWeLove San Francisco Chapter YouTube channel. https://www.youtube.com/watch?v=BBe1JzUxSB8\nBenedikt B√ºnz (https://crypto.stanford.edu/~buenz/), April, 2018. Benedikt B√ºnz : Bulletproofs. SF Bitcoin Developers YouTube Channel. https://www.youtube.com/watch?v=gMI8dkwGGcw\nHoffmann, M., Kloo√ü, M. and Rupp, A., 2019, November. Efficient zero-knowledge arguments in the discrete log setting, revisited. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (pp. 2093-2110).\nMary Maller (Ethereum Foundation), May, 2021. Inner Product Arguments - Mary Maller. ZKProof Standards YouTube Channel. https://www.youtube.com/watch?v=dD_0Vn4BhmI\nMary Maller (Ethereum Foundation), August, 2020. Mary Maller- \"Inner Product Arguments\". For IC3 Blockchain Camp 2020 on IC3 Initiative for Cryptocurrencies and Contracts YouTube channel. https://www.youtube.com/watch?v=Ne3XpSdMmzM"}},"/posts/2024/7/30/race-31-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #31 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-31, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. This month's RACE was designed by phaze, also known as Kurt, who is a blockchain technology specialist with a strong background in mathematics and recently joined the ranks of Mentors at Secureum.\nBefore starting, note that this RACE requires some basic experience with ZK, Noir, or Rust:\nThe Rust syntax in the RACE is really minimal and is a known Solidity algorithm. There are no issues related to Rust/Noir syntax. As long as you can read the code as pseudocode (and it's quite simple) you're good to go. It is helpful to have in mind some properties of ZK proofs in a similar manner as most people know how signatures are used and handled. Signatures can prove that the private key holder has signed a known message without revealing the private key. With ZK, all you need to know is that the proof verifies that all the inputs (public and private) satisfy the constraints in the program and that public inputs are passed in (publicly) along with the proof and that private inputs are kept private (like the private key of signatures).It might be a little harder to grasp, since it is unfamiliar territory, but I hope that otherwise minimal pre-knowledge is required.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nJuly 30, 2024 by patrickd","code#Code":"All 8 questions in this RACE are based on the below contracts.","question-1-of-8#Question 1 of 8":"Merkle tree: Which of the following statements is true?\n A. A Merkle proof requires the leaf, its key and DEPTH inner nodes in order to re-compute the root. \n B. The Merkle tree is constructed using the keccak256 hash function. \n C. The key parameter in the Merkle proof indicates the path chosen from the leaf to the root. \n D. The Merkle path indicating the key b'0110's position from root to leaf is: left, right, right, left \nCorrect is A, C, D.\nA) True: Evident from the compute_merkle_root function parameters\nB) False: The hashing function used is Poseidon.\nC) True: The hashing in compute_merkle_root starts from the leaf node and ends at the root.\nD) True: The key is intentionally \"symmetric\" to make the question easier. One can imagine the leaf node '0000' starting at the left most position.","question-2-of-8#Question 2 of 8":"Deposits: Which of the following statements is true?\n A. New deposits increase the size of the Merkle tree. \n B. The maximum number of deposits is 2^DEPTH - 1. \n C. The deposit function correctly ensures that any deposit can always be retrieved. \n D. The check for whether the leaf was previously committed if (leafCommitted[leaf]) prevents double withdrawals. \n E. None \nCorrect is E.\nA) False: The merkle tree never grows/shrinks and is always kept a fixed size (seen by the constant DEPTH parameter).\nB) False: Tricky one, it's 2^DEPTH deposits.\nC) False: One can easily verify the requirements of the deposit function. There is no requirement on the leaf parameter. This can be arbitrary and a pre-image to the leaf parameter might not be known.\nThis one is tricky too, because deposit is doing everything it can to ensure that deposits can be retrieved. Only the question's \"always\" is the hint that raises the question: \"but what if the depositor messes it up?\"\nD) False: This is a preventative check against double commits/deposits. But it is not required for the protocol's security.\nDouble withdrawals are prevented by the nullifier. So when a nullifier was already used, the leaf would no longer be redeemable, which would be problematic if it was used for a deposit more than once.","question-3-of-8#Question 3 of 8":"ZK/Noir: Which of the following statements is true?\n A. A zero knowledge proof can prove the inclusion of a leaf in a Merkle root without revealing the actual leaf, secret, key or proof nodes. \n B. The root parameter must be a public input (part of the calldata), since it must be compared to the Merkle root recorded on-chain. \n C. The nullifier parameter is able to mark a deposit (leaf) as spent without revealing it, since the nullifier and leaf are inextricably linked to each other through the depositor's secret parameter. \n D. The leaf is not part of the zero knowledge proof parameters, since it can be computed from the nullifier. \nCorrect is A, B, C.\nA) True: This is visible in the main Noir component.\nB) True: Explanation is in the statement: It must be verified against the on-chain root. Otherwise anyone could make up their own root containing multiple malicious commitments.\nC) True: Visible from the code: nullifierSpent mapping, leaf = hash(secret + 1), nullifier = hash(secret + 2)\nD) False: The leaf cannot be computed from the nullifier. It can only be computed from the secret.","question-4-of-8#Question 4 of 8":"Withdrawal proofs: Which of the following statements is true?\n A. A withdrawal can be made with an arbitrary nullifier value and doesn't require a previous deposit. \n B. The nullifier parameter must equal hash(secret + 2), where leaf == hash(secret + 1). \n C. A correct withdrawal function requires additional parameters for a Merkle proof. \n D. The withdraw function is missing a check for valid deposits as it is not checking whether the leaf was committed. \nCorrect is B.\nA) False: Withdrawals require valid merkle tree inclusion proofs, where the nullifier is checked to be linked to the leaf.\nB) True: Evident from the code.\nC) False: The Merkle proof is contained inside of the ZK proof and doesn't need to be provided. Providing the Merkle proof publicly would immediately reveal the depositor.\nD) False: Again, we don't check the withdrawer's leaf, because this would reveal their deposit. The valid deposit is guaranteed by the ZK proof and the root check.","question-5-of-8#Question 5 of 8":"Withdrawal vulnerabilities: Which of the following statements is true?\n A. An attacker can front-run a call to withdraw and swap out the receiver for their own address in order to receive the deposit. \n B. An attacker can drain the protocol by re-entering the withdraw function. \n C. Due to a vulnerability, an attacker can call withdraw with a proof to a root that is not any of the previously recorded roots. \n D. The withdraw function does not delete the original leaf entry from the Merkle tree, therefore multiple withdrawals can be made with the same deposit. \nCorrect is C.\nA) False: Receiver is part of the ZK proof parameters. Changing this would require creating a new proof.\nThis can be seen at the line that ties \"receiver into proof\".\nB) False: Although the code is misleading since it does not follow the CEI pattern, the check and the effect for whether the nullifier was spent is made in succession.\nSpecifically the nullifier check at line 101 would need to happen before the external call for this to be exploitable.\nC) True: The check if (!validRoot[root]) revert InvalidRoot(); should actually check the withdrawer's root_ parameter as described in the comment above the line.\nD) False: This Merkle tree is fixed size and append-only. Multiple withdrawals are prevented by check whether the nullifier was spent.","question-6-of-8#Question 6 of 8":"Privacy: Which of the following statements is true?\n A. Knowing the receiver parameter also means knowing the depositor. \n B. It's impossible to trace a withdrawal back to a deposit only by knowing the nullifier value. \n C. It's always possible to link a withdrawal to a deposit by looking at a user's submitted root argument. \n D. Users should not withdraw using the same Merkle tree root they deposited with. \nCorrect is B, D.\nA) False: The main idea of the protocol is to \"unlink\" the depositor and withdrawer/receiver.\nB) True: This would require knowing the pre-image (secret) of the Poseidon hash.\nThe nullifier value is only revealed during withdrawal, while during deposit only the leaf is revealed. To connect these two the secret is necessary, which is never revealed.\nC) False: This would be a big issue in the protocol if this were the case.\nD) True: Since the Merkle root is updated at each deposit, knowing the depositor's root would reveal their leaf.","question-7-of-8#Question 7 of 8":"Malleability: Which of the following statements is true?\n A. Depositors can always generate new valid zero knowledge proofs for new Merkle roots. \n B. Since not all nodes of the proof nodes parameter are read, the proof is malleable. This allows multiple different valid proofs to be generated from different private inputs. \n C. It is possible to withdraw multiple times from the same deposit due to malleability in the nullifier parameter as part of the publicInputs in the verifyProof function. \n D. None of the above. \nCorrect is A, B, C.\nA) True: If this were not the case, then depositors could not withdraw after new deposits are added.\nB) True: The proof is malleable (in a few ways I believe). Although, like with verifying signatures, we don't care about malleability, but only that it is valid.\nC) True: This is another vulnerability. The modulus operation should not be applied here. This allows multiple pre-images (multiple nullifiers) to the public input. publicInputs[1] = bytes32(nullifier % PRIME_FIELD_ORDER);\nRather than modulo the value, it should revert when it's larger than the prime field order as the hash function does.","question-8-of-8#Question 8 of 8":"Protocol: Which of the following statements is true?\n A. The deposit contract does not initialize and update the root and proofNodes variables correctly leading to invalid root values. This blocks earlier depositors from withdrawing. \n B.  Keeping track of proofNodes in the contract would not be necessary if they were provided by the user and then the re-computed root was verified against the on-chain root. \n C.  The protocol does not strictly require the validRoot mapping in order to function, although this could make it vulnerable to DoS attacks. \n D. None of the above. \nCorrect is B, C.\nA) False: The initialization does not matter and the updates are performed correctly. The appendLeaf function can be verified.\nB) True: Explanation is given. Keeping track of the proofNodes in contract storage is more of a convenience. We technically only require to record the root and make sure that the updates are correct.\nC) True: Valid proofs can always be generated using the current recorded root parameter. However, if another deposit occurs, then this would require a withdrawer to update their ZK proof against a new root which could lead to a DoS scenario.\nIn other words, a malicious actor could frontrun a user causing the current recorded root, the root that the victim user created a proof for, to update and cause the victim's transaction to be rejected."}},"/posts/2024/9/8/race-32-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #32 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-32, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. This month's RACE was designed by Prof. Yannis Smaragdakis from Dedaub.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 32 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nSeptember 8, 2024 by patrickd","code#Code":"All 8 questions in this RACE are based on the below contracts.","question-1-of-8#Question 1 of 8":"Which of the following are problems in this contract?\n A. A user can drain the accounts of others \n B. Legitimate transactions can be front-run \n C. A quantity can overflow or underflow, causing accounting errors \n D. A malicious caller can burn tokens of others \nCorrect is A, D.A: When calling the deposit(_token) function, it takes note of the current _token balance and stores it in balanceBefore. Then the depositCallback() function is called on the msg.sender, ie. the caller of the deposit() function. This is intended to allow the msg.sender to transfer some tokens to the InvestmentProtocol contract and return, after which the deposit() function will determine how many tokens the msg.sender sent by computing balanceAfter - balanceBefore.The problem is that, instead of sending the tokens and returning, the msg.sender could reenter the InvestmentProtocol contract by once again calling the deposit() function. Any balance sent to the contract during the following second call to depositCallback() will be counted for both the first and the second call to deposit(), effectively counting the same balance as two deposits.This means that the user's balance within the protocol (according to userBalances[msg.sender][_token]) will be double the amount of tokens that the user actually transferred. When the attacker then calls the withdraw() function to obtain their full balance, they actually end up draining tokens that were deposited by other users.\n\"This is challenging for many security experts to find. In fact, two auditors from our team missed it (though I don‚Äôt know whether they really spent much time). The difficulty in finding this reentrancy is not in recognizing that there is reentrancy: there‚Äôs a direct call to the msg.sender. The difficulty is in understanding how it can be exploited. The attack is by re-entering and doing another deposit, which causes the deposited amount to be accounted-for twice. The attacker can then call withdraw at whatever later point and get the funds of others. This seems obvious once stated, but it confuses many people because they expect that reentrancy will directly drain funds.\" ~Yannis\nB: From the partial code we have available to us, it appears that, to be able to withdraw, no tokens may be currently invested (require(tokenPoolState.invested == 0)). Therefore an attacker could front-run and block a withdrawal by calling investToken(). However the comment THIS IS SPEC-ED BEHAVIOR, not a race condition! clarifies this to be as-intended.\n\"I'd say this is all within spec. Clearly this is partial code. But the comments in the code clearly say that withdrawal is not possible after investment starts and that everyone in the world can start investments at any point. So, it's spec-ed behavior that one cannot withdraw because investment has started, via front-running or via any other means.\" ~Yannis\nC: In this Solidity version, integer over- or underflows are only possible within unchecked-blocks of which only one exists within this code's withdraw() function. The fact that a user's balance can never be larger than the tracked total deposited, and that both tokenPoolState.deposited and balance are downcast to uint128, means that no underflow can happen during this subtraction. The fact that unsafe downcasting to uint128 is used does also not pose issues in typical real-world scenarios. The fact that this code appears smelly is intended as a red herring.\n\"If uint128 overflows, a lot of services are in trouble. Even uint96 is sufficient for top DeFi services (e.g., Uniswap). I would argue this is a non-issue in anything realistic, or an extremely remote issue that would be ignored if found in a report.\" ~Yannis\nD: The investToken() function restricts token recipients using Merkle Tree Proofs. The issue here is that the tree's leaves are 64 bytes large, created by combining two 32 byte values: The _amount and _toId. Merkle Proofs are created for Binary Trees where each two neighbouring nodes (each in the form of 32 byte hashes) are combined into a single 32 byte hash repeatedly until a single hash is obtained, the Root (permissionsRoot). With that it's possible to select any pair of hashes within this tree and use them as _amount and _toId values while still reaching the same expected Root. Therefore we can proof the inclusion of bogus values within the Merkle Tree, causing the funds to be \"invested\" into the zero address returned by userIdToCurAddress[_toId] due to no valid address having been mapped to the specified bogus ID.\n\"The second serious bug is the use of 64-byte leaves in the Merkle tree, in investToken. This allows an attacker to take internal nodes of the Merkle tree (whose hash has a known reversal into an input of two other 32-byte hashes) and pretend they are leaves. This enables an attack that can burn all the funds in the contract by sending them to address 0, for all tokens that allow transfers to such an address. This is a less serious problem than the reentrancy in deposit, because of all the above qualifications: the attacker doesn‚Äôt stand to gain, and the attack is limited with respect to the token.\" ~Yannis","question-2-of-8#Question 2 of 8":"Which of the following can pose problems in this contract?\n A. The use of only uint128 for the pool state amounts \n B. The call back to the msg.sender in deposit, which allows reentrancy \n C. The unchecked operations in withdraw \n D. The token transfer in investToken(), which allows reentrancy for specific tokens \nCorrect is B.A: See explanation for C in solution of Question 1.B: See explanation for A in solution of Question 1.C: See explanation for C in solution of Question 1.D: While reentering via a token with receiver-callback (eg. ERC-777) would be possible, this would not achieve anything that isn't already possible by simply calling the investToken() function multiple times. This is because the same Merkle leaf can be reused multiple times.","question-3-of-8#Question 3 of 8":"Which function exhibits the most critical vulnerability?\n A. There are no critical vulnerabilities, only less serious threats\n B. deposit\n C. withdraw\n D. investToken\nCorrect is B.There are two critical vulnerabilities: The reentrancy in deposit() and the use of 64-byte leaves in the Merkle Tree in investToken() (See the solution of Question 1 for detailed explanations).While both vulnerabilities allow for a loss of funds, the reentrancy is arguably more critical as it allows the attacker to profit from the exploitation.","question-4-of-8#Question 4 of 8":"Which of the following risks apply to function deposit?\n A. Reentrancy (that can lead to attacker profits) \n B. Arithmetic overflow/underflow \n C. Wrong accounting, even in the absence of other threats \n D. Lack of accounting for tokens with non-standard decimals \nCorrect is A.A: See explanation for A in solution of Question 1.B: In this Solidity version, integer over- or underflows are only possible within unchecked-blocks, of which there are none within the deposit() function.C: No accounting issue in the absence of other threats.D: No need for accounting for tokens of non-standard decimals for deposits.","question-5-of-8#Question 5 of 8":"Which of the following risks apply to function withdraw?\n A. Wrong accounting, even in the absence of other threats \n B. Lack of accounting for tokens with non-standard decimals \n C. Reentrancy (that can lead to attacker profits)\n D. Arithmetic overflow/underflow \n E. None of the above \nCorrect is E.See explanations for Question 1.","question-6-of-8#Question 6 of 8":"Which of the following risks apply to function investToken?\n A. Reentrancy (that can lead to attacker profits) \n B. Lack of accounting for tokens with non-standard decimals \n C. Denial of service / burning of funds \n D. Front-running attacks of legitimate calls \nCorrect is B, C.A: See explanation for D in solution of Question 2.B: The Merkle Tree determines the amount of tokens that may be sent to each investor. These approved amounts are valid across tokens, without taking into account that different tokens may have different decimals.\n\"A third issue is that the approval in the Merkle tree is for an amount, independent of token decimals, and uniform across tokens. Also that the same Merkle leaf can be reused. These are logic issues that may not be important in real use. E.g., amounts may always be MAX_UINT in practice. Still, this makes (B) a valid additional choice for question Q6. (Though (C) is the answer one definitely expects to see in this question.)\" ~Yannis\nC: See explanation for D in solution of Question 1.D: See explanation for B in solution of Question 1.","question-7-of-8#Question 7 of 8":"The Merkle tree use in function investToken\n A. Is safe \n B. Is problematic because of the use of encodePacked \n C. Is problematic if an old version of the OZ libraries is used \n D. Is problematic because of being able to cause false verification \nCorrect is D.The use of encodePacked() on its own is not problematic in this instance. We have no information which OpenZeppelin library version is used.See detailed explanation for D in solution of Question 1.","question-8-of-8#Question 8 of 8":"The use of storage pointers in this code\n A. Is largely inefficient, causing many extra storage loads \n B. Can be safely replaced with memory annotations for the same variables everywhere \n C. Is relatively ok, perhaps with minor inefficiencies \n D. Is dangerous and error-prone\nCorrect is C.The PoolState struct packs both deposited and invested into a single storage slot. This causes minor inefficiencies when unpacking and separating the data after loading the slot when only one of them was necessary. It would be more inefficient to load two separate storage slots when both attributes of PoolState need to be accessed.\nReplacing storage with memory here would not update the actual value within storage.\n\"I don't see anything problematic with storage patterns in general, if used correctly, and they are used correctly here. The risk with storage patterns is aliasing (different expressions for the same storage location) and here we have nothing like that.\""}},"/posts/2024/6/24/race-30-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #30 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-30, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. This month's RACE was created by Secureum Mentor Josselin Feist, engineering director at Trail of Bits.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nJune 24, 2024 by patrickd","code#Code":"All 8 questions in this RACE are based on the below contracts.","question-1-of-8#Question 1 of 8":"Which of the following ERC20-related risks are applicable for this code?\n A. Missing return value checks \n B. Flash minting \n C. Fees on transfer \n D. None of the above \nCorrect is A, C.Some ERC-20 tokens revert on errors, others return false. This code does currently not check for return values of function calls such as tokenOut.transfer(), but there's no guarantee that the token behind the tokenOut variable actually reverts on error. The best practice here is using the SafeERC20 library as a wrapper for ERC20 calls.Fees that are applied on token transfer mean that the actual balance received can be lower than the value specified during a transfer function call. This can lead to losses for the protocol or errors in the accounting logic of a contract. Usage of such tokens should be avoided when the code is not specifically prepared to handle them.","question-2-of-8#Question 2 of 8":"For the below expressions in outGivenIn:\n A. tokenBalanceIn\" + \"tokenAmountIn; - overflow protection must be added \n B. ONE - quotient; - underflow protection must be added \n C. div(tokenBalanceIn, tokenBalanceIn + tokenAmountIn); - div operation must round up \n D. mul(tokenBalanceOut, right_component); - mul operation must round up \nCorrect is C.No overflow or underflow protection is required thanks to the used Solidity version having it natively.Rounding must be chosen in a manner that favors the protocol to avoid abuse. That means that amounts demanded as payment by the contract should be rounded up. While funds leaving the protocol should be rounded down.You should be able to see that a change in the rounding direction proposed by option D would happen too late.","question-3-of-8#Question 3 of 8":"Regarding reentrancy risks (not considering potential additional functions):\n A. The nonReentrant modifier must be added to prevent reentrancy between the two transfers \n B. The user can re-enter through outGivenIn \n C. A reentrancy allows to manipulate the pool‚Äôs spot price \n D. There is no reentrancy risk \nCorrect is A, C.\nThe reentrancy is tricky here, but if you re-enter on the transfer, and the callback is done after the funds have been moved, you basically end up with the pool in an transient state, where TokenOut has decreased, but TokenIn has not increased. ~jos","question-4-of-8#Question 4 of 8":"Regarding fees implementation:\n A. The pool never receives any fees \n B. To apply the 5% fees, the operation should be amountIn * 100 / 105 \n C. Rounding down on the fees allows users to steal the pool's funds \n D. None of the above \nCorrect is A.\nThe fee is applied before we call outGivenIn. As a result, the fee is not given the pool, but it just means that the users have a higher amountIn than expected (but they will also receive more amountOut). ~jos","question-5-of-8#Question 5 of 8":"Regarding the arithmetics:\n A. The arithmetics work regardless of the token‚Äôs name, decimals or total supply \n B. Fixed point arithmetic allows for unlimited precision \n C. Fixed point arithmetic prevents the rounding risks \n D. None of the above \nCorrect is D.\nFor A, USDC won't work due to the decimals being 6 (ex: outGivenIn uses 10**18 for ONE). ~jos\nFixed point arithmetic neither allows for unlimited precision (it is still restricted by the underlying datatype uint256) nor does it completely prevent rounding risks for the same reason.","question-6-of-8#Question 6 of 8":"The following statement(s) is/are true:\n A. SafeERC20 should be used to prevent issues with token transfers \n B. SafeMath should be used to prevent integer overflow \n C. Making swapOutGivenIn external will reduce gas cost \n D. Making outGivenIn public would allow an attacker to steal the pool‚Äôs funds \nCorrect is A.At the moment the contract would not be able to support ERC20 tokens that return boolean values (false) in case of error, because it's not checking for any return values at all when making transfer-calls. The SafeERC20 library would wrap interactions with ERC20 tokens and ensure that both tokens that revert and return false on error are handled correctly.No overflow protection is required in this version of Solidity.Changing a function's visibility from public to external does not impact its gas usage on its own. Rather, the data-location of its parameters does (by avoiding copying to memory but instead accessing parameter values directly in calldata). The function does not make use of datatypes that specify a data-location though, so gas cost can't be reduced this way here.The outGivenIn is a pure function (ie. it does not change state, or even read chain information at all) which makes it save to set as any visibility.","question-7-of-8#Question 7 of 8":"Regarding slippage/front running risks for swapOutGivenIn:\n A. MinAmountIn should be added as a parameter \n B. MaxAmountIn should be added as a parameter \n C. MinAmountOut should be added as a parameter \n D. MaxAmountOut should be added as a parameter \nCorrect is C.Currently the user is only specifying an amountIn, in other words, the amount he wants to exchange for the other token. The contract, based on amountIn and its current ratio of token balances, computes the amountOut that the user will receive of the other token. It's possible that the user's transaction is sandwiched, manipulating the ratio and profiting off the spread caused to the user. To prevent this, there should be an additional parameter that specifies a minimum amountOut that the user requires for this exchange.","question-8-of-8#Question 8 of 8":"Assuming there is no bug in the codebase (and all potential bugs highlighted in this RACE are fixed), which one of these swap invariants would be correct? A.\n B.\n C.\n D. None of the above\nCorrect is C.Assuming correct code, it must be the option where k increases thanks to collecting fees."}},"/posts/2025/1/10/race-36-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #36 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is the official solution of RACE-36, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. Answers and explanations have been provided by the author Zigtur, an independent security researcher and colleague at Spearbit.\nParticipants of this quiz had a single attempt to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nJanuary 10, 2025 by Zigtur","code-snippet-1#Code Snippet #1":"All 8 questions refer to the following library.","question-1-of-8#Question 1 of 8":"The alt_bn128/bn254 curve is the only pairing-friendly curve supported in the EVM through a precompile. What is it mainly used for?\n A. zk-SNARKs \n B. zk-STARKs \n C. Schnorr multi-signatures \n D. BLS multi-signatures \nCorrect is A, D.A: True, this curve is used for zk-SNARKs like Groth16. A good ressource for Groth16 and ZK is the ZK book by Rareskills.B: False. STARKs rely on hash functions and not on elliptic curves. The STARK101 course by Starknet is a good ressource to get started.C: False. Schnorr multi-signature is a digital signature algorithm used in Bitcoin. It allows multi-signature with SECP256K1.D: True, the curve can be used for BLS multi signatures. This signature scheme is really useful for validators (Ethereum consensus uses it with another curve).These signatures are based on bilinear pairings. It allows aggregating pubkeys and signatures through addition to make a single signature verification through pairing.For example, if there are 100 validators that sign the same data, ECDSA will require verifying 100 signatures. BLS signatures allow to aggregate the pubkeys and signatures into a single pubkey-signature pair. This single aggregated signature is verified with the single aggregated pubkey.","question-2-of-8#Question 2 of 8":"Which of the following curve is the most similar to alt_bn128/bn254:\n A. secp256k1 \n B. ed25519 \n C. bls12-381 \n D. brainpoolP256r1 \nCorrect is C.A/B/D: False, it is not a pairing friendly elliptic curve.C: True, both are pairing-friendly elliptic curves.","question-3-of-8#Question 3 of 8":"Which Ethereum projects rely on the alt_bn128/bn254 curve?\n A. Tornado Cash \n B. EigenLayer \n C. AAVE \n D. KyberSwap \nCorrect is A, B.A: True, ZK purposes.B: True, EigenLayer with EigenDA. EigenDA uses it to verify aggregated (BLS) signatures for AVS.C/D: False. They are DeFi protocols and don't use multi-signature or ZK.alt_bn128 elliptic curve is used for ZK purposes. It is used with the Groth16 ZK algorithm to make private transfers.Despite bls12-381 curve being used in the Ethereum consensus, it is not supported inside of the EVM. The only ZK-friendly curve in EVM is alt_bn128, which is why it is used by Tornado Cash.","question-4-of-8#Question 4 of 8":"There are 3 EVM precompiles for operations on the alt_bn128 curve. Which statements are true?\n A. ecDiv precompile allows dividing a point by another point \n B. ecAdd allows adding a scalar to a point and adding two points together. \n C. ecMul only allows multiplying a point and a scalar. \n D. ecMul expects point input in compressed format. \nCorrect is C.A: False. Division does not exist in elliptic curve calculations.B: False. Elliptic curve only supports addition of two points and not addition of a point and a scalar.C: True, elliptic curves define multiplication as an operation between a point and a scalar.D: False. ecAdd, ecMul and ecPairing precompiles support operations on uncompressed format points.","question-5-of-8#Question 5 of 8":"With e being the pairing function, H being the message hash, sk being a secret key and r being a random, which formula(s) verify a signature?\n A. e([sk * H]_1, -[1]_2) + e([H]_1, [sk]_2) == 0 \n B. e([sk * H]_1, [1]_2) ==  e([H]_1, [sk]_2) \n C. e([sk * H]_1 * r, [1]_2) ==  e([H]_1, [sk]_2 * r) \n D. e([sk * H]_1, [r]_2) + e([H * r]_1, [sk]_2) == 0 \nCorrect is A, B, C.A: True, this is the equation that the ecPairing precompile will verify for a signature.B: True, equivalent to option A. It is equivalent due to bilinear pairing properties. The ecPairing precompile is not able to handle such calculations though. The first equation form (Answer A) should be used.C: True, a random can be added during signature verification. Due to pairing properties, a random r can be added on both sides of the equation without breaking it.D: False, the random is added on both side, however one of them should be negative for the equation to be zero.","code-snippet-2#Code Snippet #2":"Note that in the original RACE the code of the validateSignature function couldn't compile due to a check on the hashed variable which wasn't initialized and wasn't setting the valid boolean in storage. The issue is corrected above and had no impact on the answers.","question-6-of-8#Question 6 of 8":"What is true about the given Validators contract?\n A. No validator is able to register. \n B. Only owner is able to aggregate a validator. \n C. Malicious validator can break aggregation. \n D. Ownership is transferable \nCorrect is A, B, C.A: True, registerValidator require the pendingPubkey to be G1, but this is not initialized in constructor. This is because G1 is not (0, 0). G1 is expected to be (1, 2).B: True, owner check is done through require.C: True, by using registerValidator with an incorrect pubkey, caller will break aggregateValidator (permanent DOS): aggregateValidator will not work because the pendingPubkey is not a point that satisfies the BN256 elliptic curve equation. It is not a valid point. When adding a valid point with an invalid point, the addition will fail.D: False. There is no such functionality in the contract.","question-7-of-8#Question 7 of 8":"What is true about validateSignature?\n A. It requires 2/3 of validators' signatures \n B. It is prone to signature malleability \n C. It requires all validators' signatures \n D. A signature marked as valid will always remain valid \nCorrect is C.A: False. A threshold mechanism could have been set with BLS signatures by subtracting the non-signer pubkeys from the aggregated pubkey to obtain a 2/3 threshold.B: False, BLS signatures are not prone to signature malleability.C: True, all validators must sign for the signature to be valid with the aggregated pubkey. The aggregated pubkey is the sum of all validators pubkey. For a signature to be valid with this aggregated pubkey, it must be the sum of all validators signature. If one of the signature is invalid (e.g. the data signed is not the same), then the whole aggregated signature is invalid.D: False, if a validator is added, the aggPubkeys change and the signature can be reset to false in the mapping.","question-8-of-8#Question 8 of 8":"What is true about verifySignature?\n A. The pseudorandom used in the pairing validation is not needed for verifying the signature \n B. The function ensures that the G1 public key matches with the G2 public key \n C. The function will revert for incorrect signatures. \n D. The function will revert for correct signatures. \nCorrect is A, C, D.A: True, it can be removed or kept to test the signature validity, as long as equation is adapted.B: False. There is no such check.C: True, it will revert everytime for both valid and invalid signatures.D: True. The pseudorandom is included in the first element of the equation but not in the second part. It is equivalent to e([sk * H * r]_1, [-1]_2) + e([H]_1, [sk]_2).The pairing never resolves into == 0 and wil never be valid.The code to verify correct signatures: valid = pairing(mul(signature, pseudorandom), negG2(), mul(mul(G1(), sigHash), pseudorandom), publicKeyG2);"}},"/posts/2024/3/25/cryptocurrency-privacy-technologies-lelantus":{"title":"Cryptocurrency Privacy Technologies: Lelantus Protocol","data":{"":"25. March, 2024 by patrickd\nDespite being regularly referred to as \"anonymous Internet money\", the ledgers of the most widely adopted cryptocurrencies are completely public. Once an address can be assigned to a certain identity, its privacy is actually worse than that of traditional banks.Previously we discussed how Zcoin's Sigma Protocol upgrade re-implemented the Zerocoin Scheme with One-Out-Of-Many Proofs. This resulted in an efficient and trustless cryptocurrency that enhances privacy by breaking the transaction graph. Its biggest drawback however was that it did not allow hiding the amounts being transacted. On the other hand, we had Confidential Transactions that became efficiently viable thanks to range proofs implemented with Bulletproofs. While the Confidential Transactions scheme hides coin amounts, its transactions remain traceable. Fusing both of these technologies, we obtain the Lelantus Protocol that achieves both confidentiality and untraceability.","the-concept#The Concept":"Understanding the core concept behind the Lelantus Protocol only requires some basic knowledge on Bitcoin transactions and Elliptic Curve Cryptography.","zerocoin-scheme-in-review#Zerocoin Scheme in Review":"At a high level, the Zerocoin Scheme basically adds a native mixer to a standard transparent ledger blockchain: A fixed denomination of public coins is \"burned\" in exchange for \"minting\" a private coin. Or more accurately, some fixed amount of public coins are locked into a pool together with all other coins of the same denomination and for each time a user does this they obtain a secret voucher that they can later redeem in order to re-obtain that same amount of public coins. When the voucher is revealed for redemption, the fact that there's no way to tell which of the public coins were originally burned in exchange for the voucher is how this scheme achieves untraceability.The Sigma Protocol implements this scheme by using Pedersen Commitments for vouchers, where  is a serial number and  is a random blinding factor.When a user mints a voucher, they publish such a commitment  as part of the transaction's output and burn the appropriate amount of public coin from some UTXO. Later, the user can redeem those vouchers by generating a Zero-Knowledge One-Out-Of-Many Proof that demonstrates that they have the knowledge necessary (, ) to open a commitment within the set of all minted vouchers. The verifier requires the serial number  to be revealed as part of this process to prevent users from redeeming the same voucher multiple times. The blinding factor  though is kept secret and without it, it's impossible to determine the commitment  that belongs to the revealed serial number.","confidential-transactions-in-review#Confidential Transactions in Review":"The Confidential Transactions scheme works by hiding transaction amounts within homomorphic commitments. Homomorphism allows the verifier to sum the hidden amounts of both transaction inputs and outputs and compare these sums to ensure that no new coins are being created from nothing. These commitments  are Pedersen as well, with the difference that they're hiding the public coin amounts  instead of a serial number .Unfortunately, checking  on its own is not sufficient since outputs could contain \"negative amounts\" that would once again allow the creation of new coins. To prevent this, the verifier requires each output to have a range proof demonstrating that all values  are \"positive\".","lelantus-core-idea#Lelantus' Core Idea":"As shown, the cryptographic primitive that both the Zerocoin Scheme and Confidential Transactions have in common are Pedersen Commitments:Lelantus combines both of these schemes through this commonality, resulting in a protocol that inherits both of their the privacy-enhancing aspects.Coin commitment  now hides both the coin's serial number  and the coin amount  it is carrying. To maintain the binding properties of this generalized Pedersen commitment (or \"double-blinded\" commitment), we assume that the logarithmic relationship between all generator points () is unknown.","minting#Minting":"For a coin commitment to be of value, it has to be \"minted\" as done in the Zerocoin scheme. To mint, the transaction signer chooses the public coin UTXOs they want to \"burn\" in exchange. The sum of the input amounts becomes the committed value  which is blinded by both the serial  and the blinding factor . The resulting coin commitment  is published by the signer as part of the transaction outputs. Before adding the commitment to the set of valid vouchers, the validators have to ensure that the committed value  within  is actually equal to .Despite knowledge of what  should be, the verifier can only open the commitment  with additional knowledge of  and . However the serial number should only be revealed as part of the redemption process, and the random blinding factor must never be revealed as this would introduce traceability by reconstructing the commitment during redemption.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nInstead of revealing information to allow the verifier to open the commitment and check the committed coin amount, a generalized Schnorr Proof is used to convince the verifier of the signer's knowledge of  and  such that the commitment opens for the correct .","splitjoin#SplitJoin":"The mint operation is purposefully kept simple with a single private output, which is sufficient to move arbitrary amounts of coins into the \"mixer\". But if the spending logic would be equally simple (ie. redeeming only a single voucher per transaction), the protocol's privacy would actually be worse than the Zerocoin scheme on its own. That is because before we basically had multiple anonymity sets, one for each fixed denomination. But if redemptions reveal the exact same amount of coins used during minting, each specific public coin amount would become its own anonymity set - and it's unlikely that an amount such as  would be minted by multiple people.This is why, instead of a simple spend operation, we have \"SplitJoin\" that allows merging multiple coin commitments in a transaction's input and splitting them apart into new commitments as output. Most interestingly, it lets us execute partial spends: If you had minted a single coin commitment with a large sum, you'll be able to anonymously redeem that commitment in exchange for a new commitment and a public coin output that contains a partial amount of the original sum.Implementing the JoinSplit operation comes with several challenges:\nWith the coin commitments now being double-blinded, changes to the One-Out-Of-Many Proof protocol are necessary to demonstrate the ability to open a commitment within the anonymity set with the specified serial number.\nSimilar to how it was part of the Confidential Transactions scheme, it's necessary to ensure that the sum of input coin amounts is equal to the sum of output amounts. But since we can't reveal the input commitments without losing untraceability, this needs to be done as part of the OOOMP somehow.\nWhen a transaction has more than a single output (which is always the case when considering fees as a transparent output), each of the output coin commitments must come with a range proof to prohibit commitments with negative coin amounts being used to create new value out of thin air.","the-math#The Math":"","generalized-schnorr-proof#Generalized Schnorr Proof":"We previously discussed the construction of simple Schnorr Proofs in the introduction of the Zerocoin scheme. In short, it allowed us to prove knowledge of a secret witness  belonging to a public key  such that .A generalized Schnorr Proof allows the prover to demonstrate knowledge of multiple such witnesses  belonging to a single commitment  without revealing them.With all  being logarithmically independent generators, the following protocol is executed between Prover and Verifier:\nProver\tVerifier\tKnows \tKnows \tChooses random scalars \t\t\t\tSends \tKnows \t\tChooses random challenge \tKnows \t Sends \tComputes\tSends \tKnows \t\t\t\nDuring the Lelantus minting process, the verifier already has knowledge of the coin amount  from the transparent UTXO inputs and is therefore able to compute . After subtracting this from the coin commitment specified as the transaction output (), the above Schnorr protocol demonstrates that the signer is able to open  without revealing its secret scalars. If this succeeds, the verifier can be sure that the original  committed to the expected amount  since .","ooomp-in-review#OOOMP in Review":"To briefly recap how One-Out-Of-Many Proofs originally worked, we assume that there are  commitments  each representing a minted private coin. We say that a Prover has knowledge of serial number  and a blinding factor  by which one of these commitments at index  can be opened, proving ownership and therefore the ability to spend the private coin.To exchange the private coin for its denomination of locked public coins, the Prover reveals the serial number  which the Verifier will remember to prevent the Prover from spending the same commitment more than once. The serial number is \"homomorphically subtracted\" from all commitments  resulting in some other  for all but the commitment at  which now commits to 0:The proof now demonstrates that the Prover is able to open one of the commitments  with his knowledge of  without revealing that it was . To do so, it begins with the execution of a 3-move-type proof committing to a valid index  in zero-knowledge, during which it responds to the challenge  with :Here, the value of  is represented by  bits , while  is a random value committed to by the Prover in the first move of the protocol. This is generalized, such that it can be applied to all indices  represented by bits :Creating the product  for any index  results in a polynomial  that can be expanded to the standard form:The Kronecker Delta  will only be  for when the bits of both  and  at position  are equal. This means that all  of the product are only multiplied by  when the current index , resulting in a polynomial of -th order. For all other indices  some  will be multiplied by  resulting in lower-order coefficients.Even without knowing  yet, the Prover is able to determine the polynomial's coefficients which are based on his own randomness . The lower-order coefficients  are used to extend the zero-knowledge protocol with additional first-move commitments:The idea is that the  commitments represent the polynomials  in a state where they have not been evaluated for a specific  yet, with the difference that the commitments lack the highest order coefficient for . On the other hand, the products based on the challenge-response values  represent the polynomials  evaluated for challenge  with all their coefficients. A subtraction between these two will therefore result in all lower-order coefficients canceling out, leaving .To prevent revealing the actual , the commitments  would additionally have blinding factors  known only to the Prover.With that, the Prover is able to demonstrate knowledge of blinding factors  and  without revealing them, and therefore proving his ability to open the commitment , and ultimately .","generalized-ooomp#Generalized OOOMP":"As can be seen above, the original One-Out-Of-Many Proof scheme assumed that the commitment, that is being proven to be part of the anonymity set, is required to commit to zero with a single blinding factor:But this does not apply in Lelantus anymore, where we basically have a commitment that is double-blinded by both the coin amount and a random factor:The Lelantus paper presents a modification to the OOOMP system that works with double-blinded Pedersen commitments:\nProver\tVerifier\tKnows \tKnows \tChooses random scalars \t\t\t\t\t\t\t\t\t\t\t\t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \t\t\t\t\t\t\t\t\t\t\tSends \tKnows \t\t\t\t\t\t\t\t\t\nThe above protocol for  anonymity sets was taken from the previous article on the Sigma Protocol. For the purposes of this article, we'll only concentrate on the .The final check originally looked like thiswith  and As explained in the review,  and  mostly cancel each other out. Specifically,  will leave behind the commitment  that we were proving inclusion within the anonymity set of, while  leaves the blinding factors .The same principle still applies for the generalized protocol for double-blinded commitments where :Looking at  more closely, we can see that the addition of the new  commitments results in something very similar to the old , with the difference that it will leave behind blinding factors for both  (for ) and  (for ).Therefore, we can see that thanks to these changes the protocol indeed works for double-blinded commitments:","balance-proof#Balance Proof":"It should be noted that the formal proof of soundness for the following balance proof provided by the paper turned out to be incorrect. So far, nobody had been able to solve the issue with the proof, though nobody was able to find a way to actually exploit that either. This was also the reason why Firo went with the Spark segregated approach.","for-transparent-output#For Transparent Output":"A yet unresolved problem is verifying the sum of coin amounts that each commitment being spent as transaction input is carrying - while not having the actual commitment but only an inclusion proof demonstrating that said commitment was indeed minted and added to the pool of valid vouchers.Since the inclusion proof carries (blinded) information about the commitment value , we can make use of the OOOMP's transcript to extract a commitment , which is basically  with a modified blinding factor:If we now want to verify whether the commitment being redeemed carries the expected coin amount , we can subtract  from it and have the user prove that he is able to open the resulting commitment . The way Lelantus handles this is by treating it as a public key and having the user sign the transaction with  as the private key.If we have multiple private coin transaction inputs, and therefore multiple OOOMPs for each, we can simply sum all of the commitments  and subtract the overall expected value from it to obtain the \"public key\":Doing this requires all One-Out-Of-Many proofs to share a common challenge value  (ie. executing them in parallel). Furthermore, in regards to this challenge being derived from committed values (ie. Fiat-Shamir), those values should not only include those of all inclusion proof commitments, but also all of the output coin commitments and their range proofs.","for-private-coin-output#For Private Coin Output":"So far we're able to prove that the sum of private inputs is equal to the sum of public outputs. But for a partial spend or split operation we need to check equality against coin commitments that were specified as transaction outputs.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThe intuitive solution would be to simply sum the output commitments  and subtract them from the sum of blinded input commitments  derived from the OOOMP transcript.Let's look at the simplest case where we have a single coin commitment being spent as input and a single new coin commitment being minted as output:If the values  of inputs and outputs are truly equal then they should cancel out leaving a commitment of the form . The balance proof can therefore be a simple generalized Schnorr Proof showing that the user is able to open this commitment without the use of the  generator (ie. with ).","abdks-critical-bug#ABDK's Critical Bug":"You might have been wondering why the transaction's private output coins should be committed to as part of the inclusion proof for input coins. The reason is a critical issue identified by ABDK Consulting during an audit of the Lelantus Protocol's cryptography.In the generalized OOOMP, commitments  and  are summed as part of the proof's verification. The core of the exploit stems from the fact that these commitments are simply assumed to be correctly formed, but they can be arbitrarily modified as long as their sum remains valid.For example, we can instead commit to  and  which contain a coin amount :This is completely legal since  causes the inclusion proof to remain valid. The balance proof however only makes use of  where this modification can now be exploited: Based on the One-Out-Of-Many Proof's transcript the attacker constructs an output coin of the form  then the Balance Proof will look likeThanks to knowledge of challenge  we can compute the inverse  for the output commitment 's construction. With that, our injected value  cancels out and the balance proof remains valid. This allows us to illegally obtain a new voucher worth  coins. But if we're required to commit to the output coins before we obtain  we will no longer be able to exploit this.","generalized-bulletproofs#Generalized Bulletproofs":"In the article about Bulletproof Range Proofs, we saw how we can prove that a value  within a Pedersen Commitment of the form  is within a specified range . Deconstructing the scheme into multiple parts, we ended up with a (1) Zero-Knowledge Inner Product Proof, a (2) Recursive Inner Product Argument, and a (3) Range Constraint Circuit.The  required for generalizing Bulletproofs to work with double-blinded commitments only affect (1) the Zero-Knowledge Proof. For this, remember that we're trying to show that the inner product of two vectors  results in a value  hidden within a commitment .The vectors are separately committed to inwhere  and  are vectors of generators ,  with unknown logarithmic relationship. Assuming a simple case of two-dimensional vectors () we can expand this to:To achieve zero knowledge, we further need random vectors  and , chosen and committed by the prover as:The updated protocol has the following structure:\nProver\tVerifier\tKnows \tKnows \tComputes \tComputes \tChooses random \t\t\tSends \tKnows \t\tChooses a random challenge scalar \tKnows \t Sends \tComputes \tComputes \tComputes \tComputes \tComputes \tSends \tKnows \t\tComputes \t\t\t\t\t\nUnder the assumption that the original protocol was correct, we can demonstrate that the protocol remains correct after the changes with a few substitutions and rearrangements:","the-code#The Code":"We've seen that the main components of the Lelantus Protocol are One-Out-Of-Many Proofs and Bulletproof Range Proofs. Reviewing how these were implemented in detail would go beyond the scope of this article, instead, we'll be looking at some implementational details that are interesting from a security point of view.","sliding-anonymity-sets#Sliding Anonymity Sets":"You might remember the zerocoin_params.h file from previous articles that defined various constants around the Zerocoin and Sigma protocol. Around the time of the Lelantus Upgrade (v0.14.1.2) the Zcoin project renamed itself to Firo, and so did this file.\nWhen Zcoin was first released, RSA Accumulators were used for storing coin commitments. Restrictions were needed on how many coins would be added to the same Accumulator since it would become increasingly easier to forge inclusion proofs with an increasing amount of accumulated commitments. At the beginning, the maximum amount of coins added to an accumulator was restricted to only 10 commitments and was later raised to 10k. With the Sigma Upgrade and OOOMP replacing RSA Accumulators, the maximum anonymity set size was further increased to 16k. The limit was no longer in place for security reasons but simply because the work necessary for generating and verifying OOOMPs increases linearly with the set size. Finally, with the Lelantus release, the limit was increased to 65k.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nBut more interestingly, Lelantus also changed how a new set would be initialized when the current one has reached its limit. Before, the sets would simply start from the beginning, meaning they'd empty and users would do best to wait a while for the anonymity set to grow before they redeem their funds from it. With Lelantus anonymity sets overlap, specifically, once the current set reaches its limit a new set would be started including the last 16k commitments from the current set. So each anonymity set in Firo's Lelantus implementation shares 16k commitments with the previous and the next set.Whenever a user wants to redeem one of the coin commitments from an anonymity set, they have to provide a groupId (anonymity set identifier) together with the inclusion proof. This means that, if the commitment being redeemed is in a shared region of the anonymity set, the wallet will decide which set it'll prove inclusion for. So for a new set to start with 16k commitments in truth, those wallets must ensure that the identifier of the new set is used, instead of the one that the commitment was actually minted in.But that wasn't actually the case until Firo's v0.14.11.2 release where the following lines of code were added to the wallet with PR1199:","trail-of-bits-audit#Trail of Bits' Audit":"Firo's Lelantus Upgrade code was audited (ie. security reviewed) by Trail of Bits who found one high and one medium issue that we'll be taking a closer look at here. It's unsurprising that both were found in the implementation of the Range Proof scheme, since this required implementing new and complex additions to the codebase.","negative-value-issue#Negative Value issue":"Output coins come with a Bulletproof Range Proof that proves that their value  is within an expected range. Normally, these proofs offer an upper boundary of a power of two (2^n) resulting in a range check of [0, 2^n - 1]. Firo however wants to check a custom range that isn't necessarily bounded by a power of two: [0, nMaxValueLelantusMint].The intention was to achieve this by adding the difference between 2^n and nMaxValueLelantusMint to the commitment. Therefore, if the commitment's value was larger than nMaxValueLelantusMint it would end up being larger than 2^n - 1.The problem with this logic is that, since scalars are operating within a field (ie. scalars are modulo the order of the curve), adding the difference can make the commitment's value overflow and therefore appear to be within the range. That would mean that \"negative\" values (ie. values that will cause wrapping when summed with other values) will pass verification because the verification itself will make it overflow.A simple solution to this problem is checking both ranges: Checking [0, 2^n] with the unmodified coin commitment to ensure it's not already a negative number. Then adding the difference and checking [0, nMaxValueLelantusMint] to ensure that the value is below Firo's custom upper bound.","fiat-shamir-issue#Fiat-Shamir issue":"At the very beginning of a Bulletproof Range Proof we commit to two input vectors with  and to two random vectors with . For constructing the range constraint circuit using random linear combination, we need two independent challenges  and .The code attempted to achieve this basically like this:Where assuming , this would result in . But that assumption is exactly the problem here: When both vector commitments are the same, the resulting hashes (even in alternating order) will also be the same.The solution was simply adding  as parameter to 's generation:","the-lelantus-attack#The Lelantus Attack":"In February 2021 an attacker managed to forge Lelantus proofs. The Firo team noticed abnormal chain activity and emergency-disabled Lelantus with the power of a killswitch that had been temporarily added with the Lelantus Upgrade to handle exact cases like this. A blacklist was implemented to lock the attacker's illicitly created funds.The exploited issue was described as an implementation error that allowed forging a coin spend, but no technical description was published and the details have mostly been forgotten at this point. The update resolving the issue (PR1012) was large and came with many changes that intended to additionally harden the protocol. I wasn't able to tell which of these changes specifically ended up fixing the issue that was exploited, though roughly discussing what has changed may be helpful either way.\nLevon Petrosyan, one of Firo's core developers clarified that the attack was actually possible due to weak Fiat-Shamir: Anonymity state was not included in challenge generation, and this allowed to do a time travel attack.\nAttacker creates tx#1 spending a non-existend coin, keeps it\nAttacker creates a coin based on tx#1, which could be spent in it\nAttacker creates tx#2, in which he spends his existing coin, and as output he puts the coin generated in step 2\nAttacker populates the tx#2\nAttacker populates tx#1 and it passes\nlelantus bug: \"something like generate coin A, coin B, then set up the state as if both were minted (but mine only one mint), then spend the second coin using the forged state\"\"And yes, it's the case that Spark doesn't need or use internal counters to maintain state\nCoins are instantiated using random nonces that the recipient recovers as part of its detection and correctness checks\"So including the anonymity set hash in the OOOMP transcript prevents creating a proof without having the actual spending coin in the set.\nMost of the changes are related to Fiat-Shamir challenge generation of the Zero-Knowledge proofs. The challenges (ie. a hash based on committed data) are now based on more data (eg. the anonymity set contents now influence the OOOMP's challenge value). In fact, it looks like everything that could possibly be committed to now is. Also, simple domain_separators (ie. string prefixes to comitted data) were added to ensure transcript information can't be reused across proofs or versions.The challenge hashing itself has also changed: Before a simple sha256 hash was generated, now messages are double-hashed (ie. sha256(sha256(m))). This prevents \"length extension\" attacks that the sha256 algorithm is vulnerable to. This type of attack exploits the padding space to append more data to the hashed message (ie. you can generate sha256(secret + attacker_controlled) from the result of sha256(secret) alone).Finally, an additional Schnorr proof was added to the protocol, proving that  is well-formed. And the inner product proofs of the Bulletproof Range Proof no longer use separate transcripts.","conclusion#Conclusion":"With the introduction of the Lelantus protocol into Firo, wallets were adjusted and encouraged to move towards a more \"private by default\" model. Before, Zcoin's privacy features were much more \"Opt-In\" for the simple reason that the RSA Accumulator inclusion proofs were so large that their use strained the system.While Lelantus was a great improvement over how Zcoin originally worked, its biggest disadvantage remained its lack of proper stealth addresses. So far, one could anonymize their funds with the protocol, but to actually send them to another person you still had to use the transparant ledger. This is the main reason why Lelantus Spark was created.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nIn Spark users now have both transparent and true stealth addresses. As long as the user only uses their stealth address, \"private by default\" is basically enforced. With this system in place it would be possible to completely remove transparent address use from Firo, although there are arguments for their usefulness (eg. for centralized exchanges).Beyond Firo, the work in creating Lelantus revived interest in One-Out-Of-Many Groth-Bootle Proofs inspiring various spin-offs such as Lelantus-MW/CLA (Beam), Anonymous Zether (Bunz et al), Triptych and Arcturus (Monero Research Labs).","appendix#Appendix":"","direct-anonymous-payments#Direct Anonymous Payments":"While the Lelantus Protocol basically adds a native mixer with a large anonymity set and arbitrary amounts, actual transfers between users still happen on the transparent ledger. Users could theoretically exchange secret voucher information with each other, but that requires trusting the person you're receiving it from to not redeem it before you're able to. To improve this, the initial design of Lelantus proposed to extend the protocol with a simple version of Direct Anonymous Payments.You might remember how the original Zerocoin Protocol used a random number for the serial  of the coin commitment. This turned out to be a protocol flaw as an attacker could observe a serial number from a pending redemption transaction in the memory pool and frontrun it with their own mint-redeem transactions to mark the serial as used. While this doesn't allow an attacker to steal other users' funds, it did present a potentially dangerous denial of service vector.To fix this, spending keys were introduced: To mint a voucher, instead of generating the serial directly from randomness, one would create a random witness . The actual serial number used for the coin commitment would be derived from the public key  by hashing it: . To redeem a voucher with the serial number , one now has to additionally sign the transaction with the spending key . Since such a valid signature could only have been generated with knowledge of the spending key, it proves that the signer is authenticated to redeem the voucher with the given serial.Simple Direct Anonymous Payments that don't require any significant changes to the protocol, can be implemented by ensuring the spending key is computed in such a manner that the sender is not able to redeem the voucher before the receiver. In short: Instead of the sender, we have the receiver come up with the spending key  and share  with the sender. To mint a coin commitment, the sender determines a serial number from  with  being a randomly sampled scalar by the sender. After the sender privately shared  with the recipient, the voucher can be spent with knowledge of  which only the receiver has.While this prevents the sender frontrunning the receiver's redemption, it's still recommended that the receiver should spend this commitment for another because the sender has knowledge of the serial number  which allows them to observe when the voucher is being redeemed. The traceability, the necessity of additional transactions, and the possibility of leaking timing information in the process made this approach less than optimal for practical use.\nSender\tReceiver\t\tSample random \t\t\tKnows address \t\tSample random \t\t\t\t\t\tMint \t\t\tKnows \t\tSpend coin commitment :  1. Reveal   2. Generate spend proof  3. Sign transaction with","untraceable-direct-anonymous-payments#Untraceable Direct Anonymous Payments":"A follow-up whitepaper proposed improved Untraceable Direct Anonymous Payments by introducing one-time shielded layer addresses. Similar to before, the recipient generates a private spending key  but this time  will be a blinded commitment . Additionally, the sender would require a Schnorr proof  on  demonstrating its representation with respect to the fixed public generators  and  (we can't allow this to include the generator  used for the coin value).\nProver\tVerifier\tKnows \tKnows \tChooses random \t\t\t\tSends \tKnows \t\tChooses random challenge \tKnows \t Sends \tComputes \t\tComputes \t\tSends \tKnows \t\t\t\nThe tuple  represents the shielded address that is shared with the sender for them to make the direct transfer. After the sender verified , they sample a random  and create a commitment . This effectively results in a coin commitment with a serial  that remains unknown to the sender:As you can see, we're no longer doing hashing to arrive at the serial number , but that doesn't have any negative impact. The product of the scalars  still results in the spending key with which we'll sign the transaction.The sender generates the necessary proofs for publishing the output coin , publishes  to the blockchain, and privately sends  to the receiver. Later, the receiver can reveal  as serial number, generate the required proofs, and sign the transaction with  as the spending key.\nSender\tReceiver\t\tSample random \t\t\t\tGenerates Schnorr representation proof \tKnows address tuple \t\tVerifies \tSample random \t\t\t\tMint  and publish \t\t\tKnows \t\tSpend coin commitment :  1. Reveal   2. Generate spend proof  3. Sign transaction with \t\nNote that  are required to be revealed for the proof verification process. After all, the sender has no knowledge of the scalars for  and  respectively, so instead  is treated as a generator of the commitment . Due to the necessity to reveal the tuple publicly with each transaction, an address can only be used once to maintain anonymity.","shielded-address-system#Shielded Address System":"In the final version of the Lelantus paper the Untraceable Direct Anonymous Payments scheme was fully integrated into the protocol. But due to its lack of being a proper (reusable) stealth address scheme, it didn't actually get implemented that way in Firo's Lelantus upgrade. That is why most of this article resembles the simpler initial design and why I've moved everything related to its shielded address system here into the appendix.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThe scheme still follows the same principle, but communication of  from the sender to the receiver is now handled on-chain. This is achieved by adding a public key  to the shielded address tuple  which is used to asymmetrically encrypt the sender's information.The receiver's wallet can then scan the ledger for its public addresses  to identify transaction outputs the user should be able to spend. It then uses the private key of  to decrypt end extract  which enables the receiver to spend them at their discretion.","receiver-address-privacy#Receiver Address Privacy":"Or \"Reusable Payment Codes\", was originally a Bitcoin Improvement Proposal (BIP-47) that wallets could opt to implement in order to improve privacy, but unfortunately hasn't seen widespread adoption in the Bitcoin ecosystem. The scheme basically allows for \"address\" reuse without negative implications on the receiver's privacy since the actual destination address on the transparent ledger will be a different one for each transaction and sender.Adopting BIP-47 into Firo was the compromise that actually ended up being implemented (instead of Direct Anonymous Payments) in a separate release after the Lelantus upgrade. Specifically, Reusable Payment Codes are basically extended public keys (xpub) from which the sender can derive receiver addresses that are unique between the sender and receiver.","hierarchical-deterministic-key-derivation#Hierarchical Deterministic Key Derivation":"To understand the scheme, we first have to roughly review how wallet key derivation works. If you've ever used a cryptocurrency wallet you have probably noticed how a single seed (usually represented by 12 words) somehow allows you to have multiple wallets/accounts with many addresses/pubkeys each. If you've further ventured into the advanced settings of such a wallet software you might've also come across a \"derivation path\" like m/44'/60'/0'.There are several BIPs that specify what these things are and how they work: BIP-39 describes how to generate mnemonic words and how to convert them into a 512 bit seed. BIP-32 describes how such a seed can be used to derive a hierarchical structure of private and public keys based on a derivation path. And BIP-44 specifies the derivation path structure to be used for \"HD\" (Hierarchical Deterministic) Wallets.Below you find a diagram showing how to arrive at a public address / private key pair from a seed based on a given derivation path. Basically, each element separated by / within the path represents a node for which we can make some computations that take the parent's keys as input and then output the resulting child keys. The master node m/ is a special case, since it does not have parents but computes the master keys based on the seed. All following nodes take an index /i that specifies the identifier of the child to be generated. For example, if we want the private key of the second wallet's fifth address we compute it based on the derivation path m/44'/60'/1'/0/4. From this private key, we're then able to create a public key and its address.Interesting for our purposes is the fact that it's possible to determine all Child Public Keys (and therefore addresses) from a Parent Public Key and the respective Parent Chain Code, as long as all following Child Nodes are not hardened. For example, if we want to share everything that has been going on in our second wallet with our accountant, we don't have to give him any private key. We can instead provide him with the \"extended public key\" (tuple ) of m/44'/60'/1' and he'll be able to generate all Child Public Keys further down the hierarchy (m/44'/60'/1'/*/*).","reusable-payment-codes#Reusable Payment Codes":"By exploiting these \"xpub\" keys, we can construct something very similar to simple Direct Anonymous Payments where sender and receiver have something akin to a payment channel, that is, the ability to send each other funds to addresses that are unique to the context of the involved parties.Practically, the receiver would share their \"RAP Address\" (ie. a payment code) with the sender. They could do so publicly, eg. by posting it on their website to ask for donations. Since addresses derived from the code will be unique for each sender, this will not have any direct negative impact on privacy. A payment code mostly consists of an extended public key encoded with some additional metadata.For simplicity, the derivation path structure for BIP-47 is purposefully mirroring that of normal HD Wallets (BIP-44). To make a transaction, a sender first derives the 0th private key  of their own payment code at index i with m/47'/0'/i'/0/0. Then the sender derives the next unused (within sender-receiver-specific context) public key  at index j using the receiver's xpub key with the sub-path /0/j. Using the sender's private key (scalar) and the receiver's public key (point) we can now compute a shared secret . This shared secret is then used to compute the ephemeral public key  that will be used as the destination address for the transaction.For the receiver to be able to arrive at the same shared secret, the sender needs to communicate their own xpub key with them somehow. This is done using a special one-time \"notification\" that is basically establishing the payment channel. The notification is sent as a separate transaction to the receiver's public key derived from the path m/47'/0'/0'/0/0. Notification data is attached to the transaction using the OP_RETURN opcode.Transferring the sender's payment code as clear-text notification data would reveal that the sender and receiver are interacting with each other. Instead, we're once again computing a shared secret, this time using the private key used to sign the notification transaction and the public key derived for the notification address. This secret is then used to encrypt (XOR) the notification data.All the receiver has to do is monitor their notification address for transactions. Once they've received one, they can scalar-multiply the notification address' private key with the notification sender's public key to obtain the shared secret necessary to decrypt the notification data. Once the receiver has recovered the sender's payment code, it's possible to derive the 0th public key  and use it to arrive at the same shared secret . From this, the receiver can compute the private key  and is now able to make use of the funds sent to the ephemeral address.This scheme's biggest issue is the fact that the origin of the notification transaction can reveal the involved participants. Ensuring that the notification transaction is not easily associated with the sender's identity is difficult on purely transparent ledger blockchains. With Lelantus however, a fresh account can be anonymously funded with a partial private coin spend to pay the notification transaction without leaving traces.","hierarchical-ooomp#Hierarchical OOOMP":"Hierarchical One-Out-Of-Many Proofs are another research artifact developed and considered for the Lelantus Upgrade that never actually ended up being implemented. The main issue was that the soundness proof, that was published as part of the paper, turned out to be invalid. It should be mentioned that a practical vulnerability to this scheme was never identified either though, so it might be that the soundness proof is actually fixable and the scheme is sound. Further research into this was abandoned in favor of Curve Trees, so unless someone else picks this up, a fix is not on the horizon.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nTo get an intuition on how HOOOMP is different, remember that conventional One-Out-Of-Many Proofs basically work by communicating in Zero-Knowledge that the commitment we are able to open is at index  in the chronological list of all  commitments. Assuming that  the proof's transcript size increases linearly with , which is a logarithmic increase with  and therefore quite efficient. Unfortunately, proof generation and verification effort are both linear to the total amount of commitments , which requires restricting the anonymity set to a reasonable maximum.The reason for this is that individual exponentiations / ECC computations have to be made for each commitment in the set. Hierarchical One-Out-Of-Many Proofs improve this situation by running a cascade of OOOMPs on sub-sets instead. This is achieved without revealing the specific sub-set that the commitment in question is part of.As a simple example, let's assume we have a two-layer cascade for an overall  amount of commitments. By convention, we can declare that starting from the first commitment, we are dividing the anonymity set into  subsets of size  resulting in the same overall amount of commitments . The prover proceeds to blind the sub-set containing the commitment at index  and first demonstrates in Zero-Knowledge that the blinded sub-set is one of all  valid sub-sets. Finally, he proves that the commitment  is one of the  commitments within the blinded sub-set.Proving time for proof generation in this example only requires , which is significantly less compared to .","tech-tree#Tech-Tree":"Note that this Tech-Tree omits detailed dependencies that are not specific to the Lelantus upgrade to maintain readability.","references#References":"Miers, I., Garman, C., Green, M. and Rubin, A.D., 2013, May. Zerocoin: Anonymous distributed e-cash from bitcoin. In 2013 IEEE Symposium on Security and Privacy (pp. 397-411). IEEE.\nGroth, J. and Kohlweiss, M., 2015, April. One-out-of-many proofs: Or how to leak a secret and spend a coin. In Annual International Conference on the Theory and Applications of Cryptographic Techniques (pp. 253-280). Berlin, Heidelberg: Springer Berlin Heidelberg.\nBootle, J., Cerulli, A., Chaidos, P., Ghadafi, E., Groth, J. and Petit, C., 2015, September. Short accountable ring signatures based on DDH. In European Symposium on Research in Computer Security (pp. 243-265). Cham: Springer International Publishing.\nSchnorr, C.P., 1990. Efficient identification and signatures for smart cards. In Advances in Cryptology‚ÄîCRYPTO‚Äô89 Proceedings 9 (pp. 239-252). Springer New York.\nFiat, A. and Shamir, A., 1986, August. How to prove yourself: Practical solutions to identification and signature problems. In Conference on the theory and application of cryptographic techniques (pp. 186-194). Berlin, Heidelberg: Springer Berlin Heidelberg.\nReuben Yap, 2019, May. What is Sigma and why is it replacing Zerocoin in Zcoin?. https://firo.org/2019/03/20/what-is-sigma.html\nReuben Yap, 2019, April. Lelantus: Firo's next gen privacy protocol. https://firo.org/2019/04/14/lelantus-firo.html\nRuffing, T., Thyagarajan, S.A., Ronge, V. and Schroder, D., 2018, June. (Short Paper) Burning Zerocoins for Fun and for Profit-A Cryptographic Denial-of-Spending Attack on the Zerocoin Protocol. In 2018 Crypto Valley Conference on Blockchain Technology (CVCBT) (pp. 116-119). IEEE.\nJivanyan, A., 2019. Lelantus: Towards Confidentiality and Anonymity of Blockchain Transactions from Standard Assumptions. IACR Cryptol. ePrint Arch., 2019, p.373. https://eprint.iacr.org/archive/2019/373/20190604:053917\nJivanyan, A., 2020. Lelantus: A new design for anonymous and confidential cryptocurrencies. Cryptology ePrint Archive. https://eprint.iacr.org/archive/2019/373/20201109:090939\nAram Jivanyan. Lelantus: Private transactions with hidden origins and amounts based on DDH. https://lelantus.io/\nReuben Yap, October, 2019. Enabling Direct Untraceable Anonymous Payments in Lelantus. https://firo.org/2019/10/03/direct-untraceable-anonymous-lelantus.html\nReuben Yap, April, 2020. Zcoin releases paper on Hierarchical One-out-of-many Proofs. https://firo.org/2020/04/16/paper-on-hierarchical-one-out-of-many-proofs.html\nReuben Yap, June, 2020. Paving the way to privacy on by default (with opt-out) with Lelantus. https://firo.org/2020/06/11/paving-the-way-to-privacy-on-by-default-with-opt-out-with-lelantus.html\nReuben Yap, August, 2020. Lelantus Cryptographic Library Audit Results. https://firo.org/2020/08/13/lelantus-cryptographic-library-audit-results.html\nReuben Yap, June, 2021. Introducing Receiver Address Privacy for Recurring Firo Payments. https://firo.org/2021/06/09/introducing-receiver-address-privacy-for-firo.html\nAram Jivanyan, June, 2019. MoneroKon 2019 - Lelantus: New Protocol for Private Transactions with Hidden Origins and Amounts. Monero Community Workgroup\nYouTube channel. https://www.youtube.com/watch?v=gb53Fe2iuqg\nAram Jivanyan, November, 2019. SFBW19 - Lelantus A New Design for Privacy Cryptocurrencies - Aram Jivanyan. San Francisco Blockchain Week YouTube channel. https://www.youtube.com/watch?v=T3i01iKrV80\nSplineapple, March, 2021. Firo Frontier Ep03 Lelantus Reactivation. Firo YouTube channel. https://www.youtube.com/watch?v=KPPH4uSISnI\nBobby Ong, Reuben Yap, February, 2020. CoinGecko Podcast Ep. 5 - Interview with Reuben Yap, Project Steward of Zcoin. CoinGecko YouTube channel https://www.youtube.com/watch?v=RNgKb_xWm5s\nReuben Yap, May, 2020. Reuben Yap Monerotopia 2023 on Firos Lelantus Spark. The Crypto Show YouTube channel. https://www.youtube.com/watch?v=d3DhUrk-8To\nJivanyan, A. and Mamikonyan, T., 2020, August. Hierarchical one-out-of-many proofs with applications to blockchain privacy and ring signatures. In 2020 15th Asia Joint Conference on Information Security (AsiaJCIS) (pp. 74-81). IEEE.\nJivanyan, A. and Noether, S., 2022. Enabling untraceable anonymous payments in the Lelantus Protocol. White paper.\nRuffing, T., Thyagarajan, S.A., Ronge, V. and Schroder, D., 2018, June. Burning Zerocoins for Fun and for Profit-A Cryptographic Denial-of-Spending Attack on the Zerocoin Protocol. In 2018 Crypto Valley Conference on Blockchain Technology (CVCBT) (pp. 116-119). IEEE.\nJustus Ranvier. BIP-47 (Bitcoin Improvement Proposal) for Receiver Address Privacy. Reusable Payment Codes for Hierarchical Deterministic Wallets. https://github.com/bitcoin/bips/blob/master/bip-0047.mediawiki\nSplineapple, May, 2021. Firo Frontier Episode 10 Rap Addresses. Firo YouTube channel. https://www.youtube.com/watch?v=9Qk-X0vnV5M\nABDK Consulting, Dmitry Khovratovich, Ilya Kizhvatov, September, 2020. Lelantus Cryptographic Audit. https://firo.org/about/research/papers/lelantus-cryptography-audit-abdk.pdf\nTrail of Bits, Jim Miller, Will Song, Suhan Hussain, July, 2020. Lelantus Summary Report. https://github.com/trailofbits/publications/blob/master/reviews/zcoin-lelantus-summary.pdf\nReuben Yap, February, 2021. Lelantus disabled temporarily. https://forum.firo.org/t/lelantus-disabled-temporarily/1486"}},"/secureum-auditor-bootcamp-2021-quizzes":{"title":"The Secureum Bootcamp","data":{"when-is-the-next-bootcamp-gonna-start#When is the next bootcamp gonna start?":"TL;DR: It's running! It's online! You can start right away! Check #participate for details!Confused? Well, let me explain...The Secureum Bootcamp started out in October 2021 with \"Epoch 0\" and was divided into LEARN and CARE phase. The LEARN phase ran for 8 weeks and each week had its own \"slot\". Learning materials for each slot was released week by week and later tested in quizzes.In December, the 128 best scoring participants (from 1024 total participants) were invited to the CARE phase. Each participant was randomly assigned to one of four projects that partnered up with Secureum for an \"audit-like\" contest. During this contest participants would review the project's provided code (as one would during an audit) and provide a report of any findings which were aggregated into a single big report in the end.In 2022, with all of the learning material from Epoch 0 already available, Epoch Infinity was launched. The bootcamp is no longer guided, meaning no more weekly emails with learning material. Instead, participants are expected to learn the materials from Epoch 0 at their own pace. Then, once they feel ready, they can sign up for the next RACE. These are monthly quizzes that will test your knowledge compared to other participants. Sometimes the top scorers of these quizes will be invited to special events like CARE or CARE-X.\nYou can relive Bootcamp Epoch 0 in Discord to get the original experience or alternatively find a summary of relevant material in the Secureum Mindmap\nYou can practice for RACEs using the quiz write-ups listed below.\nOnce you feel ready to #participate in RACEs, don't forget to sign up. Pay attention to the Important Dates to see when registrations open and when the RACE will happen. Note that registration is only required once, if you already have a RUN code you only need to monitor the #announcements channel for the link to the quiz page. There will NOT be reminder e-mail about the monthly RACEs, you have to check the channels!","write-ups#Write-Ups":""}},"/web2/mongodb/nosql-operator-injection":{"title":"MongoDB NoSQL Operator Injection","data":{}},"/posts/2021/10/30/secureum-bootcamp-solidity-201-quiz":{"title":"Secureum Bootcamp Solidity 201 Quiz","data":{"":"This is a writeup of the Secureum Bootcamp Solidity 201 Quiz containing solutions and references to the provided study material.\nFor fairness, it was published after submissions to it were closed. The quiz consisted of 32 questions with a strict timelimit of 16 minutes. The ordering of the questions was randomized, so the numbering here won‚Äôt match with the numbering elsewhere.\nOctober 31, 2021 by patrickd","question-1-of-32#Question 1 of 32":"OpenZeppelin SafeERC20 is generally considered safer to use than ERC20 because\n A. It adds integer overflow/underflow checks\n B. It adds return value/data checks\n C. It adds pause/unpause capability\n D. It adds race-conditon checks\nCorrect is B.\nOpenZeppelin SafeERC20: Wrappers around ERC20 operations that throw on failure when the token contract implementation returns false. Tokens that return no value and instead revert or throw on failure are also supported with non-reverting calls assumed to be successful. Adds safeTransfer, safeTransferFrom, safeApprove, safeDecreaseAllowance, and safeIncreaseAllowance.\nfrom point 149 of Solidity 201 - by Secureum","question-2-of-32#Question 2 of 32":"The OpenZeppelin library that provides onlyOwner modifier\n A. Is Ownable\n B. Provides role based access control\n C. Provides a function to renounce ownership\n D. None of the above\nCorrect is A, C. See the source code on GitHub\nOpenZeppelin Ownable: provides a basic access control mechanism, where there is an account (an owner) that can be granted exclusive access to specific functions. By default, the owner account will be the one that deploys the contract. This can later be changed with transferOwnership. This module is used through inheritance. It will make available the modifier onlyOwner, which can be applied to your functions to restrict their use to the owner.\nfrom point 154 of Solidity 201 - by Secureum","question-3-of-32#Question 3 of 32":"OpenZeppelin ECDSA\n A. Implements functions for signature creation & verification\n B. Is susceptible to signature malleability\n C. Both A & B\n D. Neither A nor B\nCorrect is D. See the source code on GitHub\nOpenZeppelin ECDSA: provides functions for recovering and managing Ethereum account ECDSA signatures. These are often generated via web3.eth.sign, and are a 65 byte array (of type bytes in Solidity) arranged the following way: [[v (1)], [r (32)], [s (32)]]. The data signer can be recovered with ECDSA.recover, and its address compared to verify the signature. Most wallets will hash the data to sign and add the prefix '\\x19Ethereum Signed Message:\\n', so when attempting to recover the signer of an Ethereum signed message hash, you‚Äôll want to use toEthSignedMessageHash.\nfrom point 166 of Solidity 201 - by Secureum\nExternally Owned Accounts (EOA) can sign messages with their associated private keys, but currently contracts cannot.\nfrom point 168.1 of Solidity 201 - by Secureum","question-4-of-32#Question 4 of 32":"Which of the following is/are true about Solidity compiler 0.8.0?\n A. ABI coder v2 is made the default\n B. No opt-out primitives for default checked arithmetic\n C. Failing to assert returns the gas left instead of consuming all gas\n D. Exponentiation is made right associative\nCorrect is A, C, D.\nABI coder v2 is activated by default. You can choose to use the old behaviour using pragma abicoder v1;. The pragma pragma experimental ABIEncoderV2; is still valid, but it is deprecated and has no effect. If you want to be explicit, please use pragma abicoder v2; instead.\nfrom Solidity v0.8.0 Breaking Semantic Changes, point 142.2 of Solidity 201 - by Secureum\nArithmetic operations revert on underflow and overflow. You can use unchecked to use the previous wrapping behaviour.\nfrom Solidity v0.8.0 Breaking Semantic Changes, point 142.1 of Solidity 201 - by Secureum\nFailing assertions and other internal checks like division by zero or arithmetic overflow do not use the invalid opcode but instead the revert opcode. More specifically, they will use error data equal to a function call to Panic(uint256) with an error code specific to the circumstances. This will save gas on errors while it still allows static analysis tools to distinguish these situations from a revert on invalid input, like a failing require.\nfrom Solidity v0.8.0 Breaking Semantic Changes, point 142.4 of Solidity 201 - by Secureum\nExponentiation is right associative, i.e., the expression abc is parsed as a**(bc). Before 0.8.0, it was parsed as (ab)**c. This is the common way to parse the exponentiation operator.\nfrom Solidity v0.8.0 Breaking Semantic Changes, point 142.3 of Solidity 201 - by Secureum","question-5-of-32#Question 5 of 32":"EVM memory\n A. Is linear and byte-addressable\n B. Is reserved by Solidity until 0x7f\n C. Can be accessed in bytes using MLOAD8/MSTORE8\n D. Is non-volatile or persistent\nCorrect is A, B.\nEVM Memory: EVM memory is linear and can be addressed at byte level and accessed with MSTORE/MSTORE8/MLOAD instructions. All locations in memory are initialized as zero.\nfrom point 125 of Solidity 201 - by Secureum\nReserved Memory: Solidity reserves four 32-byte slots, with specific byte ranges (inclusive of endpoints) [...]: 0x60 - 0x7f (32 bytes): zero slot (The zero slot is used as initial value for dynamic memory arrays and should never be written to)\nfrom point 127 of Solidity 201 - by Secureum","question-6-of-32#Question 6 of 32":"Dappsys provides\n A. A proxy implementation\n B. A floating-point implementation with wad & ray\n C. A flexible authorization implementation\n D. All of the above\nCorrect is A, C.\nDappsys DSProxy: implements a proxy deployed as a standalone contract which can then be used by the owner to execute code.\nfrom point 193 of Solidity 201 - by Secureum\nDappsys DSMath: provides arithmetic functions for the common numerical primitive types of Solidity. You can safely add, subtract, multiply, and divide uint numbers without fear of integer overflow. You can also find the minimum and maximum of two numbers. Additionally, this package provides arithmetic functions for two new higher level numerical concepts called wad (18 decimals) and ray (27 decimals). These are used to represent fixed-point decimal numbers. A wad is a decimal number with 18 digits of precision and a ray is a decimal number with 27 digits of precision.\nfrom point 194 of Solidity 201 - by Secureum\nDappsys DSAuth: Provides a flexible and updatable auth pattern which is completely separate from application logic.\nfrom point 195 of Solidity 201 - by Secureum","question-7-of-32#Question 7 of 32":"Libraries are contracts\n A. That cannot have state variables\n B. That cannot be inherited\n C. That always require a delegatecall\n D. That are not meant to receive Ether\nCorrect is A, B, D.\nLibraries: They are deployed only once at a specific address and their code is reused using the DELEGATECALL opcode. This means that if library functions are called, their code is executed in the context of the calling contract. They use the library keyword.\nfrom point 103.3 of Solidity 201 - by Secureum\nLibrary Restrictions: In comparison to contracts, libraries are restricted in the following ways: They cannot have state variables, they cannot inherit nor be inherited, they cannot receive Ether.\nfrom point 113 of Solidity 201 - by Secureum\nLibrary functions can only be called directly (i.e. without the use of DELEGATECALL) if they do not modify the state (i.e. if they are view or pure functions), because libraries are assumed to be stateless\nfrom point 113.6 of Solidity 201 - by Secureum","question-8-of-32#Question 8 of 32":"ERC20 transferFrom(address sender, address recipient, uint256 amount){:solidity} (that follows the ERC20 spec strictly)\n A. Transfers token amount from sender to recipient\n B. sender must have given caller (msg.sender) approval for at least amount or more\n C. Deducts amount from sender's allowance\n D. Deducts amount from caller's (msg.senders's) allowance\nCorrect is A, B, D.\nMoves amount tokens from sender to recipient using the allowance mechanism. amount is then deducted from the caller‚Äôs allowance. Returns a boolean value indicating whether the operation succeeded. Emits a Transfer event.\nfrom point 148.j of Solidity 201 - by Secureum","question-9-of-32#Question 9 of 32":"ERC777 may be considered as an improved version of ERC20 because\n A. Hooks allow reacting to token mint/burn/transfer\n B. It can help avoid separate approve and transferFrom transactions\n C. It can help prevent tokens getting stuck in contracts\n D. It removes reentrancy risk\nCorrect is A, B, C.\nOpenZeppelin ERC777: Like ERC20, ERC777 is a standard for fungible tokens with improvements such as getting rid of the confusion around decimals, minting and burning with proper events, among others, but its killer feature is receive hooks. [...] A hook is simply a function in a contract that is called when tokens are sent to it, meaning accounts and contracts can react to receiving tokens. This enables a lot of interesting use cases, including atomic purchases using tokens (no need to do approve and transferFrom in two separate transactions), rejecting reception of tokens (by reverting on the hook call), redirecting the received tokens to other addresses, among many others. Furthermore, since contracts are required to implement these hooks in order to receive tokens, no tokens can get stuck in a contract that is unaware of the ERC777 protocol, as has happened countless times when using ERC20s.\nfrom point 152 of Solidity 201 - by Secureum","question-10-of-32#Question 10 of 32":"WETH is\n A. An ERC20 pre-compile for Wrapped Ether built into Ethereum protocol\n B. Warp Ether for super-fast Ether transfers\n C. Wrapped Ether to convert Ether into an ERC721 NFT\n D. None of the above\nCorrect is D.\nWETH: WETH stands for Wrapped Ether. For protocols that work with ERC-20 tokens but also need to handle Ether, WETH contracts allow converting Ether to its ERC-20 equivalent WETH (called wrapping) and vice-versa (called unwrapping). WETH can be created by sending ether to a WETH smart contract where the Ether is stored and in turn receiving the WETH ERC-20 token at a 1:1 ratio. This WETH can be sent back to the same smart contract to be ‚Äúunwrapped‚Äù i.e. redeemed back for the original Ether at a 1:1 ratio. The most widely used WETH contract is WETH9 which holds more than 7 million Ether for now.\nfrom point 198 of Solidity 201 - by Secureum","question-11-of-32#Question 11 of 32":"OpenZeppelin SafeCast\n A. Prevents underflows while downcasting\n B. Prevents overflows while downcasting\n C. Prevents underflows while upcasting\n D. Prevents overflows while upcasting\nCorrect is B.\nOpenZeppelin SafeCast: Wrappers over Solidity's uintXX/intXX casting operators with added overflow checks. Downcasting from uint256/int256 in Solidity does not revert on overflow. This can easily result in undesired exploitation or bugs, since developers usually assume that overflows raise errors. SafeCast restores this intuition by reverting the transaction when such an operation overflows.\nfrom point 177 of Solidity 201 - by Secureum","question-12-of-32#Question 12 of 32":"OpenZeppelin ERC20Pausable\n A. Adds ability to pause token transfers\n B. Adds ability to pause token minting and burning\n C. Provides modifiers whenPaused and whenNotPaused\n D. None of the above\nCorrect is A, B. Not C, because it inherits these modifiers from Pausable and doesn't implement them\nOpenZeppelin ERC20Pausable: ERC20 token with pausable token transfers, minting and burning. Useful for scenarios such as preventing trades until the end of an evaluation period, or having an emergency switch for freezing all token transfers in the event of a large bug.\nfrom point 148 Extensions of Solidity 201 - by Secureum\nOpenZeppelin Pausable: provides an emergency stop mechanism using functions pause and unpause that can be triggered by an authorized account. This module is used through inheritance. It will make available the modifiers whenNotPaused and whenPaused, which can be applied to the functions of your contract. Only the functions using the modifiers will be affected when the contract is paused or unpaused.\nfrom point 156 of Solidity 201 - by Secureum","question-13-of-32#Question 13 of 32":"CREATE2\n A. Deploys two contracts proxy and implementation concurrently\n B. Deploys contract at an address that can be predetermined\n C. Uses a salt and contract creationCode\n D. None of the above\nCorrect is B, C.\nOpenZeppelin Create2: makes usage of the CREATE2 EVM opcode easier and safer. CREATE2 can be used to compute in advance the address where a smart contract will be deployed [...].\nfrom point 163 of Solidity 201 - by Secureum\ndeploy(uint256 amount, bytes32 salt, bytes bytecode) ‚Üí address:{:solidity}: Deploys a contract using CREATE2.\nfrom point 163.1 of Solidity 201 - by Secureum","question-14-of-32#Question 14 of 32":"Name collision error with inheritance happens when the following pairs have the same name within a contract\n A. Function & modifier\n B. Function & event\n C. Function & function\n D. Event & modifier\nCorrect is A, B, D.\nName Collision Error: It is an error when any of the following pairs in a contract have the same name due to inheritance: 1) a function and a modifier 2) a function and an event 3) an event and a modifier.\nfrom point 112 of Solidity 201 - by Secureum","question-15-of-32#Question 15 of 32":"OpenZeppelin's (role-based) AccessControl library\n A. Provides support only for two specific: Owner and User\n B. Provides support for different roles with different authorization levels\n C. Provides support for granting and revoking roles\n D. None of the above\nCorrect is B, C.\nOpenZeppelin AccessControl: provides a general role based access control mechanism. Multiple hierarchical roles can be created and assigned each to multiple accounts. Roles can be used to represent a set of permissions. hasRole is used to restrict access to a function call. Roles can be granted and revoked dynamically via the grantRole and revokeRole functions which can only be called by the role‚Äôs associated admin accounts.\nfrom point 155 of Solidity 201 - by Secureum","question-16-of-32#Question 16 of 32":"OpenZeppelin's proxy implementations\n A. Typically have a proxy contract and an implementation contract\n B. Use delegatecalls from proxy to implementation\n C. Cannot support upgradable proxies\n D. None of the above\nCorrect is A, B.\nOpenZeppelin Proxy: This abstract contract provides a fallback function that delegates all calls to another contract using the EVM instruction delegatecall. We refer to the second contract as the implementation behind the proxy, and it has to be specified by overriding the virtual _implementation function.\nfrom point 185 of Solidity 201 - by Secureum\nOpenZeppelin ERC1967Proxy: implements an upgradeable proxy. It is upgradeable because calls are delegated to an implementation address that can be changed.\nfrom point 186 of Solidity 201 - by Secureum","question-17-of-32#Question 17 of 32":"Which of the following is/are true for a function f that has a modifier m?\n A. Function f cannot have another modifier because every function function can have at most one modifier\n B. Function f's code is inlined at the point of '_' within modifier m\n C. Function f reverts if '_' is not executed in the modifier m\n D. None of the above\nCorrect is B.\nFunction Modifiers: They can be used to change the behaviour of functions in a declarative way. For example, you can use a modifier to automatically check a condition prior to executing the function. The function‚Äôs control flow continues after the ‚Äú_‚Äù in the preceding modifier. Multiple modifiers are applied to a function by specifying them in a whitespace-separated list and are evaluated in the order presented. The modifier can choose not to execute the function body at all and in that case the return variables are set to their default values just as if the function had an empty body. The _ symbol can appear in the modifier multiple times. Each occurrence is replaced with the function body.\nfrom point 22 of Solidity 101 - by Secureum","question-18-of-32#Question 18 of 32":"Zero address check is typically recommended because\n A. The use of zero address for transfers will trigger an EVM exception\n B. Ether/tokens sent to zero address will be inaccessible\n C. Ether/tokens sent to zero address can be accessed by anyone\n D. Address 0 is the Ethereum Masternode account and is forbidden for access\nCorrect is B.\nZero Address Check: address(0) which is 20-bytes of 0‚Äôs is treated specially in Solidity contracts because the private key corresponding to this address is unknown. Ether and tokens sent to this address cannot be retrieved and setting access control roles to this address also won‚Äôt work (no private key to sign transactions). Therefore zero addresses should be used with care and checks should be implemented for user-supplied address parameters.\nfrom point 144 of Solidity 201 - by Secureum","question-19-of-32#Question 19 of 32":"Solidity supports\n A. Multiple inheritance\n B. Polymorphism\n C. Contract overloading\n D. Function overloading\nCorrect is A, B, D. No such things as C.\nSolidity supports multiple inheritance including polymorphism\nfrom point 102 of Solidity 201 - by Secureum\nFunction Overloading: A contract can have multiple functions of the same name but with different parameter types. This process is called ‚Äúoverloading.‚Äù\nfrom point 25 of Solidity 101 - by Secureum","question-20-of-32#Question 20 of 32":"OpenZeppelin ERC721\n A. Implements the NFT standard\n B. safeTransferFrom(..) checks for zero-addresses\n C. approve(..) is susceptible to race-condition just like ERC20\n D. setApprovalForAll(address operator, bool _approved) approves/removes operator for all of caller's tokens\nCorrect is A, B, D. Not C, since approval is not susceptible since approval can only given or taken away for a single token, so changing approval doesn't allow stealing more than was already approved.\nOpenZeppelin ERC721: Implements the popular ERC721 Non-Fungible Token Standard.\nfrom point 151 of Solidity 201 - by Secureum\nsafeTransferFrom(..): Safely transfers tokenId token from from to to, checking first that contract recipients are aware of the ERC721 protocol to prevent tokens from being forever locked. Requirements: 1) from cannot be the zero address [...]\nfrom point 151.4 of Solidity 201 - by Secureum\nsetApprovalForAll(address operator, bool _approved): Approve or remove operator as an operator for the caller. Operators can call transferFrom or safeTransferFrom for any token owned by the caller.\nfrom point 151.7 of Solidity 201 - by Secureum","question-21-of-32#Question 21 of 32":"For contract A {uint256 i; bool b1; bool b2; address a1;}{:solidity} the number of storage slots used is:\n A. 4\n B. 3\n C. 2\n D. 1\nCorrect is C. The uint256 takes a full slot, the bools (each 1 byte) and the address (20 bytes) can packed into the same slot\nStorage Layout: State variables of contracts are stored in storage in a compact way such that multiple values sometimes use the same storage slot. Except for dynamically-sized arrays and mappings, data is stored contiguously item after item starting with the first state variable, which is stored in slot 0\nfrom point 115 of Solidity 201 - by Secureum\nStorage Layout Packing: For each state variable, a size in bytes is determined according to its type. Multiple, contiguous items that need less than 32 bytes are packed into a single storage slot if possible, according to the following rules: [...] If a value type does not fit the remaining part of a storage slot, it is stored in the next storage slot\nfrom point 116 of Solidity 201 - by Secureum","question-22-of-32#Question 22 of 32":"Which of the following is/are generally true about storage layouts?\n A. The number of slots used for a contract depends on the ordering of state variable declarations\n B. The slots for struct elements are consecutive\n C. The slot s for dynamic array contains the length with individual elements stored consecutively in slots starting at keccak256(s)\n D. The slot s for mapping is empty with individual values stored consecutively in slots starting at keccak(h(k).s) where k is the first key and h is a hash function that depends on type of k\nCorrect is A, B, C. For mappings the slots are unique for each key, they're not consecutive.\nStorage Layout & Ordering: Ordering of storage variables and struct members affects how they can be packed tightly. For example, declaring your storage variables in the order of uint128, uint128, uint256 instead of uint128, uint256, uint128, as the former will only take up two slots of storage whereas the latter will take up three.\nfrom point 120 of Solidity 201 - by Secureum\nStorage Layout & Structs/Arrays: [...] The elements of structs and arrays are stored after each other, just as if they were given as individual values.\nfrom point 117 of Solidity 201 - by Secureum\nStorage Layout for Dynamic Arrays: If the storage location of the array ends up being a slot p after applying the storage layout rules, this slot stores the number of elements in the array (byte arrays and strings are an exception). Array data is located starting at keccak256(p) and it is laid out in the same way as statically-sized array data would: One element after the other, potentially sharing storage slots if the elements are not longer than 16 bytes.\nfrom point 122 of Solidity 201 - by Secureum\nStorage Layout for Mappings: For mappings, the slot stays empty, but it is still needed to ensure that even if there are two mappings next to each other, their content ends up at different storage locations. The value corresponding to a mapping key k is located at keccak256(h(k) . p) where . is concatenation and h is a function that is applied to the key depending on its type [...]\nfrom point 123 of Solidity 201 - by Secureum","question-23-of-32#Question 23 of 32":"Assuming all contracts C1, C2 and C3 define explicit constructors in contract C1 is C2, C3{:solidity}  and both C2 and C3 don't inherit contracts, the number & order of constructor(s) executed is/are\n A. One, that of C1\n B. Three, in the order C2, C3, C1\n C. One, that of C3\n D. Three, in the order C1, C2, C3\nCorrect is B.\nBase Constructors: The constructors of all the base contracts will be called following the linearization rules.\nfrom point 111 of Solidity 201 - by Secureum\nStorage Layout & Inheritance: For contracts that use inheritance, the ordering of state variables is determined by the C3-linearized order of contracts starting with the most base-ward contract. If allowed by the above rules, state variables from different contracts do share the same storage slot.\nfrom point 118 of Solidity 201 - by Secureum","question-24-of-32#Question 24 of 32":"OpenZeppelin SafeMath\n A. Prevents integer overflow/underflow at compile-time\n B. Is not required if using Solidity compiler version >= 0.8.0\n C. Both A & B\n D. Neither A nor B\nCorrect is B. Not A, because it does not prevent them at compile- but at runtime. It can be argued it's not B since it's a recommendation and not a requirement.\nOpenZeppelin SafeMath: provides mathematical functions that protect your contract from overflows and underflows.\nfrom point 175 of Solidity 201 - by Secureum\nOverflow/Underflow Check: Until Solidity version 0.8.0 which introduced checked arithmetic by default, arithmetic was unchecked and therefore susceptible to overflows and underflows which could lead to critical vulnerabilities. The recommended best-practice for such contracts is to use OpenZeppelin‚Äôs SafeMath > library for arithmetic.\nfrom point 146 of Solidity 201 - by Secureum","question-25-of-32#Question 25 of 32":"Which of the following is/are not allowed?\n A. Function overriding\n B. Function overloading\n C. Modifier overloading\n D. Modifier overriding\nCorrect is C.\nFunction Overriding: Base functions can be overridden by inheriting contracts to change their behavior if they are marked as virtual. The overriding function must then use the override keyword in the function header.\nfrom point 102.3 of Solidity 201 - by Secureum\nFunction Overloading: A contract can have multiple functions of the same name but with different parameter types. This process is called ‚Äúoverloading.‚Äù\nfrom point 25 of Solidity 101 - by Secureum\nModifier Overriding: Function modifiers can override each other. This works in the same way as function overriding (except that there is no overloading for modifiers).\nfrom point 110 of Solidity 201 - by Secureum","question-26-of-32#Question 26 of 32":"Which of the following is/are true about abstract contracts and interfaces?\n A. Abstract contracts have at least one function undefined\n B. Interfaces can have some functions defined\n C. Unimplemented functions in abstract contracts need to be declared virtual\n D. All functions are implicitly virtual in interfaces\nCorrect is A, C, D. Note that A isn't necessarily correct since abstract classes can have all functions defined.\nAbstract Contracts: Contracts need to be marked as abstract when at least one of their functions is not implemented. They use the abstract keyword.\nfrom point 103.1 of Solidity 201 - by Secureum\nInterfaces: They cannot have any functions implemented.\nfrom point 103.2 of Solidity 201 - by Secureum\nVirtual Functions: Functions without implementation have to be marked virtual outside of interfaces. In interfaces, all functions are automatically considered virtual. Functions with private visibility cannot be virtual.\nfrom point 108 of Solidity 201 - by Secureum","question-27-of-32#Question 27 of 32":"Storage layout\n A. Refers to the layout of state variables in storage\n B. Is organized in 256-byte slots\n C. Is packed for value types that use less than 32 bytes\n D. Always starts on a new slot for reference types\nCorrect is A, C, D. Not B, because it's 256-bit slots, not Byte.\nEVM Storage: Storage is a key-value store that maps 256-bit words to 256-bit words and is accessed with EVM‚Äôs SSTORE/SLOAD instructions. All locations in storage are initialized as zero.\nfrom point 114 of Solidity 201 - by Secureum\nStorage Layout Packing: For each state variable, a size in bytes is determined according to its type. Multiple, contiguous items that need less than 32 bytes are packed into a single storage slot if possible, according to the following rules: [...] Value types use only as many bytes as are necessary to store them\nfrom point 116 of Solidity 201 - by Secureum","question-28-of-32#Question 28 of 32":"Which of the following EVM instruction(s) do(es) not touch EVM storage?\n A. SLOAD\n B. MSTORE8\n C. SSTORE\n D. SWAP\nCorrect is B, D.\nEVM Storage: Storage is a key-value store that maps 256-bit words to 256-bit words and is accessed with EVM‚Äôs SSTORE/SLOAD instructions. All locations in storage are initialized as zero.\nfrom point 114 of Solidity 201 - by Secureum\nEVM Memory: EVM memory is linear and can be addressed at byte level and accessed with MSTORE/MSTORE8/MLOAD instructions. All locations in memory are initialized as zero.\nfrom point 125 of Solidity 201 - by Secureum\nStack is made up of 1024 256-bit elements. EVM instructions can operate with the top 16 stack elements. Most EVM instructions operate with the stack (stack-based architecture) and there are also stack-specific operations e.g. PUSH, POP, SWAP, DUP etc.\nfrom point 60 of Ethereum 101 - by Secureum","question-29-of-32#Question 29 of 32":"Proxied contracts\n A. Should use constructors in implementation contract to initialize the proxy's state variables\n B. Should use an external/public initialize() function\n C. Should have their initialize() function called only once\n D. All of the above\nCorrect is B, C.\nOpenZeppelin Initializable: aids in writing upgradeable contracts, or any kind of contract that will be deployed behind a proxy. Since a proxied contract cannot have a constructor, it is common to move constructor logic to an external initializer function, usually called initialize. It then becomes necessary to protect this initializer function so it can only be called once. The initializer modifier provided by this contract will have this effect.To avoid leaving the proxy in an uninitialized state, the initializer function should be called as early as possible by providing the encoded function call as the _data argument. When used with inheritance, manual care must be taken to not invoke a parent initializer twice, or to ensure that all initializers are idempotent. This is not verified automatically as constructors are by Solidity.\nfrom point 192 of Solidity 201 - by Secureum","question-30-of-32#Question 30 of 32":"OpenZeppelin's ReentrancyGuard library mitigates reentrancy risk in a contract\n A. For all its functions by simply deriving/inheriting from it\n B. Only for functions that apply the nonReentrant modifier\n C. By enforcing a checks-effects-interactions pattern in its functions\n D. None of the above\nCorrect is B.\nOpenZeppelin ReentrancyGuard: prevents reentrant calls to a function. Inheriting from ReentrancyGuard will make the nonReentrant modifier available, which can be applied to functions to make sure there are no nested (reentrant) calls to them.\nfrom point 73 of Solidity 201 - by Secureum","question-31-of-32#Question 31 of 32":"EVM inline assembly has\n A. Its own language Yul\n B. Safety checks just like Solidity\n C. Access to all variables in the contract and function where present\n D. References to variables as their addresses not values\nCorrect is A, C, D.\nInline Assembly: Inline assembly is a way to access the Ethereum Virtual Machine at a low level. This bypasses several important safety features and checks of Solidity. You should only use it for tasks that need it, and only if you are confident with using it. The language used for inline assembly in Solidity is called Yul.\nfrom point 132 of Solidity 201 - by Secureum\nInline Assembly Access to External Variables, Functions and Libraries: You can access Solidity variables and other identifiers by using their name. Local variables of value type are directly usable in inline assembly. Local variables that refer to memory/calldata evaluate to the address of the variable in memory/calldata and not the value itself [...]\nfrom point 133 of Solidity 201 - by Secureum","question-32-of-32#Question 32 of 32":"If OpenZeppelin's isContract(address){:solidity} returns false for an address then\n A. Address is guaranteed to not be a contract\n B. Codesize at address is 0 at time of invocation\n C. Both A & B\n D. Neither A nor B\nCorrect is B.\nReturns true if account is a contract. It is unsafe to assume that an address for which this function returns false is an externally-owned account (EOA) and not a contract. Among others, isContract will return false for the following types of addresses: 1) an externally-owned account 2) a contract in construction 3) an address where a contract will be created 4) an address where a contract lived, but was destroyed\nfrom point 159.1 of Solidity 201 - by Secureum"}},"/posts/2021/10/24/secureum-bootcamp-solidity-101-quiz":{"title":"Secureum Bootcamp Solidity 101 Quiz","data":{"":"This is a writeup of the Secureum Bootcamp Solidity 101 Quiz containing solutions and references to the provided study material.\nFor fairness, it was published after submissions to it were closed. The quiz consisted of 32 questions with a strict timelimit of 16 minutes. The ordering of the questions was randomized, so the numbering here won‚Äôt match with the numbering elsewhere.\nOctober 24, 2021 by patrickd","question-1-of-32#Question 1 of 32":"User from EOA A calls Contract C1 which makes an external call (CALL opcode) to Contract C2. Which of the following is/are true?\n A. tx.origin in C2 returns A's address \n B. msg.sender in C2 returns A's address \n C. msg.sender in C1 returns A's address \n D. msg.value in C2 returns amount of Wei sent from A \nCorrect is A, C.\nBlock and Transaction Properties:\nmsg.sender (address): sender of the message (current call)\nmsg.value (uint): number of wei sent with the message\ntx.origin (address): sender of the transaction (full call chain)\nfrom point 73 of Solidity 101 - by Secureum\nThe values of all members of msg, including msg.sender and msg.value can change for every external function call. This includes calls to library functions.\nfrom point 74 of Solidity 101 - by Secureum","question-2-of-32#Question 2 of 32":"Which of the following is/are true for call/delegatecall/staticcall primitives?\n A. They are used to call contracts \n B. They only revert without returning success/failure \n C. Delegatecall retains the msg.sender and msg.value of caller contract \n D. Staticcall reverts if the called contract reads contract state of caller \nCorrect is A, C.\nCall/Delegatecall/Staticcall: In order to interface with contracts that do not adhere to the ABI, or to get more direct control over the encoding, the functions call, delegatecall and staticcall are provided. They all take a single bytes memory parameter and return the success condition (as a bool) and the returned data (bytes memory). With delegatecall, only the code of the given address is used but all other aspects (storage, balance, msg.sender etc.) are taken from the current contract. The purpose of delegatecall is to use library/logic code which is stored in callee contract but operate on the state of the caller contract. With staticcall, the execution will revert if the called function modifies the state in any way\nfrom point 49 of Solidity 101 - by Secureum","question-3-of-32#Question 3 of 32":"The gas left in the current transaction can be obtained with\n A. tx.gas() \n B. gasleft() \n C. msg.gas() \n D. block.gaslimit() \nCorrect is B.\nBlock and Transaction Properties:\nblock.gaslimit (uint): current block gaslimit\ntx.gasprice (uint): gas price of the transaction\ngasleft() returns (uint256): remaining gas.\nfrom point 73 of Solidity 101 - by Secureum\nThe function gasleft was previously known as msg.gas, which was deprecated in version 0.4.21 and removed in version 0.5.0.\nfrom Block and Transaction Properties of Units and Globally Available Variables - Solidity Docs","question-4-of-32#Question 4 of 32":"The default value of\n A. Bool is false \n B. Address is 0 \n C. Statically-sized array depends on the underlying type \n D. Enum is its first member \nCorrect is A, B, C, D.\nDefault Values: A variable which is declared will have an initial default value whose byte-representation is all zeros. The ‚Äúdefault values‚Äù of variables are the typical ‚Äúzero-state‚Äù of whatever the type is. For example, the default value for a bool is false. The default value for the uint or int types is 0. For statically-sized arrays and bytes1 to bytes32, each individual element will be initialized to the default value corresponding to its type. For dynamically-sized arrays, bytes and string, the default value is an empty array or string. For the enum type, the default value is its first member.\nfrom point 39 of Solidity 101 - by Secureum","question-5-of-32#Question 5 of 32":"Which of the following is/are true about events?\n A. Events are meant for off-chain applications \n B. Events can be accessed only by the emitting contract \n C. Indexing event parameters creates searchable topics \n D. A maximum of three events can have indexed parameters \nCorrect is A, C.\nEvents: They are an abstraction on top of the EVM‚Äôs logging functionality. Emitting events cause the arguments to be stored in the transaction‚Äôs log - a special data structure in the blockchain. These logs are associated with the address of the contract, are incorporated into the blockchain, and stay there as long as a block is accessible. The Log and its event data is not accessible from within contracts (not even from the contract that created them). Applications can subscribe and listen to these events through the RPC interface of an Ethereum client.\nfrom point 27 of Solidity 101 - by Secureum\nIndexed Event Parameters: Adding the attribute indexed for up to three parameters adds them to a special data structure known as ‚Äútopics‚Äù instead of the data part of the log. If you use arrays (including string and bytes) as indexed arguments, its Keccak-256 hash is stored as a topic instead, this is because a topic can only hold a single word (32 bytes). All parameters without the indexed attribute are ABI-encoded into the data part of the log. Topics allow you to search for events, for example when filtering a sequence of blocks for certain events. You can also filter events by the address of the contract that emitted the event.\nfrom point 28 of Solidity 101 - by Secureum","question-6-of-32#Question 6 of 32":"Function foo() uses block.number. Which of the following is/are always true about foo()?\n A. It should be marked as pure \n B. It should be marked as view \n C. It should be marked as payable \n D. Cannot determine mutability based only on this information \nCorrect is D. Since it wouldn't always be true unless you know what other things foo() uses too.\nFunction Mutability Specifiers: Functions can be specified as being pure or view:\nview functions can read contract state but cannot modify it. This is enforced at runtime via STATICCALL opcode. The following are considered state modifying: 1) Writing to state variables 2) Emitting events 3) Creating other contracts 4) Using selfdestruct 5) Sending Ether via calls 6) Calling any function not marked view or pure 7) Using low-level calls 8) Using inline assembly that contains certain opcodes.pure functions can neither read contract state nor modify it. The following are considered reading from state: 1) Reading from state variables 2) Accessing address(this).balance or address.balance 3) Accessing any of the members of block, tx, msg (with the exception of msg.sig and msg.data) 4) Calling any function not marked pure 5) Using inline assembly that contains certain opcodes.It is not possible to prevent functions from reading the state at the level of the EVM. It is only possible to prevent them from writing to the state via STATICCALL. Therefore, only view can be enforced at the EVM level, but not pure.\nfrom point 24 of Solidity 101 - by Secureum","question-7-of-32#Question 7 of 32":"Solidity functions\n A. Can be declared only inside contracts \n B. Can have named return variables \n C. Can have unnamed parameters \n D. Can be recursive \nCorrect is B, C, D.\nFree Functions: Functions that are defined outside of contracts are called ‚Äúfree functions‚Äù and always have implicit internal visibility. Their code is included in all contracts that call them, similar to internal library functions.\nfrom point 26 of Solidity 101 - by Secureum\nFunction Return Variables: Function return variables are declared with the same syntax after the returns keyword. The names of return variables can be omitted. Return variables can be used as any other local variable and they are initialized with their default value and have that value until they are (re-)assigned.\nfrom point 21 of Solidity 101 - by Secureum\nFunction parameters: Function parameters are declared the same way as variables, and the name of unused parameters can be omitted. Function parameters can be used as any other local variable and they can also be assigned to.\nfrom point 20 of Solidity 101 - by Secureum\nVariables and other items declared outside of a code block, for example functions, contracts, user-defined types, etc., are visible even before they were declared. This means you can use state variables before they are declared and call functions recursively.\nfrom point 40 of Solidity 101 - by Secureum","question-8-of-32#Question 8 of 32":"Conversions in Solidity have the following behavior\n A. Implicit conversions are never allowed \n B. Explicit conversions of uint16 to uint8 removes the higher-order bits \n C. Explicit conversion of uint16 to uint32 adds lower-order padding \n D. Explicit conversions are checked by compiler for safety \nCorrect is B.\nImplicit Conversions: An implicit type conversion is automatically applied by the compiler in some cases during assignments, when passing arguments to functions and when applying operators. Implicit conversion between value-types is possible if it makes sense semantically and no information is lost.\nfrom point 68 of Solidity 101 - by Secureum\nExplicit Conversions: If the compiler does not allow implicit conversion but you are confident a conversion will work, an explicit type conversion is sometimes possible. This may result in unexpected behaviour and allows you to bypass some security features of the compiler e.g. int to uint. If an integer is explicitly converted to a smaller type, higher-order bits are cut off. If an integer is explicitly converted to a larger type, it is padded on the left (i.e., at the higher order end).\nfrom point 69 of Solidity 101 - by Secureum","question-9-of-32#Question 9 of 32":"When Contract A attempts to make a delegatecall to Contract B but a prior transaction to Contract B has executed a selfdestruct\n A. The delegatecall reverts \n B. The delegatecall returns a failure \n C. The delegatecall returns a success \n D. This scenario is not practically possible \nCorrect is C.\nThe low-level functions call, delegatecall and staticcall return true as their first return value if the account called is non-existent, as part of the design of the EVM. Account existence must be checked prior to calling if needed.\nfrom point 87 of Solidity 101 - by Secureum","question-10-of-32#Question 10 of 32":"If a = 1 then which of the following is/are true?\n A. a += 1 makes the value of a = 2 \n B. b = ++a makes the value of b = 1 \n C. a -= 1 makes the value of a = 1 \n D. b = a-- makes the value of b = 1 \nCorrect is A, D.\nOperators Involving LValues (i.e. a variable or something that can be assigned to)\na += e is equivalent to a = a + e. The operators -=, *=, /=, %=, |=, &= and ^= are defined accordingly\na++ and a-- are equivalent to a += 1 / a -= 1 but the expression itself still has the previous value of a\nIn contrast, --a and ++a have the same effect on a but return the value after the change\nfrom point 66 of Solidity 101 - by Secureum","question-11-of-32#Question 11 of 32":"transfer and send primitives\n A. Are used for Ether transfers \n B. Trigger the receive() or fallback() functions of address \n C. Always return a value to be checked \n D. Provide only 2300 gas \nCorrect is A, B, D.\nThe receive function is executed on a call to the contract with empty calldata. This is the function that is executed on plain Ether transfers via .send() or .transfer(). In the worst case, the receive function can only rely on 2300 gas being available (for example when send or transfer is used), leaving little room to perform other operations except basic logging.\nfrom point 33 of Solidity 101 - by Secureum\n.transfer(uint256 amount) [no return value]: send given amount of Wei to Address, reverts on failure, forwards 2300 gas stipend, not adjustable\n.send(uint256 amount) returns (bool): send given amount of Wei to Address, returns false on failure, forwards 2300 gas stipend, not adjustable\nfrom point X of Solidity 101 - by Secureum","question-12-of-32#Question 12 of 32":"A contract can receive Ether via\n A. msg.value to payable functions \n B. selfdestruct destination \n C. coinbase transaction \n D. receive() or fallback() functions \nCorrect is A, B, C, D.\nA contract without a receive Ether function can receive Ether as a recipient of a coinbase transaction (aka miner block reward) or as a destination of a selfdestruct. A contract cannot react to such Ether transfers and thus also cannot reject them. This means that address(this).balance can be higher than the sum of some manual accounting implemented in a contract (i.e. having a counter updated in the receive Ether function).\nfrom point 33.3 of Solidity 101 - by Secureum\nReceive Function: A contract can have at most one receive function, declared using receive() external payable without the function keyword. This is the function that is executed on plain Ether transfers via .send() or .transfer().\nfrom point 33 of Solidity 101 - by Secureum\nFallback Function: A contract can have at most one fallback function, declared using either fallback () external [payable] or fallback (bytes calldata _input) external [payable] returns (bytes memory _output), both without the function keyword. This function must have external visibility. The fallback function always receives data, but in order to also receive Ether it must be marked payable. In the worst case, if a payable fallback function is also used in place of a receive function, it can only rely on 2300 gas being available.\nfrom point 34 of Solidity 101 - by Secureum\nA Error(string) exception (or an exception without data) is generated in the following situations: If your contract receives Ether via a public function without payable modifier (including the constructor and the fallback function)\nfrom point 91 of Solidity 101 - by Secureum","question-13-of-32#Question 13 of 32":"Structs in Solidity\n A. Are user-defined type \n B. Are reference types \n C. Can contain or be contained in arrays and mappings \n D. None of the above \nCorrect is A, B, C.\nStruct Types: They are custom defined types that can group several variables of same/different types together to create a custom data structure. The struct members are accessed using ‚Äò.‚Äô e.g.: struct s {address user; uint256 amount} where s.user and s.amount access the struct members.\nfrom point 30 of Solidity 101 - by Secureum\nMapping Types: Mappings define key-value pairs and are declared using the syntax mapping(_KeyType => _ValueType) _VariableName. The _KeyType can be any built-in value type, bytes, string, or any contract or enum type. Other user-defined or complex types, such as mappings, structs or array types are not allowed. _ValueType can be any type, including mappings, arrays and structs.\nfrom point 65 of Solidity 101 - by Secureum","question-14-of-32#Question 14 of 32":"The following is/are true about ecrecover primitive\n A. Takes a message hash and ECDSA signature values as inputs \n B. Recovers and returns the public key of the signature \n C. Is susceptible to malleable signatures \n D. None of the above \nCorrect is A, C. Although internally it first recovers the public key from the signature, it actually returns the address derived from the public key.\necrecover(bytes32 hash, uint8 v, bytes32 r, bytes32 s) returns (address): recover the address associated with the public key from elliptic curve signature or return zero on error. The function parameters correspond to ECDSA values of the signature: r = first 32 bytes of signature, s = second 32 bytes of signature, v = final 1 byte of signature. ecrecover returns an address, and not an address payable.\nfrom point 79.6 of Solidity 101 - by Secureum\nIf you use ecrecover, be aware that a valid signature can be turned into a different valid signature without requiring knowledge of the corresponding private key. This is usually not a problem unless you require signatures to be unique or use them to identify items. OpenZeppelin has a ECDSA helper library that you can use as a wrapper for ecrecover without this issue.\nfrom point 80 of Solidity 101 - by Secureum","question-15-of-32#Question 15 of 32":"Which of the following is/are valid control structure(s) in Solidity (excluding YUL)?\n A. if \n B. else \n C. elif \n D. switch \nCorrect is A, B. \"elif\" is specific to Python and switch has not been implemented in Solidity yet\nControl Structures: Solidity has if, else, while, do, for, break, continue, return, with the usual semantics known from C or JavaScript\nfrom point 85 of Solidity 101 - by Secureum","question-16-of-32#Question 16 of 32":"Address types\n A. Can always receive Ether \n B. Have members for balance, call, code \n C. Can be converted to uint160 or contract types \n D. Can be added and subtracted \nCorrect is B, C.\nAddress Type: The address type comes in two types: (1) address: Holds a 20 byte value (size of an Ethereum address) (2) address payable: Same as address, but with the additional members transfer and send. address payable is an address you can send Ether to, while a plain address cannot be sent Ether.\nfrom point 45 of Solidity 101 - by Secureum\nMembers of Address Type:\naddress.balance (uint256): balance of the Address in Wei\naddress.code (bytes memory): code at the Address (can be empty)\naddress.call(bytes memory) returns (bool, bytes memory): issue low-level CALL with the given payload, returns success condition and return data, forwards all available gas, adjustable\nfrom point 46 of Solidity 101 - by Secureum\nConversions: Implicit conversions from address payable to address are allowed, whereas conversions from address to address payable must be explicit via payable(address). Explicit conversions to and from address are allowed for uint160, integer literals, bytes20 and contract types.\nfrom point 45.2 of Solidity 101 - by Secureum\nTypeError: Operator - not compatible with types address and int_const 1. Arithmetic operations on addresses are not supported. Convert to integer first before using them.\nfrom playing around with remix Remix","question-17-of-32#Question 17 of 32":"If the previous block number was 1000 on Ethereum mainnet, which of the following is/are true?\n A. block.number is 1001 \n B. blochhash(1) returns 0 \n C. block.chainID returns 1 \n D. `block.timestamp`` returns the number of seconds since last block \nCorrect is A, B, C. Block number is the number of the block that is currently being mined, the next one. Block number 1 was too long ago and its hash can no longer be accessed due to scaling reasons. Mainnet ID Chain is 1.\nBlock and Transaction Properties:\nblockhash(uint blockNumber) returns (bytes32): hash of the given block - only works for 256 most recent, excluding current, blocks\nblock.chainid (uint): current chain id\nblock.number (uint): current block number\nblock.timestamp (uint): current block timestamp as seconds since unix epoch\nfrom point 73 of Solidity 101 - by Secureum","question-18-of-32#Question 18 of 32":"If we have an array then its data location can be\n A. memory and its persistence/scope will be the function of declaration \n B. storage and its persistence/scope will be the entire contract \n C. calldata and it will only be readable \n D. None of the above \nCorrect is A, B, C.\nReference Types & Data Location: Every reference type has an additional annotation ‚Äî the data location where it is stored. There are three data locations: memory, storage and calldata.\nmemory: whose lifetime is limited to an external function call\nstorage: whose lifetime is limited to the lifetime of a contract and the location where the state variables are stored\ncalldata: which is a non-modifiable, non-persistent area where function arguments are stored and behaves mostly like memory. It is required for parameters of external functions but can also be used for other variables.\nfrom point 55 of Solidity 101 - by Secureum","question-19-of-32#Question 19 of 32":"delete varName; has which of the following effects?\n A. varName becomes 0 if varName is an integer \n B. varName becomes true if varName is a boolean \n C. No effect if varName is a mapping \n D. Resets all struct members to their default values irrespective of their types \nCorrect is A, C.\ndelete a assigns the initial value for the type to a\nFor integers it is equivalent to a = 0\nFor structs, it assigns a struct with all members reset\ndelete has no effect on mappings. So if you delete a struct, it will reset all members that are not mappings and also recurse into the members unless they are mappings.\nfrom point 67 of Solidity 101 - by Secureum\nDefault Values: A variable which is declared will have an initial default value whose byte-representation is all zeros. The ‚Äúdefault values‚Äù of variables are the typical ‚Äúzero-state‚Äù of whatever the type is. For example, the default value for a bool is false.\nfrom point 39 of Solidity 101 - by Secureum","question-20-of-32#Question 20 of 32":"Which of the following is/are valid function specifier(s)?\n A. internal \n B. pure \n C. payable \n D. immutable \nCorrect is A, B, C.\nFunction Visibility Specifiers: Functions have to be specified as being public, external, internal or private\nfrom point 23 of Solidity 101 - by Secureum\nFunction Mutability Specifiers: Functions can be specified as being pure or view\nfrom point 24 of Solidity 101 - by Secureum\nIf your contract receives Ether via a public function without payable modifier (including the constructor and the fallback function)\nfrom point 91.3 of Solidity 101 - by Secureum\nState Variables can be declared as constant or immutable.\nfrom point 17.1 of Solidity 101 - by Secureum","question-21-of-32#Question 21 of 32":"Function visibility\n A. Goes from private-internal-external-public in decreasing restrictive order (i.e. private being the most restrictive) \n B. Goes from internal-private-external-public in decreasing restrictive order (i.e. internal being the most restrictive) \n C. May be omitted to default to internal in the latest 0.8.0+ compiler versions \n D. None of the above \nCorrect is A. Default visibility was public but is required in current Solidity versions.\nFunction Visibility Specifiers: Functions have to be specified as being public, external, internal or private:\npublic: Public functions are part of the contract interface and can be either called internally or via messages.\nexternal: External functions are part of the contract interface, which means they can be called from other contracts and via transactions. An external function f cannot be called internally (i.e. f() does not work, but this.f() works).\ninternal: Internal functions can only be accessed internally from within the current contract or contracts deriving from it\nprivate: Private functions can only be accessed from the contract they are defined in and not even in derived contracts\nfrom point 23 of Solidity 101 - by Secureum","question-22-of-32#Question 22 of 32":"For error handling\n A. require() is meant to be used for input validation \n B. require() has a mandatory error message string \n C. assert() is meant to be used to check invariants \n D. revert() will abort and revert state changes \nCorrect is A, C, D.\nassert(bool condition): causes a Panic error and thus state change reversion if the condition is not met - to be used for internal errors.\nrequire(bool condition): reverts if the condition is not met - to be used for errors in inputs or external components.\nrequire(bool condition, string memory message): reverts if the condition is not met - to be used for errors in inputs or external components. Also provides an error message.\nrevert(): abort execution and revert state changes\nrevert(string memory reason): abort execution and revert state changes, providing an explanatory string\nfrom point 78 of Solidity 101 - by Secureum\nThe assert function creates an error of type Panic(uint256). Assert should only be used to test for internal errors, and to check invariants. Properly functioning code should never create a Panic, not even on invalid external input.\nfrom point 88 of Solidity 101 - by Secureum","question-23-of-32#Question 23 of 32":"Which of the following is/are true?\n A. Constant state variables can be initialized within a constructor \n B. Immutable state variables are allocated a storage slot \n C. Gas costs for constant and immutable variables is lower \n D. Only value types can be immutable \nCorrect is C, D.\nFor constant variables, the value has to be a constant at compile time and it has to be assigned where the variable is declared.\nfrom point 17.2 of Solidity 101 - by Secureum\nThe compiler does not reserve a storage slot for these variables, and every occurrence is replaced by the respective value.\nfrom point 17.4 of Solidity 101 - by Secureum\nCompared to regular state variables, the gas costs of constant and immutable variables are much lower\nfrom point 18 of Solidity 101 - by Secureum","question-24-of-32#Question 24 of 32":"Integer overflows/underflows in Solidity\n A. Are never possible because of the language design \n B. Are possible but prevented by compiler added checks (version dependent) \n C. Are possible but prevented by correctly using certain safe math libraries \n D. Are possible without any mitigation whatsoever \nCorrect is B, C.\nIntegers in Solidity are restricted to a certain range. For example, with uint32, this is 0 up to 2**32 - 1. There are two modes in which arithmetic is performed on these types: The ‚Äúwrapping‚Äù or ‚Äúunchecked‚Äù mode and the ‚Äúchecked‚Äù mode. By default, arithmetic is always ‚Äúchecked‚Äù, which means that if the result of an operation falls outside the value range of the type, the call is reverted through a failing assertion. You can switch to ‚Äúunchecked‚Äù mode using unchecked . This was introduced in compiler version 0.8.0.\nfrom point 43 of Solidity 101 - by Secureum","question-25-of-32#Question 25 of 32":"Which of the following is true about mapping types in mapping(_KeyType => _ValueType){:solidity}?\n A. _KeyType can be any value or reference type \n B. _ValueType can be any value or reference type \n C. Can only have storage (not memory) as data location \n D. Can be iterated over natively (i.e. without implementing another data structure) \nCorrect is B, C.\nThe _KeyType can be any built-in value type, bytes, string, or any contract or enum type. Other user-defined or complex types, such as mappings, structs or array types are not allowed. _ValueType can be any type, including mappings, arrays and structs. They can only have a data location of storage and thus are allowed for state variables, as storage reference types in functions, or as parameters for library functions. You cannot iterate over mappings, i.e. you cannot enumerate their keys. It is possible, though, to implement a data structure on top of them and iterate over that.\nfrom point 65 of Solidity 101 - by Secureum","question-26-of-32#Question 26 of 32":"receive() and fallback() functions\n A. Can rely only on 2300 gas in the worst case \n B. May receive Ether with payable mutability \n C. Are mandatory for all contracts \n D. Must have external visibility \nCorrect is A, B, D.\nReceive Function: A contract can have at most one receive function, declared using receive() external payable without the function keyword. This function cannot have arguments, cannot return anything and must have external visibility and payable state mutability. In the worst case, the receive function can only rely on 2300 gas being available (for example when send or transfer is used), leaving little room to perform other operations except basic logging. A contract without a receive Ether function can receive Ether as a recipient of a coinbase transaction (aka miner block reward) or as a destination of a selfdestruct.\nfrom point 33 of Solidity 101 - by Secureum\nFallback Function: A contract can have at most one fallback function, declared using either fallback () external [payable] or fallback (bytes calldata _input) external [payable] returns (bytes memory _output), both without the function keyword. This function must have external visibility. The fallback function always receives data, but in order to also receive Ether it must be marked payable. In the worst case, if a payable fallback function is also used in place of a receive function, it can only rely on 2300 gas being available.\nfrom point 34 of Solidity 101 - by Secureum","question-27-of-32#Question 27 of 32":"In Solidity, selfdestruct(address)\n A. Destroys the contract whose address is given as argument \n B. Destroys the contract executing the selfdestruct \n C. Sends address's balance to the calling contract \n D. Sends executing contract's balance to the address \nCorrect is B, D.\nselfdestruct(address payable recipient): Destroy the current contract, sending its funds to the given Address and end execution.\nfrom point 81.2 of Solidity 101 - by Secureum","question-28-of-32#Question 28 of 32":"Which of the following is/are correct?\n A. Solidity file with pragma solidity ^0.6.5; can be compiled with compiler version 0.6.6 \n B. Solidity file with pragma solidity 0.6.5; can be compiled with compiler version 0.6.5 \n C. Solidity file with pragma solidity ^0.6.5; can be compiled with compiler version 0.7.0 \n D. Solidity file with pragma solidity >0.6.5 <0.7.0; can be compiled with compiler version 0.7.0 \nCorrect is A, B.\nVersion Pragma: This indicates the specific Solidity compiler version to be used for that source file and is used as follows: pragma solidity x.y.z; where x.y.z indicates the version of the compiler.\nUsing the version pragma does not change the version of the compiler. It also does not enable or disable features of the compiler. It just instructs the compiler to check whether its version matches the one required by the pragma. If it does not match, the compiler issues an error.\nA ^ symbol prefixed to x.y.z in the pragma indicates that the source file may be compiled only from versions starting with x.y.z until x.(y+1).z. For e.g., pragma solidity ^0.8.3; indicates that source file may be compiled with compiler version starting from 0.8.3 until any 0.8.z but not 0.9.z. This is known as a ‚Äúfloating pragma.‚Äù\nComplex pragmas are also possible using >,>=,< and <= symbols to combine multiple versions e.g. pragma solidity >=0.8.0 <0.8.3;\nfrom point 7 of Solidity 101 - by Secureum","question-29-of-32#Question 29 of 32":"The impact of data location of reference types on assignments is\n A. storage assigned to storage (local variable) makes a copy \n B. memory assigned to memory makes a copy \n C. memory assigned to storage creates a reference \n D. None of the above \nCorrect is D. They all do the opposite.\nData Location & Assignment: Data locations are not only relevant for persistence of data, but also for the semantics of assignments.\nAssignments between storage and memory (or from calldata) always create an independent copy.\nAssignments from memory to memory only create references. This means that changes to one memory variable are also visible in all other memory variables that refer to the same data.\nAssignments from storage to a local storage variable also only assign a reference.\nAll other assignments to storage always copy. Examples for this case are assignments to state variables or to members of local variables of storage struct type, even if the local variable itself is just a reference.\nfrom point 56 of Solidity 101 - by Secureum","question-30-of-32#Question 30 of 32":"Which of the below are value types?\n A. Address \n B. Enum \n C. Struct \n D. Contract \nCorrect is A, B, D.\nValue Types: Types that are passed by value, i.e. they are always copied when they are used as function arguments or in assignments ‚Äî Booleans, Integers, Fixed Point Numbers, Address, Contract, Fixed-size Byte Arrays (bytes1, bytes2, ‚Ä¶, bytes32), Literals (Address, Rational, Integer, String, Unicode, Hexadecimal), Enums, Functions.\nfrom point 37 of Solidity 101 - by Secureum\nReference Types: Types that can be modified through multiple different names. Arrays (including Dynamically-sized bytes array bytes and string), Structs, Mappings.\nfrom point 38 of Solidity 101 - by Secureum","question-31-of-32#Question 31 of 32":"Arrays in Solidity\n A. Can be fixed size or dynamic \n B. Are zero indexed \n C. Have push, pop and length members \n D. None of the above \nCorrect is A, B, C.\nArrays: Arrays can have a compile-time fixed size, or they can have a dynamic size. Indices are zero-based.\nfrom point 57 of Solidity 101 - by Secureum\nArray members:\nlength: returns number of elements in array\npush(): appends a zero-initialised element at the end of the array and returns a reference to the element\npush(x): appends a given element at the end of the array and returns nothing\npop: removes an element from the end of the array and implicitly calls delete on the removed element\nfrom point 58 of Solidity 101 - by Secureum","question-32-of-32#Question 32 of 32":"Solidity language is\n A. Statically typed \n B. Object-oriented \n C. Supports inheritance \n D. Supports inline assembly \nCorrect is A, B, C, D. Inline assembly support is passively mentioned several times in Solidity 101.\nSolidity is statically typed, supports inheritance, libraries and complex user-defined types. It is a fully-featured high-level language.\nfrom point 3 of Solidity 101 - by Secureum\nThe syntax and OOP concepts are from C++.\nfrom point 2 of Solidity 101 - by Secureum"}},"/posts/2021/11/7/secureum-bootcamp-security-pitfalls-amp-best-practices-101-quiz":{"title":"Secureum Bootcamp Security Pitfalls & Best Practices 101 Quiz","data":{"":"This is a writeup of the Secureum Bootcamp Security Pitfalls & Best Practices 101 Quiz containing solutions and references to the provided study material.\nFor fairness, it was published after submissions to it were closed. No syntax highlighting was used in the original quiz, so it was skipped here as well. Make sure to read code comments carefully. The quiz consisted of 16 questions with a strict timelimit of 16 minutes. The ordering of the questions was randomized, so the numbering here won‚Äôt match with the numbering elsewhere.\nThis Quiz from Epoch 0 was declared to be RACE-0 of Epoch Infinity.\nNovember 8, 2021 by patrickd","question-1-of-16#Question 1 of 16":"The use of pragma in the given contract snippet\n A. Is incorrect and will cause a compilation error \n B. Allows testing with 0.6.11 but accidental deployment with buggy 0.6.5 \n C. Is illustrative of risks from using a much older Solidity version (assume current version is 0.8.9) \n D. None of the above \nCorrect is B, C.\nUnlocked pragma: Contracts should be deployed using the same compiler version/flags with which they have been tested. Locking the pragma (for e.g. by not using ^ in pragma solidity 0.5.10) ensures that contracts do not accidentally get deployed using an older compiler version with unfixed bugs.\nfrom point 2 of Security Pitfalls & Best Practices 101 - by Secureum\nThere were bugs that were fixed in versions between 0.6.5 and 0.6.11 which means those fixes were absent in 0.6.5. Choice B was about these aspects.\nfrom Rajeev on Secureum Discord Server\nIllustrative means \"serving as an example or explanation.\" So the use of ^0.6.0 when the latest available version is 0.8.9 (as mentioned in choice C) is an example of using a much older compiler version when newer versions with bug fixes and more security features e.g. built-in overflow checks in ^0.8.0 are available.\nfrom Rajeev on Secureum Discord Server","question-2-of-16#Question 2 of 16":"The given contract snippet has\n A. Unprotected call to selfdestruct allowing anyone to destroy this contract \n B. Dangerous use of zero address leading to burning of contract balance \n C. A compiler error because of the use of the kill reserved keyword \n D. None of the above \nCorrect is A, B.\nUnprotected call to selfdestruct: A user/attacker can mistakenly/intentionally kill the contract. Protect access to such functions.\nfrom point 6 of Security Pitfalls & Best Practices 101 - by Secureum\nZero Address Check: address(0) which is 20-bytes of 0‚Äôs is treated specially in Solidity contracts because the private key corresponding to this address is unknown. Ether and tokens sent to this address cannot be retrieved and setting access control roles to this address also won‚Äôt work (no private key to sign transactions). Therefore zero addresses should be used with care and checks should be implemented for user-supplied address parameters.\nfrom point 144 of Solidity 201 - by Secureum\nReserved Keywords: These keywords are reserved in Solidity. They might become part of the syntax in the future: after, alias, apply, auto, case, copyof, default, define, final, immutable, implements, in, inline, let, macro, match, mutable, null, of, partial, promise, reference, relocatable, sealed, sizeof, static, supports, switch, typedef, typeof, unchecked\nfrom point 131 of Solidity 201 - by Secureum","question-3-of-16#Question 3 of 16":"The given contract snippet has\n A. Missing use of onlyAdmin modifier on transferFunds \n B. Missing return value check on transfer \n C. Unprotected withdrawal of funds \n D. None of the above \nCorrect is A, C.\nIncorrect access control: Contract functions executing critical logic should have appropriate access control enforced via address checks (e.g. owner, controller etc.) typically in modifiers. Missing checks allow attackers to control critical logic.\nfrom point 4 of Security Pitfalls & Best Practices 101 - by Secureum\nUnprotected withdraw function: Unprotected (external/public) function calls sending Ether/tokens to user-controlled addresses may allow users to withdraw unauthorized funds.\nfrom point 5 of Security Pitfalls & Best Practices 101 - by Secureum\ntransferFunds() clearly lets anyone withdraw any amount to any address. The only hint in the Q is the onlyAdmin modifier. While some other access control may also have been acceptable, the focus is on the code snippet provided and hence (A).\nfrom Rajeev on Secureum Discord Server\ntransfer (unlike send) does not return a success/failure return value. It reverts on failure. So there is nothing to be checked. Note that ERC20's transfer() returns a boolean which should be checked\nfrom Rajeev on Secureum Discord Server","question-4-of-16#Question 4 of 16":"In the given contract snippet\n A. getAddress returns the expected addresses if check is true \n B. getAddress returns zero address if check is false \n C. getAddress reverts if check is false \n D. None of the above \nCorrect is A, B. If you have any doubts, give it a try in Remix!\nIncorrect modifier: If a modifier does not execute _ or revert, the function using that modifier will return the default value causing unexpected behavior.\nfrom point 8 of Security Pitfalls & Best Practices 101 - by Secureum\nFunction Modifiers: They can be used to change the behaviour of functions in a declarative way. For example, you can use a modifier to automatically check a condition prior to executing the function. The function‚Äôs control flow continues after the ‚Äú_‚Äù in the preceding modifier. Multiple modifiers are applied to a function by specifying them in a whitespace-separated list and are evaluated in the order presented. The modifier can choose not to execute the function body at all and in that case the return variables are set to their default values just as if the function had an empty body. The _ symbol can appear in the modifier multiple times. Each occurrence is replaced with the function body.\nfrom point 22 of Solidity 101 - by Secureum","question-5-of-16#Question 5 of 16":"The security concern(s) in the given contract snippet is/are\n A. Potential controlled delegatecall risk \n B. delegatecall return value is not checked \n C. delegate() may be missing onlyAdmin modifier \n D. delegate() does not check for contract existence at addr \nCorrect is A, B, C, D.\nControlled delegatecall: delegatecall() or callcode() to an address controlled by the user allows execution of malicious contracts in the context of the caller‚Äôs state. Ensure trusted destination addresses for such calls.\nfrom point 12 of Security Pitfalls & Best Practices 101 - by Secureum\nReturn values of low-level calls: Ensure that return values of low-level calls (call/callcode/delegatecall/send/etc.) are checked to avoid unexpected failures.\nfrom point 37 of Security Pitfalls & Best Practices 101 - by Secureum\nIncorrect access control: Contract functions executing critical logic should have appropriate access control enforced via address checks (e.g. owner, controller etc.) typically in modifiers. Missing checks allow attackers to control critical logic.\nfrom point 4 of Security Pitfalls & Best Practices 101 - by Secureum\nAccount existence check for low-level calls: Low-level calls call/delegatecall/staticcall return true even if the account called is non-existent (per EVM design). Account existence must be checked prior to calling if needed.\nfrom point 38 of Security Pitfalls & Best Practices 101 - by Secureum","question-6-of-16#Question 6 of 16":"The vulnerability/vulnerabilities present in the given contract snippet is/are\n A. Reentrancy \n B. Integer underflow leading to wrapping \n C. Missing check on user balance in withdraw() \n D. All of the above \nCorrect is B, C. The code in this question was unintentionally missing inheritance from the ReentrancyGuard Contract. While there's a lot of discussion about the correct meaning of the term \"underflow\", this is how it is used in the Solidity Documentation and other related literature.\nReentrancy vulnerabilities: Untrusted external contract calls could callback leading to unexpected results such as multiple withdrawals or out-of-order events. Use check-effects-interactions pattern or reentrancy guards.\nfrom point 13 of Security Pitfalls & Best Practices 101 - by Secureum\nInteger overflow/underflow: Not using OpenZeppelin‚Äôs SafeMath (or similar libraries) that check for overflows/underflows may lead to vulnerabilities or unexpected behavior if user/attacker can control the integer operands of such arithmetic operations. Solc v0.8.0 introduced default overflow/underflow checks for all arithmetic operations.\nfrom point 19 of Security Pitfalls & Best Practices 101 - by Secureum\nI hope that the comment // Assume nonReentrant modifier is always applied implied that the intent was to apply the modifier \"correctly\" (i.e. with successful compilation üòÖ), which further implies reentrancy risk mitigation i.e. A is not a correct choice. Also, if A were to also be correct then that would again result in the \"All of the above\" ambiguity (A+B+C or D or A+B+C+D) which I have consciously tried to avoid. If that confused folks, apologies ‚Äî sorry about that. I've tried to compile all snippets (in Remix) to avoid such silly mistakes, but not sure how this one slipped through.\nfrom Rajeev on Secureum Discord Server","question-7-of-16#Question 7 of 16":"The security concern(s) in the given contract snippet is/are\n A. diceRoll() visibility should be public instead of external \n B. The private variable secret is not really hidden from users \n C. block.timestamp is an insecure source of randomness \n D. Integer overflow \nCorrect is B, C.\nPrivate on-chain data: Marking variables private does not mean that they cannot be read on-chain. Private data should not be stored unencrypted in contract code or state but instead stored encrypted or off-chain.\nfrom point 16 of Security Pitfalls & Best Practices 101 - by Secureum\nWeak PRNG: PRNG relying on block.timestamp, now or blockhash can be influenced by miners to some extent and should be avoided.\nfrom point 17 of Security Pitfalls & Best Practices 101 - by Secureum\nMaking it public in this case should not affect gas given that there are no function arguments to copy over (if there were parameters/arguments, making it public would increase gas). Even otherwise, making it public from external should not affect the attack surface of the contract because it will only further allow (trusted) contract functions to call it.\nfrom Rajeev on Secureum Discord Server\nE.: The logic of diceRoll() is broken as it returns only 1 or 4 üôÇ\nfrom lukasz.glen on Secureum Discord Server","question-8-of-16#Question 8 of 16":"The security concern(s) in the given contract snippet is/are\n A. Incorrect use of transfer() instead of using send() \n B. Potential man-in-the-middle attack on admin address authentication \n C. Assumption on contract balance might cause a revert \n D. Missing event for critical emergencyWithdraw() function \nCorrect is B, D. Neither transfer nor send are recommended anymore.\nAvoid transfer()/send() as reentrancy mitigations: Although transfer() and send() have been recommended as a security best-practice to prevent reentrancy attacks because they only forward 2300 gas, the gas repricing of opcodes may break deployed contracts. Use call() instead, without hardcoded gas limits along with checks-effects-interactions pattern or reentrancy guards for reentrancy protection.\nfrom point 15 of Security Pitfalls & Best Practices 101 - by Secureum\nDangerous usage of tx.origin: Use of tx.origin for authorization may be abused by a MITM malicious contract forwarding calls from the legitimate user who interacts with it. Use msg.sender instead.\nfrom point 30 of Security Pitfalls & Best Practices 101 - by Secureum\n[Regarding C.:] 0 transfers should not revert üòÖ. Even if they did, in this context, it wouldn't be considered a \"security\" concern because there would be nothing to withdraw and so a revert wouldn't be a concern w.r.t. any locked funds as such.\nfrom Rajeev on Secureum Discord Server\nMissing events: Events for critical state changes (e.g. owner and other critical parameters) should be emitted for tracking this off-chain.\nfrom point 45 of Security Pitfalls & Best Practices 101 - by Secureum","question-9-of-16#Question 9 of 16":"The given contract snippet is vulnerable because of\n A. Integer overflow leading to wrapping \n B. Overly permissive function visibility of contribute() \n C. Incorrect use of msg.sender \n D. Use of strict equality (!=) may break the MAX_FUND_RAISE constraint \nCorrect is D. Visibility of external or public is required for a function to be payable. This use of message sender is very common and correct.\nDangerous strict equalities: Use of strict equalities with tokens/Ether can accidentally/maliciously cause unexpected behavior. Consider using >= or <= instead of == for such variables depending on the contract logic.\nfrom point 28 of Security Pitfalls & Best Practices 101 - by Secureum\nUnexpected Ether and this.balance: A contract can receive Ether via payable functions, selfdestruct(), coinbase transaction or pre-sent before creation. Contract logic depending on this.balance can therefore be manipulated.\nfrom point 26 of Security Pitfalls & Best Practices 101 - by Secureum\nGiven the compiler version, even if there is an attempted integer overflow at runtime, it will revert before overflowing (because of inbuilt checks) with an exception but will not wrap. So A is not a vulnerability. While this is true in general, this snippet cannot be overflowed because of its dependence on msg.value.\nfrom Rajeev on Secureum Discord Server","question-10-of-16#Question 10 of 16":"In the given contract snippet, the require check will\n A. Pass only if target is an existing contract address \n B. Pass for a non-existent contract address \n C. Pass always \n D. Fail always \nCorrect is B.\nAccount existence check for low-level calls: Low-level calls call/delegatecall/staticcall return true even if the account called is non-existent (per EVM design). Account existence must be checked prior to calling if needed.\nfrom point 38 of Security Pitfalls & Best Practices 101 - by Secureum","question-11-of-16#Question 11 of 16":"The security concern(s) in the given contract snippet is/are\n A. Missing access control on critical function \n B. Missing zero-address validation \n C. Single-step change of critical address \n D. Missing event for critical function \nCorrect is A, B, C, D.\nIncorrect access control: Contract functions executing critical logic should have appropriate access control enforced via address checks (e.g. owner, controller etc.) typically in modifiers. Missing checks allow attackers to control critical logic.\nfrom point 4 of Security Pitfalls & Best Practices 101 - by Secureum\nMissing zero address validation: Setters of address type parameters should include a zero-address check otherwise contract functionality may become inaccessible or tokens burnt forever.\nfrom point 49 of Security Pitfalls & Best Practices 101 - by Secureum\nCritical address change: Changing critical addresses in contracts should be a two-step process where the first transaction (from the old/current address) registers the new address (i.e. grants ownership) and the second transaction (from the new address) replaces the old address with the new one (i.e. claims ownership). This gives an opportunity to recover from incorrect addresses mistakenly used in the first step. If not, contract functionality might become inaccessible.\nfrom point 50 of Security Pitfalls & Best Practices 101 - by Secureum\nMissing events: Events for critical state changes (e.g. owner and other critical parameters) should be emitted for tracking this off-chain.\nfrom point 45 of Security Pitfalls & Best Practices 101 - by Secureum","question-12-of-16#Question 12 of 16":"The security concern(s) in the given contract snippet is/are\n A. Uninitialized pool storage variable which assumes setPoolAddress() will be called before addLiquidity() \n B. Incorrect use of modifier onlyAdmin on setPoolAddress() \n C. Missing zero-address validation for _pool in setPoolAddress() \n D. Transaction order dependence risk from admin front-running with pool address change \nCorrect is A, C, D.\nFunction invocation order: Externally accessible functions (external/public visibility) may be called in any order (with respect to other defined functions). It is not safe to assume they will only be called in the specific order that makes sense to the system design or is implicitly assumed in the code. For e.g., initialization functions (used with upgradeable contracts that cannot use constructors) are meant to be called before other system functions can be called.\nfrom point 145 of Security Pitfalls & Best Practices 201 - by Secureum\nUninitialized state/local variables: Uninitialized state/local variables are assigned zero values by the compiler and may cause unintended results e.g. transferring tokens to zero address. Explicitly initialize all state/local variables.\nfrom point 67 of Security Pitfalls & Best Practices 101 - by Secureum\nMissing zero address validation: Setters of address type parameters should include a zero-address check otherwise contract functionality may become inaccessible or tokens burnt forever.\nfrom point 49 of Security Pitfalls & Best Practices 101 - by Secureum\nTransaction order dependence: Race conditions can be forced on specific Ethereum transactions by monitoring the mempool. For example, the classic ERC20 approve() change can be front-run using this method. Do not make assumptions about transaction order dependence.\nfrom point 21 of Security Pitfalls & Best Practices 101 - by Secureum","question-13-of-16#Question 13 of 16":"The security concern(s) in the given proxy-based implementation contract snippet is/are\n A. Imported contracts are not upgradeable \n B. Multiple initialize() calls possible which allows anyone to reset the admin \n C. rewards will be 0 in the proxy contract before setRewards() is called by it \n D. All the above \nCorrect is B, C. There are no imported contracts that need to be made upgradable (by implementing Initializable).\nImport upgradeable contracts in proxy-based upgradeable contracts: Contracts imported from proxy-based upgradeable contracts should also be upgradeable where such contracts have been modified to use initializers instead of constructors.\nfrom point 97 of Security Pitfalls & Best Practices 101 - by Secureum\nUnprotected initializers in proxy-based upgradeable contracts: Proxy-based upgradeable contracts need to use public initializer functions instead of constructors that need to be explicitly called only once. Preventing multiple invocations of such initializer functions (e.g. via initializer modifier from OpenZeppelin‚Äôs Initializable library) is a must.\nfrom point 95 of Security Pitfalls & Best Practices 101 - by Secureum\nInitializing state-variables in proxy-based upgradeable contracts: This should be done in initializer functions and not as part of the state variable declarations in which case they won‚Äôt be set.\nfrom point 96 of Security Pitfalls & Best Practices 101 - by Secureum","question-14-of-16#Question 14 of 16":"The security concern(s) in the given contract snippet is/are\n A. Potential out-of-gas exceptions due to unbounded external calls within loop \n B. ERC20 approve() race condition \n C. Unchecked return value of transfer() (assuming it returns a boolean/other value and does not revert on failure) \n D. Potential reverts due to mismatched lengths of recipients and amounts arrays \nCorrect is A, C, D. There's no guarantee that the passed arrays are of same length, so if one would be longer than the other one it can cause an Out Of Bounds error, which is why D is correct.\nCalls inside a loop: Calls to external contracts inside a loop are dangerous (especially if the loop index can be user-controlled) because it could lead to DoS if one of the calls reverts or execution runs out of gas. Avoid calls within loops, check that loop index cannot be user-controlled or is bounded.\nfrom point 43 of Security Pitfalls & Best Practices 101 - by Secureum\nERC20 transfer() does not return boolean: Contracts compiled with solc >= 0.4.22 interacting with such functions will revert. Use OpenZeppelin‚Äôs SafeERC20 wrappers.\nfrom point 24 of Security Pitfalls & Best Practices 101 - by Secureum\nThis is ERC20 token transfer and not Ether transfer (which throws on failure). ERC20 transfer is typically expected to return a boolean but non-ERC20-conforming tokens may return nothing or even revert which is typically why SafeERC20 is recommended.\nfrom Rajeev on Secureum Discord Server","question-15-of-16#Question 15 of 16":"The vulnerability/vulnerabilities present in the given contract snippet is/are\n A. Reentrancy \n B. Integer overflow leading to wrapping \n C. Integer underflow leading to wrapping \n D. None of the above \nCorrect is A.\nReentrancy vulnerabilities: Untrusted external contract calls could callback leading to unexpected results such as multiple withdrawals or out-of-order events. Use check-effects-interactions pattern or reentrancy guards.\nfrom point 13 of Security Pitfalls & Best Practices 101 - by Secureum\nInteger overflow/underflow: Not using OpenZeppelin‚Äôs SafeMath (or similar libraries) that check for overflows/underflows may lead to vulnerabilities or unexpected behavior if user/attacker can control the integer operands of such arithmetic operations. Solc v0.8.0 introduced default overflow/underflow checks for all arithmetic operations.\nfrom point 19 of Security Pitfalls & Best Practices 101 - by Secureum","question-16-of-16#Question 16 of 16":"The security concern(s) in the given contract snippet is/are\n A. Signature malleability risk of ecrecover \n B. Missing use of nonce in message hash may allow replay attacks across transactions \n C. Missing use of chainID in message hash may allow replay attacks across chains \n D. Missing zero-address check for ecrecover return value may allow invalid signatures \nCorrect is A, B, C, D.\nSignature malleability: The ecrecover function is susceptible to signature malleability which could lead to replay attacks. Consider using OpenZeppelin‚Äôs ECDSA library.\nfrom point 23 of Security Pitfalls & Best Practices 101 - by Secureum\nInsufficient Signature Information: The vulnerability occurs when a digital signature is valid for multiple transactions, which can happen when one sender (say Alice) sends money to multiple recipients through a proxy contract (instead of initiating multiple transactions). In the proxy contract mechanism, Alice can send a digitally signed message off-chain (e.g., via email) to the recipients, similar to writing personal checks in the real world, to let the recipients withdraw money from the proxy contract via transactions. To assure that Alice does approve a certain payment, the proxy contract verifies the validity of the digital signature in question. However, if the signature does not give the due information (e.g., nonce, proxy contract address), then a malicious recipient can replay the message multiple times to withdraw extra payments. This vulnerability was first exploited in a replay attack against smart contracts [36]. This vulnerability can be prevented by incorporating the due information in each message, such as a nonce value and timestamps\nfrom point 3.1.13 of A Survey on Ethereum Systems Security: Vulnerabilities, Attacks, and Defenses\nIndistinguishable Chains: This vulnerability was first observed from the cross-chain replay attack when Ethereum was divided into two chains, namely, ETH and ETC [10]. Recall that Ethereum uses ECDSA to sign transactions. Prior to the hard fork for EIP-155 [7], each transaction consisted of six fields (i.e., nonce, recipient, value, input, gasPrice, and gasLimit). However, the digital signatures were not chain-specific, because no chain-specific information was even known back then. As a consequence, a transaction created for one chain can be reused for another chain. This vulnerability has been eliminated by incorporating chainID into the fields.\nfrom point 3.2.1 of A Survey on Ethereum Systems Security: Vulnerabilities, Attacks, and Defenses\necrecover() returns (address): recover the address associated with the public key from elliptic curve signature or return zero on error.\nfrom Mathematical and Cryptographic Functions ‚Äì Solidity Documentation"}},"/posts/2022/11/5/write-up-eko2022-blockchain-ctf":{"title":"Write-Up: EKO2022 Blockchain CTF","data":{"":"November 5, 2022 by patrickd\nThis is a write-up for a collection of challenges made for EKOparty, an annual information security conference in Latin America.","the-lost-kitty#The Lost Kitty":"Lucas is a scientist who lives with his cat in a big house that has 2^256 rooms. His cat likes to play hide and seek and jumps to a random room whenever it hears a door opening in another one. Can you find Lucas' cat? Set the variable catFound to true to win this challenge.by Br0niclŒû | luksgrin.lens\nThe Factory contract, which takes care of this Challenge's setup, deploys a House, which when called with a room-guess deploys a HiddenKittyCat and checks whether the correct slot was specified.\nThe use of the word \"slot\" and the fact that the House has  \"rooms\" pretty much gives away that the kitty is hiding in storage slots. In Ethereum this is the place where data can be stored persistently between transactions. Storage is accessed like a simple key-value-store where both the key and the value can be 32 bytes large. This maximum size of keys restricts the amount of available storage slots to the above-mentioned number of rooms.\nDuring the deployment of a new HiddenKittyCat a \"random\" slot is selected by hashing the current block's timestamp and the blockhash from 69 blocks ago. What \"blockhashes\" are has changed quite a bit since The Merge, but it's irrelevant for the scope of this challenge - just take it as a block's unique hash. The string \"KittyCat!\" is written to the generated slot as the value, so it can later check for it when the slot we guessed is tested.This is a very classic example of \"Bad/Predictable Randomness\" in Ethereum: Although you can't really guess the slot correctly when calling isKittyCatHere() directly before making a transaction, you can instead simply deploy another contract that creates the same hash (since both block-values will be the same for all contracts executed within the same block) and then calls the challenge with the correct guess for you.","hack-the-mothership#Hack the Mothership":"You and a small group of scientists have been working on a global counteroffensive against the invader.\nWe've recovered some of the ship's source code and need to find a way to hack it!\nYou have already studied the code and realized that to survive you need to take control of the Mothership.\nYour objective is to hack the Mothership instance (change the hacked bool to true).\nGood luck, the earth's future depends on you!by nicobevi.eth\nThis challenge seems a bit overwhelming from all of the code given, but the goal is once again to flip a boolean. And for that, we apparently have to become the leader of the Mothership.\nThe leader of the Mothership is elected by the captains of the SpaceShips. A SpaceShip's crew member can ask for a new captain but only if there's currently none set.The thing that stands out though is how SpaceShips have modules. The constructor deploys several \"Module\" contracts which the contract delegate-calls to via the fallback() function.\nThe fallback() function is called whenever there's no function matching the signature that was requested in the calldata. So whenever someone attempts to call a function that SpaceShip itself does not implement, the fallback will be triggered instead.\nA delegate-call is similar to a normal external call but with the significant difference that the caller's context will be used instead of the context of the called contract. Most importantly, this means that the called contract will apply any changes to storage to the caller's state instead of to its own.\nTwo of these Modules do not have any access controls and they also have a bug.\nWhenever the CleaningModule overwrites the current cleaningCompany, it would also overwrite whatever is currently held at this same position in storage of the callee. In case of SpaceShip delegate-calling to CleaningModule, setting the cleaningCompany would end up overwriting the captain state variable.","trickster#Trickster":"We might have spotted a honeypot... Can you manage to obtain the real jackpot?\nHacking casino slot machines is considered illegal.by matta\nThe ChallengeFactory that sets this one up shows that the JackpotProxy will be deployed with 0.0001 of ether and we must make it return a balance of zero.\nThe JackpotProxy then deploys a Jackpot contract and transfers all of the ether it received to it. Apparently, the code wants to make us believe that the proxy keeps the balance...\nBut the actual Jackpot contract that now has the balance was never actually initialized. We can call initialize with our own address and then simply execute claimPrize() with half of the balance.\nThere are two tricks to this challenge: Not falling for purposefully confusing code and attempting to claimPrize() on JackpotProxy instead. Plus figuring out the address of the Jackpot contract which JackpotProxy keeps behind a private state variable.\"Private\" only means that it's not allowing other contracts to read the value though. All storage is still readably stored on the blockchain and even if it weren't stored one could still look at the contract creations that happened during the deployment transactions (eg. on etherscan).","smart-horrocrux#Smart Horrocrux":"Some security researchers have recently found an eighth Horrocrux, it seems that Voldemort has link to a smart contract, can you destroy it?by bengalaQ\nFrom the description, it's quite clear that we somehow have to call the kill() function without it reverting, and to do that it needs to be called from the contract itself.\nThe function that would allow us to do that is destroyIt(), but it too has some requirements to call it:\nThe first 32 bytes of the spell parameter need to match the value from the _spell constant.\nThe invincible state variable has to be flipped to false.\nThe magic integer will be subtracted from the spell and the result needs to be calldata that ends up calling the kill() function.\nThe first 4 bytes of keccak-256 hashing kill() are 41c0e1b5. From that we can calculate the magic number\nNow, all that's left, is flipping invincible and to do so we need to somehow remove 1 wei from the contract's balance.\nTo do that, it appears we can first trigger the fallback() function to drain the 2 initial wei and then inject 1 wei via self-destruct (we can't simply send it since that would trigger fallback() again and just send it back).\nAll of this can be nicely packed up into an Exploit:","gas-valve#Gas Valve":"The evil Dr. N. Gas has put into orbit a machine that can suck all the air out of the atmosphere. You sneaked into his spaceship and must find a nozzle to open the main valve and stop the machine! Assert the situation and don't panic. Hint: on the valve is marked \"model no. EIP-150\"by Bahurum\nTo open the Valve we need to call openValve() with an address that has a contract implementing INozzle's insert() function.\nThe difficulty lies in having this function cause an error despite reverts being caught by the try-catch. This might seem impossible at first but the model number hints at EIP-150 which discusses gas costs - and that basically gives away the solution: Whenever a CALL is made to another contract, 1/64th of gas is kept by the caller to increase the likelihood that even if the callee consumes all gas that was sent, the caller still has enough gas left to do a few actions afterward.Therefore we have to do 2 things:\nSend a transaction with a gas limit that is high enough for openValve() to succeed but too low for useNozzle() to function without reverting.\nHave the insert() function use up all gas that was sent (we can use the INVALID opcode to do so).\nNow, you could do some gas usage measurements and calculations to determine the exact range of values that work - or you could just try a couple guesses (100k worked for me in remix).","pelusa#Pelusa":"You just open your eyes and are in Mexico 1986, help Diego to set the score from 1 to 2 goals for a win, do whatever is necessary!by 0x4non\nThis is another puzzle-type challenge where certain conditions have to be met to be able to call certain functions in a certain way to adjust certain state variables. More specifically, we have to increase goals by one.There's nowhere in the code where this variable is updated but we can instead do it ourselves during the delegate-call made by the shoot() function:\nRequirements are that\nisGoal() returns true, and for that, the player contract needs to return the owner value when getBallPossesion() is called.\nThe delegate-called handOfGod() function must succeed and return the integer 22061986 (you can just ignore the underscores, they have no effect).\nDetermining the owner is quite simple although it's a private immutable variable. We just have to find out the msg.sender that deployed the contract and the blockhash of when the transaction was included, then hash these values and convert them to an address.\nThe tricky part is setting a player that is an EOA (a wallet with a key-pair and not a contract) and at the same time a contract that implements the getBallPossesion() and handOfGod() functions. The checks want to ensure that the msg.sender is an EOA by ensuring that there's no code at its address, but while it's correct that EOAs do not have code, it's easy to forget that contracts currently being deployed (meaning their constructor is being executed) do not have any code at their address yet either (because the runtime bytecode that'll be placed at the address is being generated by the constructor).The last condition is that our contract needs to have an address that when divided by 100 has a rest of 10. Addresses are based on hashing and to find one that matches a specific requirement one does not come around brute force.\nWe could brute force different public keys of the EOA that'll deploy the contract until the first deployed contract's address will end up what we need.\nWe could keep using the same EOA and just increase the nonce by making other successful transfers until we find a nonce that'd deploy the contract to a fitting address.\nWe could deploy a contract that keeps increasing its own nonce by deploying other contracts until once again a nonce is found to satisfy the requirement for deploying the actual exploit.\nWe could make use of the CREATE2 opcode to, instead of increasing a nonce, trying different salts until one works.\nThere's no need to get too fancy with this though since it's quite easy to find a satisfying address even by just repeatedly hitting the deploy button.","phoenixtto#Phoenixtto":"Within the world of crossovers there is a special one, where the universes of pokemon, harry potter and solidity intertwine. In this crossover a mixed creature is created between dumbledore's phoenix, a wild ditto and since we are in the solidity universe this creature is a contract. We have called it Phoenixtto and it has two important abilities, that of being reborn from it's ashes after its destruction and that of copying the behavior of another bytecode.Try to capture the Phoenixtto, if you can...by Rotciv\nIn the challenge's factory contract, we can see that the Laboratory contract is deployed and then its mergePhoenixDitto() function is called. We can also see that the goal is to change the Phoenixtto's owner variable to us, the player.\nWhat happens during the merge-call seems complicated, but the first part could basically be rewritten to getImplementation = new Phoenixtto(). Afterward though, some raw bytecode is deployed, which is most likely a simple proxy.\nProxies in Ethereum smart contracts can be used to create \"cheap clones\" of other contracts. Cheap because there's no need to re-deploy the entire large bytecode holding the actual implementation when you can instead just deploy multiple smaller smart contracts that delegate-call all of the requests made to them to the implementation. That way, all that the proxies hold is their own state but the code is re-usably stored at a single address. For some reason, reBorn does not appear to reuse the same implementation though...Since the CREATE2 opcode deploying the proxy statically re-uses the same bytecode and salt (0), no matter how many times the Phoenixtto contract self-destructs itself it should always end up back at the same address.\nLike the previous challenge, this too wants to make sure that the msg.sender is an EOA, it does so in a way that cannot be bypassed though. It achieves this by comparing  msg.sender to tx.origin, which always contains the address of the EOA that signed the current transaction. Only EOAs can sign transactions, therefore the msg.sender will be forced to be an EOA.And as also previously mentioned, to generate an EOA's address we need to hash its public key. So the solution is quite simply calling the capture() function directly while supplying the EOA's public key as parameter.\nTo check my assumptions so far, I disassembled the hardcoded bytecode to find what is clearly not a proxy: What seems to be actually happening here is that it'll call getImplementation() on the Laboratory to then copy the Phoenixtto's code to itself. Well, that's less gas efficient than expected but it won't change the solution.","the-golden-ticket#The Golden Ticket":"The organizers of Ekoparty decided that the tickets for the 2023 conference would be purchased through a smart contract. However, the conference is oversold and you have to sign up for a waitlist to get your ticket. The problem is that they put you on hold for ten years and the only option you have is to extend the wait. After the wait is over, you have to enter a raffle to see if you get the ticketby chiin\nIn this challenge, we (or rather, the EOA that is the player that deployed the challenge instance) need to obtain a ticket (flipping hasTicket to true).But at the beginning, we'll only be able to successfully call the joinWaitlist() function.\nAfter that, we'll be able to call updateWaitTime() to increase the time we'd have to wait even further. With uint40 though the maximum possible timestamp will be  and thanks to the fact that this update happens within a unchecked-block we can easily cause the integer to overflow by supplying a sufficiently high _time value.\nHaving bypassed the waitlist, we're now able to take part in the raffle by exploiting the bad randomness. In this version of the challenge, a giftTicket() function was added to allow exploiting this the easy way using smart contracts and then transferring the ticket to the player's EOA.Before though, you'd have needed to somehow trick the system into thinking that your player is actually a smart contract, or use something like flashbots to gain more control over the block that your transaction will be placed in. (This turned out to be a little too hard)","stonks#Stonks":"You have infiltrated in a big investment firm (name says something about arrows), your task is to loose all their money.by 0x4non\nThere's not really much to do here other than calling the buy and sell functions.\nIntegers and division should ring a bell. What's 49/50?So basically you just have to first sell all TSLA. Then keep buying TSLA for amountGMEins smaller than TSLA's price of 50.","metaverse-supermarket#Metaverse Supermarket":"We are all living in the Inflation Metaverse, a digital world dominated by the INFLA token. Stability has become a scarce resource and even going to the store is a painful experience: we need to rely on oracles that sign off-chain data that lasts a couple of blocks because updating prices on-chain would be complete madness.\nYou are out of INFLAs and you are starving, can you defeat the system?by adriro\nThis one seems a bit more complex, so let's start by checking the success conditions from the factory contract.\nApparently we need to obtain at least 10 MEAL NFTs and we're starting with 10 wei of Infla coins, which is nowhere close to the hardcoded price of 1 million wei we need for a single meal!\nAll that leaves us with is somehow tricking the buyUsingOracle() to accept a manipulated price.\nNow, you might start checking the implementation of the signature validation for any errors. You can take a look at OpenZeppelin's contracts and usage instructions and you might learn that this sure looks like it's vulnerable to signature malleability. But that wouldn't matter, since even if we change a signature into a different form, it would still sign the same thing. And we don't have anything to change and replay in the first place.But this is the right train of thought: Checking what is missing. And what really is missing here is that the oracle's address is actually never set. Not by the factory. Not during construction. Therefore it's the zero-address.Now the important detail one has to know is that whenever ecrecover() fails it doesn't revert, but instead it returns the zero-address. So if we submit any OraclePrice with an invalid Signature the returned address of _recover() will match the oracle address and therefore the signature will be considered valid.\nIf you run into trouble while attempting to solve this, don't forget to give an allowance to the store if you pick a price of 1 wei. And make sure that your contract implements ERC721TokenReceiver if you're using one to solve this.","rootme#RootMe":"Can you trick the machine to get root access?by tinchoabbate\nThe goal of this challenge is setting the boolean victory to true which is only possible via the write() function that allows to arbitrarily change the contract's storage slots. Victory will end up in slot 0 and simply writing the maximum uint256 to it should flip the bit even without knowing how exactly booleans are stored within the slot.Preventing us from doing this is a onlyRoot modifier that is part of kind of a user-based access control that was implemented in this contract. The bug that this system has, is actually quite obvious if you're familiar with this pattern:\nWhat encodePacked() does, is simply concatenating both strings. One might think that by hashing them this way, they would always end up with a unique hash for each user. After all, the usernames map makes sure that we aren't able to register the same user with a different salt.The issue here is typically referred to as \"Hash Collision\" and happens when values of variable length are hashed: keccak256(abi.encodePacked(\"ROOT\", \"ROOT)) == keccak256(abi.encodePacked(\"ROO\", \"TROOT\")){:solidity}.The solution is therefore registering a user with a salt that will produce a hash colliding with the hash of the ROOT user."}},"/posts/2022/3/12/evm-puzzles-second-wind":{"title":"EVM Puzzles ‚Äì Second Wind","data":{"":"March 12, 2022 by patrickd\nJust shortly after publishing my write-up on Franco Victorio's EVM Puzzles he finished to \"Re-write the whole thing\"!It's now interactive, meaning you no longer have to edit the puzzle files to solve them but instead just type your solution directly into the console. And instead of having to look at the bytecode embedded in JavaScript, it now too is nicely displayed within the console with colors and addresses. But most importantly, the puzzles themselves have changed and there's now 10 of them!Note that I'll assume you've read my previous blog post already, or that you've solved the old version, since I'll not explain the opcodes that were already discussed there in detail again.","puzzle-1#Puzzle #1":"The new first puzzle is very similar to the old one, just this time the JUMP's destination value is not based on CALLDATASIZE (the length of data sent) but instead taken from the CALLVALUE. Here we no longer have to do any counting in order to determine where we want to jump to, we can just directly read the address of the JUMPDEST opcode which is 08.So let's send a transaction value of 8 as solution, which will cause a jump to the only available jump-destination and then ends the execution without error. It doesn't really matter that STOP is the next instruction after JUMPDEST since that's implicitly the case at the end of an EVM program anyway.","puzzle-2#Puzzle #2":"Once more, very similar to the previous version with the biggest difference in it using the value instead of calldata-size again. The CODESIZE is 10 (last address + 1) and the only JUMPDEST is located at offset 06.We need to calculate CODESIZE minus the value we send, and have the result be 6. Therefore, sending a 4 is the solution.This time it was essential that STOP came after JUMPDEST since otherwise, the instruction pointer would have run into a REVERT causing a failure of execution.","puzzle-3#Puzzle #3":"It appears CALLDATASIZE is now being introduced as a new \"mechanic\". Again, we can see that the only valid jump destination is located at 04, which means we'll have to send any 4 bytes (eg. 00000000) as transaction calldata in order to reach the end.","puzzle-4#Puzzle #4":"This code will calculate CODESIZE ^ CALLVALUE and use the result as jump target with the only valid jump-destination being at 0A (or 10 in decimal). With the code size being 12, what we have to solve here is 12 ^ CALLVALUE = 10 or 12 ^ 10 = CALLVALUE:\nThe solution is 6!","puzzle-5#Puzzle #5":"The first 3 lines do CALLDATA * CALLDATA and later that result is compared to 0100. Only if that yields 1 (true) the JUMPI will jump to the JUMPDEST at offset 0C.The 0100 bytes are 256 in decimal and its square root is 16, which is the solution.","puzzle-6#Puzzle #6":"Here the calldata itself is actually being pushed on the stack and used as jump-destination offset. Just like in the old Puzzle #3, we can't simply send the offset 0a as calldata since it would be padded by 31 zero-values and become a much larger number.To prevent that, we again have to send 32 bytes: .","puzzle-7#Puzzle #7":"Things get a lot more interesting now, with the introduction of the CREATE opcode that deploys a new contract. Immediately after the contract was created, it fetches the size of its bytecode using EXTCODESIZE and compares it to 1. Therefore only if the newly deployed contract has a code size of 1 it will make the jump and stop without reverting.A CREATE consumes 3 items from the stack: A value of wei to transfer, an offset of where the new contract's bytecode is located in the current contract's memory and the length of said bytecode. So far the puzzles never touched memory, but this changes with CALLDATACOPY which also takes its 3 parameters from the stack: The memory destination offset to copy bytes from calldata to, starting at the specified offset and again the final item is the length.Let's look at how the stack changes instruction by instruction:\nThe effect so far is simply that all of the calldata was copied into memory - both calldata and memory have exactly the same contents now. After this, the CREATE is called similarly, telling it to create a contract with the calldata as bytecode which it can copy from memory:\nAs hinted before, the address returned by CREATE is then consumed by EXTCODESIZE:\nAt this point, the obvious solution would appear to be sending a single byte, which is then deployed as a new contract's bytecode. But the description of the CREATE operation is misleading because what it expects to find in the memory location you point to is not the runtime bytecode to deploy, but the construction bytecode!During all of these puzzles we have been looking at the runtime bytecode, so it is easy to forget that every contract is first initialized by a separate piece of code. If you're familiar with Solidity: it's basically the code that you'd put into the constructor. OpenZeppelin has a great blog post about Deconstructing a Solidity Contract, explaining the difference and what is really going on under the hood.What this init bytecode needs to do in short, is returning the actual runtime bytecode that should be deployed as a new contract, and to do that it has to use the RETURN opcode. Return takes offset and size parameters from stack which is a location within memory that should be copied.To solve this puzzle we have to return a single byte as runtime bytecode within the construction bytecode. Since we don't care what this byte is and because memory is zero-initialized we don't have to actually write any code to memory before returning. We can just tell it to return 1 byte at any offset:\nSince this is really short we don't need an assembler, we can just concatenate the opcodes and end up with 0x60016000F3 as the solution.","puzzle-8#Puzzle #8":"The beginning of the puzzle is exactly the same as in the previous one: All calldata is copied into memory and is then used as construction bytecode by CREATE.Things get interesting afterwards:\nWhile it looks awfully complicated, this basically just prepares the stack for executing the CALL opcode, which as the name suggests, calls into the bytecode of another contract.\nThe amount of gas that should be made available for the execution of the contract being called. Here it's the result of the GAS opcode that returns the overall amount of gas still available.\nThe target address of the account to call into. Here it's the address that was returned by CREATE, meaning the puzzle will be calling into the contract deployed using our calldata.\nThe amount of Wei to send as value. Here no ether (00) is sent during the call.\nThe memory offset where the arguments passed during the call are stored.\nThe size of the argument data to pass from memory. Here it's 00 meaning no argument data is passed and the memory offset (00) is also irrelevant.\nThe memory offset where the returned data should be stored to.\nThe size of the returned data that should be copied into memory. Here it's 00 meaning none of the returned data (if there were any) should be copied into memory, also making the memory destination offset (00) irrelevant.\nIn summary, we're calling into the contract that was just deployed using the sent calldata. For this puzzle to succeed we need the \"success\" return value of the CALL opcode to be EQual to 00. That means that the call into the contract must fail, we need it to REVERT.The solution is similar to the previous one in that we again have to create construction bytecode that deploys a contract with a single byte, but this time the byte must be 0xFD, the REVERT opcode:\nUsing the MSTORE8 opcode we can write a single byte to memory. Using that, the construction bytecode we have to send as calldata is 0x60FD60005360016000F3.","puzzle-9#Puzzle #9":"See Puzzle #7 of the previous version.","puzzle-10#Puzzle #10":"See Puzzle #8 of the previous version.","conclusion#Conclusion":"Most of the changes in this version are user experience improvements, only Puzzle #7 and #8 are fresh additions. But they were great ones at that! With the introduction of the CREATE and CALL opcodes they force you to become familiar with the concept of construction and runtime bytecode which is important to understand even when developing with a high-level language such as Solidity."}},"/posts/2022/5/26/more-evm-puzzles-part-2":{"title":"More EVM Puzzles - Part 2","data":{"":"May 26, 2022 by patrickd\nThis continues the series on Dalton Sweeney's \"10 more EVM puzzles\" collection. If you haven't read it yet, you should probably check out Part 1 first.","puzzle-3#Puzzle #3":"Just like in the previous puzzle, there's only one JUMPDEST we have to reach located at offset 0x1E and all we have to make sure to get there is, that the EQ comparison before the JUMP succeeds.This is actually the first time that storage is touched during these puzzles: SLOAD will consume one item from the stack as \"key\" and push the \"value\" it found there to stack. It's a simple read operation from a key-value store.We can see that the jump-condition can be written as: SLOAD(0x05) == 0xAA, basically we have to make sure that the value for the storage key 0x05 equals 0xAA. There's no SSTORE opcode writing to storage though, but there is a DELEGATECALL, meaning that we can run code of another smart contract on our context, which includes storage.The entire first part of the bytecode before the CREATE opcode is the same as before, so we'll again be able to deploy a contract via the calldata we send. Similarly, the bytecode before the DELEGATECALL is almost the same as it was for the CALL in the previous puzzle. The only difference is that one less 0x00 is duplicated, because there's no \"value\" to send here - that wouldn't make sense since any wei we send with the call would end up at the same contract again due to delegation.In summary: We have to send construction bytecode that returns runtime bytecode of a contract that'll store the value 0xAA at the key 0x05. After it was deployed that runtime bytecode will be delegate-called and we'll write to the puzzle's storage instead of the storage of the contract that was deployed. This storage value is then checked and if it was correctly set we'll successfully jump to the end.","runtime-bytecode#Runtime Bytecode":"This time we don't actually have to RETURN anything, instead we can just STOP (which is implicit at the end of bytecode) after writing to storage.Runtime bytecode: 0x60AA600555","construction-bytecode#Construction Bytecode":"Like last time, we write the runtime bytecode to memory so we can RETURN it during initiation.Construction Bytecode: 0x6460AA600555600052600A601BF3","solution#Solution":"This was relatively easy to solve since we could mostly re-purpose the simple solution from Puzzle #2.","puzzle-4#Puzzle #4":"We can see a new combination of opcodes here: First ADDRESS which pushes the puzzle contract's own address onto the stack, and then BALANCE which consumes this address and returns the amount of ether the account at said address has, in this case: The puzzle's balance which we can manipulate by sending value when prompted.Ignoring the first balance that is pushed onto the stack, what follows is the same CREATE operation as in the previous puzzles with the only difference that this time the entire puzzle's balance is forwarded to the contract being created.\nImmediately after, the returned address of the newly created contract is then consumed to obtain its balance. The old balance of the puzzle is then integer-divided by the current balance of the new contract and the result of this division must be 2 in order for the jump to happen. But without doing anything the result would currently always be 1.So to solve this puzzle the construction bytecode we send as calldata has to send away half of its balance. For example, if we send 4 wei to the puzzle, our contract receiving them has to burn 2 wei so that 4//2 == 2. The construction bytecode doesn't have to return any runtime bytecode since that's not necessary for it to simply hold on the the remaining balance.\nThe above construction bytecode sends 2 wei to the 0x0 address in order to burn it and then returns nothing as runtime bytecode.\nInitially I forgot that the old balance of the puzzle is used. Assuming that it's the current one I thought about having to send some of the balance back to the puzzle. Since sending value with CALL would trigger the puzzle's bytecode again though the only way to do that would be deploying another contract that uses SELFDESTRUCT to inject value into the puzzle without that happening. Surprised by how convoluted this solution seemed, I later noticed the challenge was actually way easier than that - took me a while though."}},"/posts/2022/5/24/more-evm-puzzles-part-1":{"title":"More EVM Puzzles - Part 1","data":{"":"May 24, 2022 by patrickd\nDalton Sweeney has recently published his own collection of EVM puzzles: \"10 more EVM puzzles\"! Promising to be even more challenging than the ones of Franco Victorio's original collection. Since we've solved all the other one's on this blog already and they've been good fun, let's do these too!Note that it's probably best to read my previous blog posts first, or that you've already solved the original version yourself, since I'll not explain all of the opcodes that were already discussed there in detail again.","puzzle-1#Puzzle #1":"The first puzzle is already quite complex, but as usual, we know that the goal is to ensure that the transaction doesn't revert, meaning it needs to reach the STOP opcode. And since there are each two JUMP and JUMPDEST opcodes, we most likely need to make sure to send a transaction that will ensure that both destinations are successfully jumped to.A JUMP consumes one item from the stack: The offset it should jump to, at which location needs to be a JUMPDEST opcode.The first 4 lines can be rewritten to the following pseudocode: jumpTo(CALLVALUE ** CALLDATASIZE) - and first thing to wonder about is: Can we jump directly to the last JUMPDEST skipping half of the challenge? Well, its offset is at 0x47 which is 71 in decimal, so sending a wei value of 71 and 1 byte as calldata should work? (because 71**1=71)\nIndeed! That worked, but it's kind of cheating so let's take a look at how this was probably intended to be solved and first jump to the JUMPDEST at 0x40 which is 64 - that means we can actually send any power of 2 that results in 64: 2*6, 4*3, 8**2After jumping there, the Program Counter's current value is pushed on the stack with the PC opcode. The handy evm.codes website tells us that this is \"the value of the program counter prior to the increment corresponding to this instruction\". So the Program Counter is basically a simple integer offset within the EVM that always keeps increasing by 1 while working through the contract's bytecode, except when it hits some kind of jump that overwrites the offset to point to a different location. We'll get the PC opcodes own address here, which is 0x41.This is then added to CALLDATASIZE and this sum becomes the offset the following JUMP will take us to. So to get to the final JUMPDEST at 0x47 we need to add 6 to the PC opcode's location at 0x41. That means we need a CALLDATASIZE of 6 and we already know that we can send 2**6 to keep the first jump working.\nIt appears we have now found the intended solution!","puzzle-2#Puzzle #2":"The CREATE opcode, previously reserved for the most challenging puzzles, is back and that already in the second one! This makes the puzzle a whole lot more intimidating, but the goal hasn't changed: Make a jump to the one and final JUMPDEST at offset 0x1F.This offset is already pushed onto the stack as target before the JUMPI, so what we have to do is making sure that the jump condition is met and the second item on the stack is a non-zero value. For that to be the case RETURNDATASIZE needs to be equal to 0x0A. That means the size of the output data resulting from the previous CALL opcode should be 10 bytes long.To understand what's happening with the CREATE and CALL opcodes, it's best to look at how the stack builds up and what each item will end up being used for:\nFirst of all CALLDATACOPY copies calldata to memory at offset 0x0 from the calldata starting at offset 0x0. The size of this copy operation is CALLDATASIZE, meaning the entirety of the calldata we send is copied to memory.After that, CREATE is executed with an ether value of zero (0x0) to send to the new contract, and to use the entirety of the calldata that we have just copied to memory at offset 0x0 as construction bytecode.So whatever we send as calldata will be executed as construction bytecode. And whatever the result of that is will be deployed as a smart contract. And finally we'll have the address of it as the top item on our stack.\nThis contract is then called with all of the gas that is still available at this point. All of the zeros the stack was filled with, tell CALL that there are no arguments to copy from memory (as new calldata for the call to the new contract) and that none of the return data should be copied to memory either. And finally, we'll have either 1 or 0 on the stack, telling us whether the call succeeded.But this success value isn't actually used. Instead, it gets the size of the data that was returned by the previous call with RETURNDATASIZE (data which is buffered somewhere for us within the EVM).So in summary: We need to send bytecode as calldata, that'll return a contract, that'll return 10 bytes of data when called.First: We need to build a runtime bytecode that returns 10 bytes.\nWe don't actually have to write anything to memory first in order to return it. Since the memory is already zero-initialized we can just return 10 zeros from it.Runtime bytecode: 0x600A6000F3Second: We need to build a construction bytecode that returns our runtime bytecode.\nHere we're writing the bytecode to memory and then we return it. You might wonder why the starting offset of the return data begins at 0x1B although we stored the runtime bytecode at 0x0 - that's because it'll look like this is memory: .We have to skip the 27 zeros that are at the beginning of the value to reach the bytecode. We could have prevented that using PUSH32 , but that'd have significantly increased the construction bytecode's size.Construction bytecode: 0x64600A6000F3600052600A601BF3\nBut that was somewhat boring, right? Wouldn't it be more fun if we only had one bytecode.. bytecode that returns itself and can be used for both construction and runtime.. and is 10 bytes long?We can actually do that quite easily because with the CODECOPY opcode the bytecode can copy itself:\nThis is 12 bytes long though, so we have to do some optimizations. A simple one is using duplication opcodes instead of pushing duplicate items onto the stack:\nThe bytecode is now exactly 10 bytes long and able to deploy itself:\nThis was a lot of stuff and we're only 2 puzzles in, so let's not overdo it and make it a series instead!"}},"/posts/2022/6/7/more-evm-puzzles-part-4":{"title":"More EVM Puzzles - Part 4","data":{"":"June 7, 2022 by patrickd\nThis concludes the series on Dalton Sweeney's \"10 more EVM puzzles\" collection. If you're looking for the start, take a look at Part 1.","puzzle-8#Puzzle #8":"Yes, that's right, I solved it immediately without reading by simply pressing enter on accident. What the heck just happened here?\nRight at the start it checks whether the call value sent is zero, which is strange since we were never given a chance to specify it.. Anyway, the result of this check is then inverted with NOT. Note that inverting a number 1 of type uint256 flips all of its bits, that means it doesn't turn into 0 but into the biggest number possible minus one. Since JUMPI jumps for any value different from 0, it would've jumped no matter which value we'd have sent since any non-zero value would have resulted in  after the inversion. So it seems we can just completely ignore this part...\nWe sent no calldata but it'll still attempt copying it to memory here. Then it'll deploy that empty calldata as initialization bytecode which doesn't return anything, so basically we're successfully deploying an empty contract. This contract is then immediately called. The puzzle obtains its own account balance using the SELFBALANCE opcode which is used for the first time here, and it would send all of this balance during the call if it had any. As things stand, this is a call made against an address that contains empty bytecode and the call succeeds. You shouldn't be surprised by that, after all we already know that we don't have to write STOP at the end of a contract. So an empty contract is just like a contract that has a single STOP instruction.Furthermore, we can even send invalid calldata that'll result in the CREATE opcode to fail and return the zero-address. Calls made to the zero-address will also succeed without reverting since it too has empty bytecode, like any uninitialized address. It's actually harder to fail this puzzle than to win it since we'd have to make the call fail by deploying runtime bytecode that'll revert.\nFinally, it checks whether the SELFBALANCE after the successful call is the same as the balance was before the call.So what was actually supposed to happen here? I think the first part was supposed to ensure that value must have been sent to the puzzle. Then all of this value would have been sent to the contract created using the specified calldata. This contract would have needed to send all of this value back to the puzzle contract to pass the final check. But it wouldn't have been able to do that via the CALL opcode since that would execute the puzzle code again which would send it away to another contract it created. Instead, the contract we created via calldata would've had to use the SELFDESTRUCT opcode to inject the value back into the puzzle during the call.The key takeaway here is that although uint256 are used as booleans by the EVM (0 is false, non-0 is true) doesn't mean that inverting it via NOT opcode will result in flipping the boolean, rather than that it'll flip every single byte in the uint256, and that is something that can be easily missed!","puzzle-9#Puzzle #9":"Right away, we're storing the CALLVALUE we send to offset 0x0 in memory. Then 2 new opcodes are introduced:You've probably heard about SHA3 before. It's a one-way hashing algorithm, the EVM makes this algorithm available to us via this instruction and passes it the data to hash via memory. Two items will be consumed from stack: The \"offset\" specifying where it should begin hashing the data and the \"size\" telling it when to stop. The resulting 256 bit long hash will be pushed as a new item onto the stack. Here, we'll always be hashing the first memory slot, which are the first 32 bytes (0x20).The SHR opcode stands for SHift Right, and that's exactly what it does on the bit level. It too consumes two items from the stack: The number of times to shift right and the value that should be shifted. For example, if the first item would be a 2 and the second a 17 (00010001), the result would be 4 (00000100). In case of this puzzle, we're always shifting 248 (0xF8) times and the value being shifted is the SHA-3 hash that was calculated just before.In case of the stack, we're shifting a value with 256 bits though: 256 - 248 is 8, and that means that everything except the first byte (most-left, or highest order) of the hash is shifted into nothingness. Finally the usual comparison and jump: Only if this byte is equal to 0xA8 the puzzle is solved!In summary: We need to find an integer value to send whose hash starts with 0xA8.Since hashing algorithms are one-way functions, there's no way to say which values will produce it without guessing. Luckily it's only about the first byte, and that should be pretty easy to stumble upon even when just continuously increasing the value by one.So what's the laziest way to do this? A shellscript! Don't judge me, it works and it only took me a couple minutes:\nGranted, it's not performant what so ever, but I didn't expect it to be a high number anyway, and it wasn't. It was 47, which is  and  after being shifted right.","puzzle-10#Puzzle #10":"Right at the beginning, the first 32 bytes (0x20) from calldata are copied to the first memory slot, where it's immediately read from again using MLOAD. Afterwards, a bitwise AND is applied to it with . This means that only the upper 4 bits of each byte from the first 32 bytes we sent as calldata are still there. The lower 4 bits have been \"masked\" out and the result is left on the top of the stack.The next 32 bytes from calldata are now copied to memory overwriting the first ones. This word of bytes is again loaded onto stack like before with MLOAD and then ORed with the result of the AND operation. The JUMPI condition to solve the puzzle is that the result of these operations ends up being equal to .If we call the first word from calldata A and the second word B, the following is the \"formula\" we have to solve here:\nIf we send zeros for A the result of the AND operation will be a zero-word too. That way we can discard A completely and just send the end result as B:\nPutting the words together as a single calldata, this is indeed a solution:\nBut bit-masking is an interesting thing to understand: If we had sent  as A, it would've ended up as  after the AND operation masked out the lower bits. Then we could've sent  as B which, ORed with A, would've produced the desired result too:"}},"/posts/2022/6/29/damn-vulnerable-defi-v2-12-climber":{"title":"Damn Vulnerable DeFi V2 - #12 Climber","data":{"":"June 29, 2022 by patrickd\nThis is the final part 9 of the write-up series on Damn Vulnerable DeFi V2. Please consider attempting to solve it on your own first since it's a lot less fun after being spoiled!\nChallenge #12 - ClimberThere's a secure vault contract guarding 10 million DVT tokens. The vault is upgradeable, following the UUPS pattern.The owner of the vault, currently a timelock contract, can withdraw a very limited amount of tokens every 15 days.On the vault there's an additional role with powers to sweep all tokens in case of an emergency.On the timelock, only an account with a \"Proposer\" role can schedule actions that can be executed 1 hour later.Your goal is to empty the vault.\nAn UUPS proxy.. maybe we can write into the special storage slot and overwrite the Logic Contract's location? Or maybe it's possible to trigger an emergency that allows us to drain the Vault? Maybe we can become a proposer and schedule a malicious action?","code-review#Code Review":"As usual, we start by looking at the scenario setup and success conditions that can be found in climber.challenge.js.\nNormally the accounts that tests get from ethers.getSigners() have a balance of 10.000 ether, but in this case it's ensured that the attacker account only has 0.1 ether available...\nAs the challenge teased, the Vault is indeed deployed behind a UUPS proxy. In case you're wondering, the array items deployer, proposer, and sweeper are the function arguments that are passed to the contract during initialization.\nThis part is simply making the Timelock available to us within the tests. The ClimberTimelock contract itself appears to be deployed as part of the construction of ClimberVault.And finally, the DVT Token contract is deployed and 10 million of them are transferred into the Vault. That's it for the setup.The success conditions aren't much to speak of either:\nAs expected, we just have to move all of the tokens to the attacker's account.Let's look at ClimberVault next since it seems to create ClimberTimelock.First of all the comment Upgrades are to be triggered by the owner. pops into my eyes and makes me wonder: Oh? Are they now? Is this a hint?Now I previously said that the Timelock contract appears to be deployed during the construction of the Vault - but the constructor is actually empty and there's a function called initialize instead. That's because you can't, or rather, you shouldn't make use of constructors in contracts that are behind a proxy. Constructors in Solidity are commonly used to initialize a contract's state, but it wouldn't make much sense to initialize the state of the Logic Contract since the state that will actually be used is the Proxy's. So all the constructor of the Logic Contract does is mark itself as initialized so nobody can trigger initialization on it directly. And instead, taken care automatically by upgrades.deployProxy(), the initialize function is called on the Proxy contract that delegate-calls into the Logic Contract. After that, the Proxy should be properly initialized according to the Logic Contract's needs.Note also that the proxy couldn't call into the constructor of the Logic Contract to do this since it's not part of the runtime bytecode - constructors are executed once, during the deployment of the contract and in fact they return the runtime bytecode, they're not part of it.\nSolidity normally takes care of inheritance and calling the constructors in the correct order but there's no such thing for initialize, therefore the initialize function starts with a list of other init functions that need to be called. Last time I checked there's no tool to help with doing that making this step error-prone and cumbersome.A quick way to check for correctness, in this case, could be OpenZeppelin's Contracts Wizard. Here we can get an automatically generated UUPS Proxied/Upgradable contract that is also Ownable and see that the order of init calls seems to be correct and complete.\nAs expected, here the ClimberTimelock contract is being deployed as part of ClimberVault's initialization. Since by default the ownership is given to the current msg.sender, it needs to be transferred to the Timelock here.\nFinally, the timestamp of the last withdrawal is set to the current time (or rather the timestamp contained in the current block set by its miner). This is probably to prevent the owner from being able to immediately make a withdrawal without 15 days passing first.\nThe withdraw function allows getting up to 1 Token every 15 days out of the Vault - and I don't really see a way around that here so let's instead look at the \"emergency\" sweeping function:\nApparently, the EOA that was declared to be the \"sweeper\" can just freely decide whether there's an emergency or not. Naturally, we don't have their private keys, so most likely we'll need some way to overwrite who the sweeper is, but I don't see a way to do that in this contract.I now expect to find some kind of issue in the ClimberTimelock contract that allows us to upgrade the Logic Contract to whatever we tell it to...The Timelock makes use of OpenZeppelin's AccessControl contract to have a role-based access control.\nWhat I find quite interesting is, that the Timelock contract itself is added as an administrator. Although the ADMIN_ROLE is seemingly unused in the Timelock, there are actually several public functions that it's inheriting from AccessControl, most importantly: grantRole(bytes32 role, address account). So it should be possible to grant roles via a proposal, since they are executed by this contract which is an admin.\nWhile the schedule function can only be called by proposers the execute function has an interesting comment:\nAlthough this function does indeed check that the operation state of a given ID must be ReadyForExecution, it does so after executing the function calls. That means we're free to do all the calls we want as long as we ensure that at the end of them there is an operation that is actually marked as ReadyForExecution.So we know that, since the contract is an admin, we can simply grant us the PROPOSER_ROLE as well and then schedule the proposal we're already executing before the operation state check is made. But what about getting the operation into the ready state that requires a 1-hour delay?\nWell, apparently we can just get rid of the delay before creating the proposal and have it ready immediately.[EDIT: On Discord silent_mastodon#1304 noted that it's actually unnecessary to update the delay at all since there's a subtle error in the getOperationState function]In Summary, we have to execute a proposal that doesn't exist and make sure that at the end of its execution it does exist after all and was ready to be executed. If we manage this we've basically taken control of the owner of the Vault.But in order to drain all of the tokens at once, we need to be the sweeper, and there's currently no external function that allows changing who that is after initialization. Good thing though that, as the owner, we're able to upgrade the Logic Contract to whatever we want!And we do that by calling upgradeTo(address newImplementation) which is a function that ClimberVault inherits from UUPSUpgradeable.","exploit#Exploit":"Normally when you want to upgrade a contract you want to do so while extending the old one. So the \"proper way\" would've been to create a contract ClimberVaultV2 is ClimberVault. The reason for doing so is that it ensures that the storage slot variables must stay aligned. If the ClimberExploit contract had any storage variables they would likely clash with the storage variables of ClimberVault. But we didn't need any storage variables in the exploit and we don't really care if we brick the Vault by replacing it with an entirely different contract, we just take the tokens and run!Adjusting climber.challenge.js...\nAaaand...\nThis exploit was really fun to put together! At first, I tried making the Timelock Contract the proposer instead and schedule-call within the operations, but I abandoned it soon since I got a headache from infinite recursion.Although it's not your textbook \"reentrancy\" vulnerability, it could in fact have been prevented by following the checks-effects-interactions pattern in the Timelock's execute function."}},"/posts/2022/8/23/paradigm-ctf-2022-trapdooor-amp-trapdoooor":{"title":"Paradigm CTF 2022 - Trapdooor & Trapdoooor","data":{"":"August 23, 2022 by patrickd\nWhen building complex puzzles, unintended solutions can get overlooked. Paradigm CTF's Trapdooor Challenge had a lot of potential for these and offered a deep rabbithole to dive into...","introducing-trapdooor#Introducing: Trapdooor":"DESCRIPTION: In theoretical computer science and cryptography, a trapdoor function is a function that is easy to compute in one direction, yet difficult to compute in the opposite direction (finding its inverse) without special information, called the \"trapdoor\".\nWhen I tackled Trapdooor, the second version (with an additional o) of this Challenge was already released mentioning that players had found a backdoor. The ZIP file of the updated version was password protected, so it wasn't possible to look for this backdoor by comparing them but it gave a big hint: There's probably a much easier unintended way to solve this Challenge.The Challenge's structure was quite different from the others. While there's still the usual chal.py script to set things up, there was no Setup.sol this time. Instead, there's a Script.sol and those familiar with Foundry should quickly realize that this is nothing supposed to be deployed anywhere. These scripts are only executed locally within a development environment, removing the need to switch between different languages during development.\nThese are the most significant parts of the python script that is executed every time we'll call into the Challenge (using the nc 34.68.217.8 31337 command):\nIt asks for hex-encoded runtime bytecode\nChecks whether the provided hex is valid\nReads the same Script.sol file that we were provided with\nGenerates two 128 bit prime numbers\nMultiplies both numbers and replaces the string NUMBER in the code with the result\nReplaces the string CODE in Script.sol with our hex-encoded bytecode\nWrites this updated version to a random temporary directory (usually located at /tmp)\nRuns the updated script with forge script Script.sol --tc Script\nChecks whether the process' return-code was successful (aborts if forge exited with an error)\nGets the last line from the forge stdout (what it would output to screen)\nPrints the last line\nChecks whether this last line starts with the string \"you factored the number!\", and if so, the Challenge's FLAG will be printed\nIn Script.sol we can find the NUMBER and CODE placeholders.When run, the bytecode that we provided is deployed via the Deployer contract. The assembly in its constructor will ensure that our runtime bytecode is returned and deployed instead of the original Deployer's runtime bytecode (which would be pretty much empty since it has no functions).Then the factorize() function is called on our deployed bytecode and it is supposed to return two numbers. Basically, we're supposed to find the original two numbers that were multiplied resulting in NUMBER and to ensure we didn't cheat several checks are done.If the checks pass and the script determines that we found the correct factors we get the message that the python script is looking for. Otherwise we'll get a negative message. It's interesting how the two numbers that our script found are returned as part of both log messages. Since the last message is printed we'll be able to see the numbers that our bytecode ended up picking.\nIf you haven't used foundry yet, you might have heard about the console.log() feature from Hardhat before, they're basically the same in Foundry. But normally you don't reimplement the console library like it's done here, you simply import it from forge-std.","initial-considerations#Initial Considerations":"Why isn't it using the official forge-std console.log library?A quick check in the official GitHub repository shows that, aside from missing all the other log() function versions, the _sendLogPayload() function is exactly the same. So it's unlikely that there's an issue with it. So maybe this is hinting that we need to somehow interfere with the logging?Are the number checks correctly implemented?If you've heard about RSA encryption you might know that its security is based on the fact that two really large, pseudo-random prime numbers multiplied yield a product with the following features: It can only be divided by either of the prime numbers, by itself and by one. The checks in the script ensure that we are only allowed to return both prime numbers. I can't see a problem with the checks.Is the randomness of python's getPrime() predictable?Numbers that computers generate are always pseudo-random. So they are somewhat predictable in theory but as far as I understand you usually need to know a sample of generated random numbers to be able to predict the next one. But we're never told the primes that were chosen, only the product. And if the challenge would require us to predict random numbers I'd expect it to be tagged with CRYPTO instead of PWN.Can the verification of the hex-encoded bytecode be bypassed?If that was possible, we could break out of the hex\"CODE\" string, console.log() the success message and then end the function before the checks are executed. It would basically allow us to rewrite the code:\nI was unable to find a way to make unhexlify() accept any non-hex input though and Internet search didn't give me a hint on how to do that either.Can we manipulate the order of messages?Since only the last line from the script's output is actually checked, we should be able to solve the Challenge by either\nprinting the success message and then somehow suppressing the failure message\nlogging the success message and then somehow re-order them to make it print last\nsomehow hooking into the print of the failure message and printing a success afterwards\nI tried various terminal escape characters and other special chars to manipulate the order in which the messages are displayed. And one actually worked, but only in a terminal - it didn't fool the python script, likely because the escape characters have no impact there and are simply read like any other:\nThe above will output a special control character that'll tell a console to move the cursor 5 lines up. So after our success message is printed, it'll print the failure message above it.Can we make use of cheatcodes?Once you enter this rabbit hole, it's hard to come back out...","enter-cheatcodes#Enter: Cheatcodes":"Foundry didn't entirely reinvent the wheel: When they wanted to add convenient logging, they re-implemented what Hardhat and DappTools already had on offer. When they needed cheats, they used what HEVM had already come up with.Cheatcodes are basically functions of a \"special contract\", located at a special address that do magical things. They only work in the context of the local chain, but they're very useful during testing.This was the beginning of scrolling through the list of cheatcodes over and over again:Function mocking?\nWhen I stumbled on this one, I was pretty sure that I found the solution: We can just make the address return static data instead of executing the actual function!\nThis didn't have any effect at all though! Weird.Timestamp manipulation?\nMaybe the output of the logs is sorted and if we change the timestamp or blocknumber between the messages they'll be re-ordered? ‚Äì Nope.Overwriting bytecode?\nAlthough I doubt that the \"magic logging address\" has anything to do with actual binary code, maybe overwriting it with code that always reverts has any impact?\nNope.FFI?\nFFI sounds fancy but it's quite simple: It allows you executing shell commands via a Solidity Script. Sounds dangerous? Well, I guess that's why it's turned off by default. There's nothing here implying that it has been turned on. No foundry.toml file and the python script doesn't pass --ffi as a parameter. Even so, I gave it a quick test... Nope.Environment variables?I was so sure about mockCall() being the solution that I got somewhat desperate at this point. I mean, mocking is a quite good metaphor for a trapdoor, right? Still though, it really felt like there's a lot more potential in this long list of cheatcodes, so I started seriously considering them one-by-one.\nWait.. environment variables.. wasn't there something...\nAH! This must be the backdoor to the trapdoor!","exploiting-the-backdoor#Exploiting the Backdoor":"The chal.py script gets the Challenge flag from the FLAG environment variable. And Foundry has a cheatcode that allows us to read environment variables. And thanks to the failure message printing a and b returned by our bytecode we can leak it to us.\nAfter testing and compiling this locally we can find the runtime bytecode in the artifact json-files contained within the out folder.\nUsing eth-toolbox.com we can conveniently convert these two leaked numbers to ascii strings:\nSolved!","introducing-trapdoooor#Introducing: Trapdoooor":"It's always a good sign when you feel like you've already run out of ideas before you start a new Challenge, oh well.Having the flag of the first Challenge's version, we're now able to open the archive of the second one. And as you might expect, things are mostly the same except for one significant change:\nThe newly added env={} will make sure we won't be ablet to access the system's environment variables.","digging-into-foundry#Digging into Foundry":"Not having any other leads, I'm starting to look into one question that is still bugging me: Why did mockCall() not work to suppress the failure message's call to the logging address?The explanation to this question can be found in inspector/cheatcodes/mod.rs: Basically various modules in foundry can define Inspectors that can hook into the EVM at certain points, such as a CALL being made, and change the EVM behavior. This file deals with catching CALLS made to mocked addresses and returning the mocked data instead of executing the actual code behind it. But before any of that, it checks whether the destination contract is the HARDHAT_CONSOLE_ADDRESS and in that case it'll skip this handling entirely.While exploring the repository I stumbled on something interesting though: In executor/abi/mod.rs where the \"abi bindings\" for all cheatcodes are defined, I noticed some cheatcodes I had't heard before:\nWe can do all this without the FFI flag enabled?After some more searching I found some documentation on these cheats in testdata/cheats/Cheats.sol:\nThis doesn't feel like it will lead to the intended solution, but maybe to another backdoor?","mirages-of-backdoors#Mirages of Backdoors":"No FFI? You don't tell me what to do!Remember how I mentioned that FFI is usually turned off by default? Well, now that we know we can write files, can't we just create a config file and flip it on?\nThat definitely does create a file...\nAnd yes, FFI works now. Huh. It feels like this shouldn't be allowed...Now that's an interesting finding, but how could we actually exploit it for this challenge? For the new configuration file to become active we need to execute another forge script within the same current-working-directory. And these switch to another temporary directory with each attempt. Seems like a dead end.. too bad!Looking for environmental leaksHave you heard about /proc/$PID/environ? Just like there are special \"addresses\" in Ethereum to do magical things, there are special \"files\" in linux that contain all of a process' environment variables.\nBut trying to run it...\nRight. So I can enable FFI but accessing a system file is asking for too much. C'mon man.Tried various things to bypass this blacklist, but the only thing I found is this: You can access forbidden paths only if you're already in one. So our script needs to be executed somewhere within /proc for us to be able to access that \"file\". Another dead end..Info please?If you spend a lot of time in the Paradigm CTF 2021 GitHub Repository, you might remember there being \"info.yaml\" files in each Challenge's directory:\nNow wouldn't it be convenient if we could access them?\nAccording to the Dockerfile for this challenge however, they'd be out of reach. Only files contained in /public/deploy/ are being copied and the info.yaml would be just outside of public...Attempting total pwnageNow that we have already spent some time on how a Challenge's environment is set-up.. might as well go all the way!Each of the Challenges is running within a container based on gcr.io/paradigmxyz/ctf/eth-base:latest. We can fetch ourselves a copy of this Docker image and explore it a bit.\nInteresting. So every time we call into a Challenge via nc, the xinetd service will start a /home/ctf/handler.sh script and that will in turn run the actual chal.py.Now, it would be really nice to be able to overwrite any of the files involved and execute our own little leak-the-flag-service, but even if we could bypass Foundry's path blacklist there's a problem: xinetd is configured to run each request under the user ctf and all of the interesting files belong to root and are not writable for anyone else.\nWell, I tried anyway! Turns out that the home directory wasn't blacklisted by foundry and this worked locally. But the real deal was prevented by file permissions, as expected.(Now that I'm writing this, I'm wondering whether I could've written to /home/ctf/.bashrc to execute arbitrary commands each time a script runs... Hmmm...)","circling-back#Circling Back":"At this point, I was completely out of ideas and went back to where I started. This is when I realized that there was still another question to answer: Although we now know why mockCall() didn't work, why did setting the logger address' bytecode to revert not cause a revert?The answer made me facepalm:\nBecause the status r returned by staticcall() is never checked (it's probably just there to suppress compiler warnings). So even if it reverts, we'd never notice... But doesn't that mean that we might be able to execute bytecode here after all?After reading the Foundry source, I now know that the log-call is being handled by an Inspector at /executor/inspector/logs.rs. These are basically pre-hooks and executed before normal EVM behavior. So the failure message will be logged first and only after that, the EVM will continue on. So if we use the etch() cheatcode to set bytecode for the logging-address that makes another log-call but with the success message, then we should have the correct order to solve this challenge?","the-intended-solution#The (intended?) Solution":"This actually ends up creating an infinite loop, but that's no issue since it'll run out of gas and errors are being ignored anyway.\nAfter copying the bytecode from the build artifact in /out we can paste it into the challenge server. The python script will be able to find that the last line of stdout started with \"you factored the number!\" and we get a FLAG as a reward.What a journey."}},"/posts/2022/8/29/secureum-bootcamp-epoch-august-race-9":{"title":"Secureum Bootcamp Epoch‚àû - August RACE #9","data":{"":"This is a write-up of the Secureum Bootcamp Race 9 Quiz of Epoch Infinity with solutions.\nThis quiz had a strict time limit of 16 minutes for 8 questions, no pause. Choose all and only correct answers.Syntax highlighting was omitted since the original quiz did not have any either.\nAugust 31, 2022 by patrickd","code#Code":"All 8 questions in this RACE are based on the following contracts. You will see them for all the 8 questions in this RACE. The questions are below the shown contracts.","question-1-of-8#Question 1 of 8":"The function signature is the first 4 bytes of the keccak hash which\n A. Includes the function name \n B. Includes a comma separated list of parameter types \n C. Includes a comma separated list of return value types \n D. Is generated only for public and external functions \nCorrect is A, B, D.A function's signature is created by hashing its name and a comma separated list (no spaces) of the types of all its parameters. For example: add(uint256,uint256){:solidity}.The fact that the return value type isn't part of the signature is basically given away by the fact that the creation of the counter() function's signature doesn't mention int256.Since it's used for calling external and public functions of a contract, only these functions need a signature to be called by. Internal and private functions can only be directly JUMPed to within the bytecode of the contract that contains them.","question-2-of-8#Question 2 of 8":"The Proxy contract is most similar to a\n A. UUPS Proxy \n B. Beacon Proxy \n C. Transparent Proxy \n D. Metamorphic Proxy \nCorrect is C.A UUPS (or Universal Upgradeable Proxy Standard) would have it's upgradeability logic within the implementation which ensures there won't be function signature clashes. This is not the case here with the setImplementationForSelector() function being part of the Proxy.A Beacon Proxy would ask another contract where it can find the implementation, this isn't the case here since the implementation address is managed and stored in the Proxy contract itself.This makes it most similar to a Transparent Proxy.A \"Metamorphic\" Proxy isn't really a thing. Contracts referred to being metamorphic usually achieve upgradeability not thanks to a proxy, but due to the fact that they can be re-deployed to the same address via CREATE2.","question-3-of-8#Question 3 of 8":"Gas will be saved with the following changes\n A. Skipping initialization of counter variable \n B. Making increase() function external to avoid copying from calldata to memory \n C. Packing multiple implementation addresses into the same storage slot \n D. Moving the calculation of the counter() function's signature hash to a constant \nCorrect is A.Avoiding initialization of state variables to zero can indeed save gas and are usually not necessary when deploying contracts to fresh addresses where all state variables will be zero-initialized by default.If initialization in the Mastercopy contract would attempt setting a value different from 0 it wouldn't even have any effect, since it's not setting this value in the Proxy's state - this would be considered a bug.The increase() function does not have any function parameters that are being copied from calldata to memory. Introducing this change would have no effect.Addresses are too large (20 bytes) for multiple of them to be packed into a single storage slot (32 bytes).Constants are basically placeholders in the bytecode for expressions that are filled during compile time. It would not make a difference whether the compiler fills them or whether we've already \"filled\" them by hand. It might however improve readability to do so.","question-4-of-8#Question 4 of 8":"Calling the increase() function on the Proxy contract will\n A. Will revert since the Proxy contract has no increase() function \n B. Will revert for any other caller than the one that deployed the Proxy \n C. Increases the integer value in the Proxy's storage slot located at index 1 \n D. Delegate-call to the zero-address \nCorrect is B, C.When the Proxy is called with the function signature for increase(), Solidity will call the fallback() function since the Proxy contract itself does not have a function with a matching signature.The fallback() function will determine that, for this signature, it has stored the mastercontract's address as an implementation and will delegate-call it.The Mastercontract's code will be execute in the context of the Proxy contract, meaning that the state being manipulated by the Mastercontract's code is that of the Proxy.The function-selection logic of the Mastercontract will find that it indeed has a matching function signature belonging to increase() and will execute it.The increase() function will increment the value of the counter state variable by one, who's index is at 1 because the first index is already reserved by Ownable's owner state variable.This means that whatever value is currently located at the Proxy contract's storage slot with index 1 will be increased by one even if there's no variable called counter in the Proxy itself.","question-5-of-8#Question 5 of 8":"Calling the decrease() function on the Proxy contract will\n A. Will revert because it was not correctly registered on the proxy \n B. Will succeed and return the value of counter after it was decreased \n C. Will succeed and return the value of counter before it was decreased \n D. Will succeed and return nothing \nCorrect is D.When checking for the implementation address of the decrease() function's signature, the Proxy contract won't find one since it wasn't registered in the constructor like the increase() function was.But that doesn't mean it'll revert, it'll instead get the default state value: The zero address.Since no check is made to prevent calls when no matching signature is found in the implementations mapping, a delegate-call will be made to the zero address, and like all calls that are made to addresses that do not have runtime bytecode, this call will succeed without returning anything.The EVM implicitly assumes that all bytecode ends with the STOP opcode, even if the STOP opcode isn't explicitly mentioned in the bytecode itself. So to the EVM an empty bytecode actually always contains one opcode: STOP - the opcode for stopping execution without errors.","question-6-of-8#Question 6 of 8":"Due to a storage clash between the Proxy and the Mastercopy contracts\n A. The Proxy's implementations would be overwritten by 0 during initialization of the Mastercopy \n B. The Proxy's implementations would be overwritten when the counter variable changes \n C. The Proxy's implementations variable's storage slot being overwritten causes a DoS \n D. None of the above \nCorrect is D.Mappings leave their assigned storage slot unused. The actual values of a mapping are stored at location's determined by hashing the mapping slot's index with the key.That means that, even though the Proxy's implementations and the Mastercopy's counter variables make use of the same slot, they actually do not interfere with each other and nothing will happen when counter's value changes.","question-7-of-8#Question 7 of 8":"The Proxy contract\n A. Won't be able to receive any ether when calldatasize is 0 due to a missing receive() \n B. Will be the owner of the Mastercopy contract \n C. Has a storage clash in slot 0 which will cause issues with the current mastercopy \n D. None of the above \nCorrect is B.Thanks to its payable fallback() function it'll still be able to receive ether without issues.Ownable always initializes the owner with the msg.sender. When the Proxy deploys the Mastercopy contract, the Proxy will be the msg.sender and therefore become the owner of the Mastercopy contract.Both the Proxy contract and the Mastercopy contract first inherit from Ownable ensuring that the storage slot at index 0 will be used in the same manner on both contracts preventing any issues.","question-8-of-8#Question 8 of 8":"The fallback() function‚Äôs assembly block\n A. Can be marked as \"memory-safe\" for gas optimizations \n B. The result of the delegate-call overwrites the the call parameters in memory \n C. Interferes with the Slot-Hash calculation for the implementations-mapping by overwriting the scratch space \n D. None of the above \nCorrect is B.The assembly block doesn't respect Solidity's memory management and can't be considered to be \"memory-safe\". And even if it did, this Solidity version does not have the option to mark assembly blocks as such yet, this was introduced with version 0.8.13.The use of the CALLDATACOPY opcode will copy the full CALLDATASIZE to memory starting at offset 0. Then, after the delegate-call was finished, the use of the RETURNDATACOPY opcode will copy the full RETURNDATASIZE to memory, also starting at offset 0. This effectively means that the output will overwrite the input of the delegate-call.The slot-hash calculation has already finished when the assembly block begins, therefore there should not be any interference by overwriting the scratch space that was used for it."}},"/posts/2023/5/1/race-17-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #17 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-17, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors.   It was designed by Secureum Mentor Desmond (aka Hickup), one of my colleagues at Spearbit DAO.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nMay 1, 2023 by patrickd","code#Code":"All 8 questions in this RACE are based on the below contract. This is the same contract you will see for all the 8 questions in this RACE. The question is below the shown contract.","question-1-of-8#Question 1 of 8":"deposit() can revert:\n A. If insufficient ETH was sent with the call \n B. If the caller has insufficient WETH balance \n C. With the \"cap exceeded\" error \n D. If called internally \nCorrect is A, B.\nA: If the ether (msg.value > 0) provided with the call to deposit() is less than the specified amount, the attempt to call WETH.deposit() may revert.\nB: If no ether was provided with the call and the caller has an insufficient balance of WETH, or given insufficient approval to their WETH balance, the attempt to call WETH.transferFrom() will revert.\nC: For the \"cap exceeded\" error to be thrown, totalContributions + amount > TOTAL_CONTRIBUTION_CAP. But TOTAL_CONTRIBUTION_CAP = type(uint72).max and totalContributions is uint72. So the attempt to add an amount to totalContributions that would make it larger than type(uint72).max would revert from an integer overflow error before the require() ever could. This makes the require() basically redundant and it can be removed.\nD: The deposit() function is a external. An attempt to call it internally would not revert but never compile in the first place. The contract could still call this function via this.deposit() though, which would make the contract CALL itself like it would an external contract.","question-2-of-8#Question 2 of 8":"What issues pertain to the deposit() function?\n A. Funds can be drained through re-entrancy \n B. Accounting mismatch if user specifies amount > msg.value \n C. Accounting mismatch if user specifies amount < msg.value \n D. totalContributionCap isn't enforced on an individual level \nCorrect is C.\nA: For a re-entrancy an unsafe external call needs to be made. All the external calls being made within the deposit() are to the trusted WETH contract which does also not have any sender callbacks/hooks like an ERC777 would have.\nB/C: If the specified amount was larger than the sent msg.value, the function would revert. But on the other hand, if the amount was smaller than the actual sent msg.value the deposit would only handle the amount and the rest of the ether would be left in the Vault contract (allowing someone else to pick it up on another deposit).\nD: The fact that totalContributionCap isn't enforced on an individual level does not cause an issue as totalContributions's value would revert before any individual contributor would be able to make deposits beyond the cap.","question-3-of-8#Question 3 of 8":"Which of the following is/are true about withdraw()?\n A. Funds can be drained through re-entrancy \n B. Funds can be drained due to improper amount accounting in deposit() \n C. Assuming a sufficiently high gas limit, the function reverts from the recipient (caller) consuming all forwarded gas \n D. May revert with \"failed to transfer ETH\" \nCorrect is D.\nA: The withdraw() function follows the Checks-Effects-Interactions pattern: (Check) Subtracting the amount from the individual's contribution would revert if the integer were to underflow. (Effect) The individual contributor's balance is updated before the value is transferred. (Interactions) The unsafe external call to the msg.sender is only made once all Checks and Effects have been applied. Re-entering the contract would not allow draining any funds.\nB: It's true that there's improper accounting, but the effect is that funds that were left over from a deposit() can be recovered/stolen by another depositor and then withdrawn. This does not allow to drain the Vault of any funds that have been properly accounted for though.\nC: As only  gas is forwarded, the function should have sufficient gas remaining for execution (hence the high gas limit assumption).\nD: True, if msg.sender reverts (eg. is a contract that lacks payable / fallback function).","question-4-of-8#Question 4 of 8":"Which of the following parameters are correctly emitted in the ContributorsUpdated() event?\n A. newContributor \n B. oldNumContributors \n C. newNumContributors \n D. None of the above. \nCorrect is A.\nA: Option A is understandably ambiguous: if withdrawals were working, then you could have an existing contributor be recognized as a new one. Nevertheless, in its current state, we can take it to emit for only legitimately new contributors.\nB: Generally, it's not save to make assumptions about the evaluation order of expressions in Solidity. How obscure this can be especially for event emissions has been shown in last year's Underhanded Solidity contest: \"The indexed parameters are evaluated first in right-to-left order, then the non-indexed parameters are evaluated left-to-right\". Because of this, the general best practice is to avoid nested expressions whenever possible and do them within separate lines of code.\nC: One of the most common gas optimizations seen in Code4rena reports is how the prefix increment (++i) is more efficient than postfix (i++). However, most aren't aware of a key difference: Prefix increments returns the value AFTER the increment, postfix returns the value BEFORE.","question-5-of-8#Question 5 of 8":"The vault deployer can pause the following functions:\n A. deposit() \n B. withdraw() \n C. requestAllowance() \n D. None of the above \nCorrect is D.The contract can't be paused because the pause and unpause functionality aren't exposed.Author notes: Sort of a trick question that requires knowledge about the Pausable contract. As mentioned in the Spearbit community workshop I gave recently, it can be difficult to spot what's absent, not just what's present.","question-6-of-8#Question 6 of 8":"What is the largest possible allowance given to the controller?\n A. 40% of totalContributionCap \n B. 60% of totalContributionCap \n C. 100% of totalContributionCap \n D. Unbounded \nCorrect is C.The allowance is only capped as long as the specified amount is larger the current totalContributions. That means as soon as the totalContributions are larger than ALLOWANCE_CAP, it's possible to give an allowance of 100%.","question-7-of-8#Question 7 of 8":"The requestAllowance() implementation would have failed after the 1st call for tokens that only allow zero to non-zero allowances. Which of the following mitigations do NOT work?\n A. safeApprove(0) followed by safeApprove(type(uint256).max) \n B. safeIncreaseAllowance(type(uint256).max) \n C. safeIncreaseAllowance(0) followed by safeIncreaseAllowance(type(uint256).max) \n D. safeDecreaseAllowance(0) followed by safeApprove(type(uint256).max) \nCorrect is B, C, D.There are some implementations of ERC20 tokens that require an approval to be reset to 0 before it can be updated to another non-zero value.The safeIncreaseAllowance() / safeDecreaseAllowance() functions request the current allowance, adding/subtracting the specified amount and then update it by calling approve(). These functions do not set the approval to 0 in between, so for these tokens the function would still fail after the 1st call.","question-8-of-8#Question 8 of 8":"Which of the following gas optimizations are relevant in reducing runtime gas costs for the vault contract?\n A. Changing ALLOWANCE_CAP type from immutable to constant, ie. uint256 public constant ALLOWANCE_CAP = 40 * uint256(TOTAL_CONTRIBUTION_CAP) / 100; \n B. Increase number of solc runs (assuming default was 200 runs) \n C. Renaming functions so that the most used functions have smaller method IDs \n D. Use unchecked math in withdraw() \nCorrect is B, (C).\nA: Changing ALLOWANCE_CAP to be constant would actually consume more runtime gas as the expression would then be evaluated on every call, while with immutable the expression would be calculated during deployment and then become a static value. (Note that there's no difference between these options anymore when the via-IR compilation pipeline and optimization is used).\nB: Increasing the optimizer's runs configuration will increase the deployment bytecode size but decrease the gas usage at runtime.\nC: (The smaller the function's ID the earlier it can be found by the function selector.) ‚Äì In hindsight, this is not entirely true: Including the public variable getters, the Vault contract has 9 function signatures exposed. For contracts with more than 4 function IDs the Solidity compiler starts using a binary search algorithm instead of using a sorted-if-then selector. So in the case of the Vault contract, there's no guarantee that renaming the functions in the described manner would actually ensure reduced runtime costs for all of the most used functions. It's still true that the function with the lowest ID will have the shortest path (one pivot, on EQ), but the same is not guaranteed for the other functions.\nD: It wouldn't be safe to use unchecked math in the withdraw() function as it would potentially allow users to withdraw more than they should be able to. (The wording is intended to imply that the entire function's body would be put into an unchecked block.)"}},"/posts/2023/9/5/race-21-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #21 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-21, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. Secureum Mentor Noah, one of my colleagues at Spearbit DAO.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes (yes, the time limit includes reading the code). If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nSeptember 9, 2023 by patrickd","code#Code":"All 8 questions in this RACE are based on the below contracts. This is the same contracts you will see for all the 8 questions in this RACE. The question is below the shown contracts.","question-1-of-8#Question 1 of 8":"Which of the following statement(s) is/are true about the burry() function?\n A. The function always reverts when an out of bounds index (idx) is passed\n B. The function always reverts when localBunnies[idx].rabies is not Rabies.Symptomatic\n C. The function always reverts when less than 7 days have passed since the Rabbit was minted\n D. None of the above\nCorrect is A.\nA: Solidity automatically checks for out-of-bounds index access and will revert.\nB/C: There is an error in the function, the && should be || as it should revert when either side is true. At the moment it would require both conditions to be true in order to revert.","question-2-of-8#Question 2 of 8":"clones-with-immutable-args are used to deploy ResearchLabs. Which of the following statement(s) about this pattern is/are true?\n A. Arguments originating from the proxy to the implementation are immutable\n B. All arguments passed to the implementation are immutable\n C. Gas costs are lower when cloning versus deploying new implementations\n D. The implementation can be selfdestruct‚Äôed by a malicious caller\nCorrect is A, C, D.This is based on the Astaria disclosure (described here by @devtooligan). Don‚Äôt let the name fool you, the proxy is where args are immutable; the implementation receives these arguments as calldata.When called by the proxy, everything works as intended. When the implementation is called directly, the caller controls the arguments and delegatecalls become particularly problematic.\nA: As long as the proxy is involved, the arguments originating directly from it are indeed immutable.\nB: The proxy may receive calldata from the msg.sender. It appends the immutable arguments to this before forwarding the call.\nC: This is true because the bytecode of a proxy is usually much smaller and therefore cheaper to deploy than the actual implementation it points to (proxy clone pattern)\nD: The ResearchLab contract can be directly called (without a proxy passing a valid immutable argument). This means that the address it delegate-calls to can be pointed to a contract that makes it self-destruct.","question-3-of-8#Question 3 of 8":"The adopt() function has a random calculation. Which of the following statement(s), if any, about the source of randomness is/are true?\n A. By using Flashbots a caller can guarantee that Bunny.rabies == Rabies.None\n B. There is an error present that can be corrected by hashing randomSeed before using it\n C. Integer overflow introduces unexpected behavior in this function\n D. None of the above\nCorrect is D.There is certainly a problem with the randomSeed, it is not random at all. The use of blockhash for the current block returns 0 (see evm.codes) meaning randomSeed behaves more like a constant than a random number.\nA: Misdirection: Not block hash will always be 0, can't be influenced using Flashbots.\nB: Misdirection: Hashing a constant value will just return another constant.\nC: Misdirection: No overflows possible in this Solidity version without the use of unchecked-blocks","question-4-of-8#Question 4 of 8":"Which of the following statement(s) is/are true about the adopt() function?\n A. Minting does not take place unless precisely ADOPTION_PRICE is paid in lidoToken\n B. lidoToken is a known token and is immutable in this contract, therefore transferFrom is safe to use\n C. In some but not all cases the burry() function can cause the adopt() function to be DOS‚Äôd\n D. Duplicate bunnies array entries will be produced if they burn their Rabbits NFT using the Rabbits ERC721 contract directly instead of using the RabidRabbits.burry() function\nCorrect is C.\nA/B: The use of Lido token introduces MiniMe Token to the system. These tokens do not revert on failure and instead return false. Even though a user cannot introduce a problematic token to the system, we have caused the issue ourselves in the constructor. The unchecked return value, i.e. not using safeTransferFrom, allows calls to succeed even when no tokens are transferred.\nC: burry() reduces the length of the bunnies storage array. In the adopt function bunnies.length is used to decide which token id to mint. If any but the last bunnies index is buried, the array length decrements and the next call to adopt fails due to attempting to mint a duplicate token id.\nD: While the array entries of those burned bunnies will remain, this won't cause duplicates as the array length isn't decreased, and therefore later bunnies will have a non-duplicate id.","question-5-of-8#Question 5 of 8":"TrulyRandomOracleMock.oracleResult() calls calculateResult() passing a struct in memory. calculateResult() attempts to modify the memory struct and does not return a value. Finally, revealResearchResult() writes the entropy struct to storage; this is going to:\n A. Silently fail to update the newEntropy struct and will store the original\n B. Correctly update and then store the newEntropy struct\n C. Revert at runtime\n D. Fail at compile time\nCorrect is B.When an internal function operates on memory, memory is updated in place. Contrasted with external calls where memory may be passed but is not updated in the calling contract.\nA/B: The oracleResult function calls calculateResult and passes it the entropy state variable's value as a reference in memory. Since calculateResult is an internal function, it's able to adjust the Entropy values as oracleResult uses the same memory. This updated memory reference variable is then used to update the value in storage.\nC/D: Misdirecting answers. It doesn't revert and compiles fine.","question-6-of-8#Question 6 of 8":"Which of the following statement(s) about the constructor is/are true?\n A. Gas can be saved by not double casting lidoToken = IERC20(_lidoToken)\n B. Gas can be saved by making rabbitToken immutable\n C. Gas can be saved by storing cloneArgsTarget as bytes \n D. _lidoToken as an argument helps in deploying on multiple networks\nCorrect is B, D.\nA: At the bytecode level, casting is a noop and incurs no gas.\nB: Immutable is cheaper than reading from storage\nC: bytes would be a storage array (immutable variables cannot have a non-value type) meaning we would incur sload costs.\nD: True, we can reference bridged Lido on other networks when deploying there. Further, testing and fuzzing benefits exist when not hardcoding addresses.","question-7-of-8#Question 7 of 8":"Which of the following statement(s) is/are true about the burry() function?\n A. A user can DOS the burry() function for their idx if they burn their Rabbits NFT using the Rabbits ERC721 contract directly instead of using the RabidRabbits.burry() function\n B. The Bunny.researchLabs array is correctly reset for the deleted idx\n C. The entire burry() function can be DOS‚Äôd resulting in all calls to this function failing\n D. None of the above\nCorrect is A, B, C.A: This function attempts to burn, meaning that if a token is burned already, calls to burry will revert.B: While possibly high in gas costs, the array is cleared and the index may be reused in the adopt function.C: Whatever we thought we were doing to be efficient in our array handling is negated by loading the entire array into memory with Bunny[] memory localBunnies = bunnies;{:solidity}. With at least 5 storage reads per array entry, after a few thousand entries in the array, the block gas limit is hit trying to load it all into memory.","question-8-of-8#Question 8 of 8":"Which of the following statement(s) is/are true about ResearchLab.revealResearchResult() and the functions it interacts with?\n A. The Solidity compiler will fail to compile\n B. The storage for entropy is updated\n C. The storage for researchEndeavors[idx] is updated\n D. None of the above\nCorrect is B.\nA: Misdirection: This compiles without issues.\nB/C: This question is included as a reminder to be aware of storage and memory context when calling between functions. To step through the functions and argument passing: revealResearchResult references endeavor in storage then calls trulyRandomExternalCall which loads the data into memory. TrulyRandomOracleMock.oracleResult is delegatecall‚Äôed again with the endeavor data passed as memory. Delegatecall means TrulyRandomOracleMock.oracleResult has access to all of our storage, including entropy which it updates in storage. endeavor on the other hand is in memory, meaning researchEndeavors[idx] is not updated at all."}},"/posts/2024/1/30/race-26-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #26 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-26, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. Secureum Mentor Luksgrin (Bronicle) tests our understanding of Vyper's intricacies. Solutions are based on the notes from both Lucas and pcaversaccio.\nParticipants of this quiz had to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nJanuary 30, 2024 by patrickd","question-1-of-8#Question 1 of 8":"Given the following Vyper contract, which of the following statements are true?\n A. The contract is vulnerable to reentrancy attacks \n B. The contract is vulnerable to denial of service attacks \n C. The contract is vulnerable to overflow attacks \n D. None of the above \nCorrect is A.\nA: The Vyper version used, 0.3.0, is affected by a compiler bug that allows cross-function reentrancy between transfer() and withdrawAll() despite the fact that both are using a nonreentrant decorator with the same mutex key. Effectively, an attacker could make a call to withdrawAll() to initiate the withdrawal of the deposit, but before the user balance is reset to 0, a raw_call is made back to msg.sender. This triggers a malicious contract's fallback handler, allowing the attacker to reenter the contract through transfer() and move the balance to another address before it is reset to 0. This process can be repeated over and over again until the entire vault is drained. There are two other Vyper versions affected by this bug: 0.2.15, 0.2.16. You can see how this affects the storage layout using the following compiler command: vyper -f layout vault.vy.\nB: Would mean that the contract can be brought into a state that denies service to users. In this contract's case, there isn't really any state that is shared between users that would allow a permanent denial of service.\nC: Vyper has native protection against arithmetic overflows: \"Bounds and overflow checking: On array accesses as well as on arithmetic level.\"","question-2-of-8#Question 2 of 8":"Given the following Vyper contract, which of the following statements are true?\n A. The reentrancy locks are not necessary \n B. The reentrancy locks do not protect against cross-function reentrancy \n C. Collisions in the _balances array can occur in theory, but are unlikely to occur in practice \n D. Collisions in the _balances array can occur both in theory and in practice \nCorrect is A, C.\nA: The functions correctly follow the Checks-Effects-Interactions pattern, therefore the reentrancy locks are indeed unnecessary.\nB: The Vyper version used in this snippet has reentrancy locks that work as intended.\nC/D: An address type consists of 20 bytes, which is 160 bits. The _indexer() function maps arbitrary addresses to a space that is around 64 bits - a much smaller than addresses. This theoretically allows finding two addresses that would map to the same index, but it would still be impractical to actually exploit this:\nLet  be the total possible indexes in our array (). Using the birthday paradox, the probability of a collision after using  different addresses is .To have a  chance of collision, we must solve for  in , which has a solution , which implies that to have a  chance of collisions to occur, more than half of the entire planet must have an ethereum address and have interacted with the contract.So, in practice, impossible.","question-3-of-8#Question 3 of 8":"Given the same Vyper contract from previous Question, select the true statement(s):\n A. A malicious admin can steal all the funds in the contract \n B. Anyone can hijack the admin in theory, but it is highly unlikely to happen in practice \n C. Anyone can hijack the admin both in theory and in practice \n D. None of the above \nCorrect is A, B.\nA: A malicious administrator can call the kill() function which executes SELFDESTRUCT. This will destroy the contract and send all of its balance to the msg.sender (the admin calling the function).\nB/C: While the probability of a collision is very low, in theory one could call the contract from an address whose index (as returned from _indexer()) is . The balance being written to this storage slot would break the contract because Vyper versions <= 0.3.7 are affected by a compiler bug that can cause storage clashes when large arrays are used.","question-4-of-8#Question 4 of 8":"Given the same Vyper contract from previous Question, what would be the Solidity equivalent of the _indexer function?\n A. function _indexer(address _address) private pure returns (uint256) { return uint256(sha256(abi.encodePacked(_address))) % MAX_USERS; } \n B. function _indexer(address _address) internal view returns (uint256) { return uint256(sha256(abi.encode(_address))) % MAX_USERS; } \n C. function _indexer(address _address) internal pure returns (uint256) { return uint256(sha256(abi.encodePacked(_address))) % MAX_USERS; } \n D. function _indexer(address _address) internal pure returns (uint256) { return uint256(sha256(abi.encode(_address))) % MAX_USERS; } \nCorrect is D.Like its Vyper equivalent, the function should be internal and pure, that leaves only options C and D. It cannot be C since the hash resulting from the input of abi.encodePacked() would not be equivalent to the _indexer() function due to its difference in encoding the address (it would not add zero-padding to the address to fill full 32 bytes).","question-5-of-8#Question 5 of 8":"Which function would be more suitable to deploy a Gnosis Safe module?\n A. create_minimal_proxy_to \n B. create_copy_of \n C. create_from_blueprint \n D. None of the above \nCorrect is A.All of these functions are \"built-ins\" for contract creation: \"All three contract creation built-ins rely on the code to deploy already being stored on-chain, but differ in call vs deploy overhead, and whether or not they invoke the constructor of the contract to be deployed.\"\ncreate_minimal_proxy_to(address, ...) deploys an immutable proxy pointing to a specified implementation contract. Minimal proxies commonly are usually EIP-1167 forwarder bytecode and use DELEGATECALLs, making them cheap to deploy but expensive to call.\ncreate_copy_of(address, ...) makes a copy of the runtime code at the specified address, making it expensive to deploy but cheaper to call than a proxy.\ncreate_from_blueprint(address, ...) re-deploys the contract at the specified address by using its initialization code (ie. the constructor will be executed too). Being a copy instead of a contract makes it again expensive to deploy but cheaper to call.\nIn Safe Wallets, modules follow EIP-1167, which is what create_minimal_proxy_to() implements. In theory you could also use create_from_blueprint() to deploy a Gnosis Safe module, but it's much more complex to do so.","question-6-of-8#Question 6 of 8":"Given the functions below, which of the following statements are true?\n A. This contract cannot be compiled as it calls an external function from another external function \n B. weird1 and weird3 are equivalent \n C. weird2 and weird3 will always revert \n D. weird1 and weird4 are equivalent \nCorrect is D\nA: The contract compiles just fine and there's no general problem with an external function calling another external function in Vyper.\nB: The weird3() function does not encode the function signature correctly. Signature should be added through method_id instead of being ABI encoded as a parameter with padding.\nC: The weird2() function, similarly to weird3(), does not correctly prepend the signature to the ABI encoded calldata. Furthermore, the signature isn't even calculated correctly since the hash is not reduced to 4 bytes. But, despite all this, this doesn't necessarily mean that they'll always revert.\nD: Both weird1() and weird4() correctly use method_id to specify the function signature. Also, the result of method_id(\"unsafeSum(uint256,uint256)\") is equivalent to hashing and shortening the string's hash down to 4 bytes, resulting in the same signature 0x4f388eb1.","question-7-of-8#Question 7 of 8":"Under which circumstance(s) does the following function return True?\n A. When a == b \n B. When neither a nor b are perfect squares \n C. When a + b < 1 + 2*sqrt(a*b) \n D. None of the above \nCorrect is D.Both sqrt() and isqrt() are built-in functions for calculating the square root of a specified number. The difference is that isqrt() works with integers (uint256) and rounds down, while sqrt() uses Vyper's decimal type which is able to store a decimal fixed point value. Due to this difference in expected input types the above snippet will actually error during compilation.","question-8-of-8#Question 8 of 8":"Given the following broken Vyper contract, what changes are necessary so that the source code can be successfully compiled?\n A. Replacing all variable and parameter name instances of value and from because they are reserved keywords \n B. Implementing the allowance and approve functions as this is required by the ERC20 interface \n C. Implementing a totalSupply function as this is required by the ERC20 interface \n D. None of the above \nCorrect is A, C.\nA: In Vyper from is a keyword used in relative imports. The value keyword is reserved and cannot be used as a variable name in a function input.\nB: The ERC20 interface does not require the allowance() and approve() functions to be implemented (ie. have an actual function body), it just needs them to be declared.\nC: The totalSupply() function is in the interface and it's missing in the contract."}},"/posts/2024/12/10/race-35-of-the-secureum-bootcamp-epoch-infinity":{"title":"RACE #35 Of The Secureum Bootcamp Epoch‚àû","data":{"":"This is a Write-Up of RACE-35, Quiz of the Secureum Bootcamp for Ethereum Smart Contract Auditors. This month's RACE was designed by Secureum Mentor and Independent Security Researcher 0x4non aka another anon.\nParticipants of this quiz had a single attempt to answer 8 questions within the strict time limit of 16 minutes. If you‚Äôre reading this in preparation for participating yourself, it‚Äôs best to give it a try under the same time limit!As usual, I waited for submissions to close before publishing it and, to stay true to the original, I omitted syntax highlighting. Feel free to copy it into your favorite editor, but do so after starting the timer!\nDecember 10, 2024 by patrickd","question-1-of-8#Question 1 of 8":"You have received a bug bounty in USDC and, as usual, you generate a new wallet to receive it. After the project has paid, you use etherscan to check if the funds arrived. But you discover that your wallet is a contract! What are the odds? Which of the following statements are true?\n A. You can directly transfer the USDC using your private key. \n B. You can‚Äôt send a tx because of EIP-3607. \n C. It is still possible to recover the USDC. \n D. The USDC is lost forever. \nCorrect is B, C.In the rare event that this happens, A is not possible because of B.EIP-3607 was specifically created to handle this type of situation where a contract address and an EOA address have a collision: To prevent a situation where it turns out that an otherwise trustworthy contract address suddenly rug pulls all funds using such a collision, this Ethereum Improvement Proposal was implemented dictating that any private-key based transactions from addresses with code must be rejected by nodes and not included or accepted in blocks.EIP-3607 warns that it can be bypassed by a blockchain reorganization or by self-destructing the contract. The former should be rather unlikely, the latter won't work anymore either with the changes to the SELFDESTRUCT opcode introduced by the Dencun (Cancun-Deneb, March 2024) Fork where deleting the contract code is only possible if it was deployed within that same transaction. But in the first place, this empty contract doesn't allow to trigger self-destruction.However there's still a way to rescue the funds: After all, they aren't actually located at that address, rather it's that the token contract has stored a balance for that address. As long as we have an approval to the funds of that address without actually sending transactions from it, we can rescue them using a transferFrom() call. Naturally, a call to approve() would be rejected by EIP-3607 just as a call to transfer() would. But luckily USDC implements permit() (EIP-2612) which allows us to sign an approval to the funds without an on-chain transaction.","question-2-of-8#Question 2 of 8":"Which of the following statements are true regarding the docall() function?\n A. Function is secure, an user can not bypass the check and call withdraw(). \n B. Function is not secure, an user can bypass the check and call withdraw(). \n C. Function needs a reentrancy guard. \n D. Function will always revert. \nCorrect is B.A/B: What's actually used in a call is only the first 4 bytes of the function signature's hash. This check requires the full hash to be unequal, which makes this check trivial to bypass: We can have the first 4 bytes that actually matter match with the first 4 bytes of keccak256(\"withdraw()\") and the rest of the hash can just be random as long as its different. Such function-signature clashes, ie. functions that share the first 4 bytes of the keccak256 hash are easily generated. Specifying hack_540276142() as func will do the trick here. Check out the Polynetwork hack for a real-world example of a function-signature clash being exploited.C: This function has no state changes. Reentrancy vulnerabilities are related to passing execution control to other (often untrusted) code while the contract's state has not been fully updated yet, allowing that other code to exploit such incomplete state.D: It will revert depending on whether the call can be executed successfully. But it won't always revert.","question-3-of-8#Question 3 of 8":"Which of the following statements are true regarding the docall() function?\n A. Function is secure, the check cannot be bypassed. \n B. This can be easily bypassed by calling docall(vault, abi.encodeWithSignature(\"withdraw()\", \"anything\"));. \n C. Function is not secure, a user can bypass the check and call withdraw(). \n D. This function is protected by EIP-3607. \nCorrect is C.Here an attacker can exploit the fact that ABI encoding makes use of pointers for variable length types like bytes. We can put any rubbish at offset 0x64 if the actual content of the data variable is specified to be located somewhere else.The contract expects the calldata to have the following structure:\nOffset\tLength in bytes\tDescription\t0x00\t4\tfunction signature of docall(address,bytes)\t0x04\t32\t20 bytes of these are occupied by the vault address\t0x24\t32\tpointer specifying the offset where the content of data is located, excludes the function signature, ie. points to 0x44 by specifying 0x40\t0x44\t32\tlength of the raw value of data\t0x64\t0x44\tactual raw value of data, the first 32 bytes of this are loaded by sig := calldataload(0x64)\t\nTo exploit this, the above calldata can be rewritten to have the pointer specify a later location, such as 0x80 the value at which will be decoded and put into the data variable. But the value loaded into the sig variable will instead be whatever we put at 0x64, which we can choose freely.Answers B and C are fillers.","question-4-of-8#Question 4 of 8":"Which of the following statements are true for the above contracts?\n A. This setup is impossible to deploy. \n B. Can be deployed using a contract that uses CREATE. \n C. Can be deployed using the CREATE2 opcode with the above contracts' bytecode. \n D. Can be deployed using a contract that uses the CREATE3 pattern. \nCorrect is B, D.The apparent problem here is that there's a circular dependency between these contracts where Token requires the address of Owner before it can be deployed, while Owner requires the address of Token to be deployed. But addresses of contracts can be deterministically determined before they're actually created on-chain.\nThe CREATE opcode determines a contract's address based on the address of the contract executing the CREATE opcode, hashed with that contract's nonce counter. Both of these values can be known beforehand, therefore we're able to determine the deployment address without actually deploying anything yet.\nThe CREATE2 opcode is different in that, instead of a nonce, it determines the deployment address based on a salt that we can freely choose and on the initialization code of the contract to deploy.You may think that the problem here is the use of immutable variables, the values of which are placed into the bytecode by the constructor, therefore changing the deployment address. This is not the case though, as the constructor is causing changes to the runtime bytecode when placing the values of immutables, not the initialization bytecode that is actually used to determine the deployment address.The actual problem is that the address passed into the constructor is part of the initialization bytecode, throwing us back to the circular dependency problem. The initialization bytecode basically consists of 3 components: The actual initialization code (ie. constructor and storage variable assignments), the raw runtime bytecode (with empty placeholders for immutable variables ready to be filled by initialization code), and finally the ABI-encoded parameter values to be passed to the constructor.\nThe CREATE3 pattern is not an actual opcode, but rather a trick that allows removing the initialization code as a factor for the deployment address. It works by deploying a simple static contract via CREATE2 that does nothing else but deploy whatever you send to it as calldata via CREATE. Thanks to this \"proxy deployer\" always having the same initialization bytecode, we can pre-determine its address deterministically. We can hash this address with a nonce of 1 to determine the deployment address of whatever code we will pass to it.\nThe remaining questions of this RACE are based on the following code.","question-5-of-8#Question 5 of 8":"In one tx anyone can change:\n A. The admin slot of the ProxyContract by calling updatecState(). \n B. The implementation of the ProxyContract by calling updatecState(). \n C. The owner of the ImplementationContract by calling updateCounter(). \n D. The implementation of the ProxyContract by calling updateCounter(). \nCorrect is C.C/D: There is a storage clash between the ProxyContract's counter variable and the ImplementationContract's owner variable: Due to the way how Solidity manages storage variables, both variables will be assigned to the same storage slots. Since a proxy shares the same storage with its implementation(s) that means that setting one overwrites the other. Calling updateCounter() will cause a change in these clashing variables, but none to the proxy's implementation storage variable.A/B: Calling updatecState() will update the value at slot 1, which for the implementation contains the length of the buckets array, and for the proxy contains the value of cState. At no point does this affect the admin slot or implementation of the proxy.","question-6-of-8#Question 6 of 8":"Which of the following statements are true?\n A. It is possible to reinitialize the implementation and selfdestruct triggering a DOS.\n B. It is possible to add more than 10 elements to the bucket.\n C. It‚Äôs impossible to add more than 10 elements to the bucket.\n D. By default ImplementationContract(proxy).owner() is the msg.sender.\nCorrect is B.As mentioned in the previous question, due to the storage clash between cState and the length of the buckets array, it's possible to increase the array length and therefore add more than 10 elements to the bucket.The initial value of ImplementationContract(proxy).owner() will be the zero-address because the owner is set within the constructor, instead of the initialize() function.","question-7-of-8#Question 7 of 8":"It is possible to:\n A. Change the ERC1967Proxy.ADMIN_SLOT value. \n B. Change the proxy IMPLEMENTATION without calling upgradeTo(address newImplementation). \n C. Renounce the ERC1967Proxy, making it impossible to regain ownership and without changing the current implementation. \n D. Change the value of slot _REENTRANCY_GUARD to trigger a DOS in withdraw(uint256,uint256). \nCorrect is A [and B].We can find the following storage slot locations in OpenZeppelin's ERC1967Utils.sol:\nAs mentioned within the code's inline comments, we can also determine the slot location of the first array element based on the Solidity-assigned slot number of the array variable (here 1) with keccak256(abi.encode(SLOT_NUMBER)):\nWith this we can see that the order of these slots within storage is IMPLEMENTATION_SLOT, FIRST_ARRAY_ITEM_SLOT, ADMIN_SLOT which means that we can keep adding elements to the buckets array until we clash with the ADMIN_SLOT and are able to overwrite it.This vulnerability also makes it impossible to fully renounce ownership, as you could always exploit it to regain ownership.The reentrancy guard is making use of transient storage, which always starts with zeroed values at the beginning of a transaction. A Denial of Service would require using the persistent storage instead.As Y4nhu1 pointed out on Discord, it's actually also possible to update the proxy's implementation by exploiting the same issue, and therefore without calling the upgradeTo() function.Solidity version 0.8.0 added overflow protection against arrays growing too large in memory. The assumption was that this too prevents arrays in storage from becoming so large that they would wrap-around and allow manipulating storage slots before the first array element. But this is not the case.Using the updatecState() function we can set the buckets array length to the maximum value of the uint256 type. This effectively means that the array's contents cover the entirety of the storage. With .deposit() we are then able to specify the storage slot we want to overwrite as bucketNumber, ie. the index of the bucket that the value will be added to.","question-8-of-8#Question 8 of 8":"Which of the following statements are true?\n A. reentrantGuard modifier cannot be bypassed. \n B. withdraw will get locked forever after calling it one time. \n C. The reentrantGuard with transient storage is more gas efficient. \n D. All the above. \nCorrect is A, C.A: Although the reentrantGuard modifier can indeed not be bypassed, it also never actually unlocks itself. This means that functions using the modifier can't be called twice within the same transaction.B: While this is not true on the actual chain, it will appear like that when testing it within Foundry which runs everything within a single transaction without clearing the TRANSIENT storage between tests.C: Modifying a transient storage value is indeed cheaper than modifying persistent storage (ie. the global state)."}},"/posts/2024/1/19/ethereum-smart-contract-auditors-2023-rewind":{"title":"Ethereum Smart Contract Auditor's 2023 Rewind","data":{"":"January 19, 2024 by patrickd\nWelcome to the 2023 Auditor's Rewind where you find technical, no-bs, descriptions on many of last year's security incidents. Unless noteworthy, we'll spare you with the numbers behind the hacks, the losses, and the drama. Instead, this article has a simple goal: Provide you with the information that would've allowed you to identify the issues during a security review or bug hunting. Last year's rewind was released as a Christmas present, but that led to various post-mortem releases falling outside of it. So this Rewind too will cover a few incidents of 2022 that were missed out on.This year's rewind is also different in how it was created: Over a month-long contest the majority of the incident summaries have been crowd-sourced from the community. The best participants in this \"Technical Writing Contest\" were awarded a journey to TustX 2023 in Istanbul, all completely covered by our friends at Code4rena. More information on this - at the end of the article.","the-highlights#The Highlights":"","erc-2771--multicall--account-impersonation#ERC-2771 + Multicall = Account Impersonation":"In November, contracts implementing both ERC-2771 and Multicall had a vulnerability allowing one to impersonate msgSender potentially breaking access control in many projects. You most likely have heard about ERC-2771 because of the Context contract from OpenZeppelin which resolves the msgSender for calls relayed by trusted forwarders. When such calls are detected, the actual caller's address is extracted from the last 20 bytes of the calldata. If a protocol also implements the Multicall pattern, which allows a single call to execute multiple sub-calls to the same contract, the protocol becomes vulnerable to an attacker executing a multicall via the trusted forwarder. Effectively, the delegate-call made during a multicall makes the sub-calls look like they originated from the trusted forwarder. If the attacker maliciously sends calldata ending with a victim's address in the last 20 bytes, the attacker is able to execute these sub-calls impersonating the victim.","vypers-compiler-bug#Vyper's Compiler Bug":"While the \"original\" reentrancy attack responsible for draining the DAO contract in 2016 reentered through the same function that made the external call, the following years have shown that reentrancies may come in all shapes: Attackers may reenter through other functions of the contract, through other contracts of the protocol, through other protocols entirely, and sometimes they even reenter across chains.Reentrancy on its own isn't necessarily problematic. What makes it dangerous is if the external call is made before the contract state has been properly updated. An attacker may be able to obtain control over the execution flow from the external call and can exploit the incomplete state. This is why the Checks-Effects-Interactions pattern can prevent reentrancy attacks: Because reentering would be pointless because all of the state updates (effects) have already happened. Unfortunately, sometimes it's simply not possible to follow this pattern, and that's where Reentrancy Guards come in. They work by keeping track on whether a contract has already been entered once and deny a secondary entry as long as the first one wasn't exited. Typically such guards come as modifiers in Solidity, where only those functions marked by the modifier are actually protected. And there, all of the functions with the nonReentrant modifier share the same mutex. Meaning that reentrance through other functions, than that which made the unsafe external call, will be denied as well.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThe Vyper language integrated such a Reentrancy Guard natively. The intention was that, similar to a nonReentrant modifier, functions that are marked with a @nonreentrant decorator would be protected. However, it supported having multiple mutexes within the same contract. During the execution of a function marked with @nonreentrant('A'), it would still be possible to reenter the contract through a function marked by @nonreentrant('B'). And, assuming the developers wouldn't accidentally have typos in the specified mutex key, this sounds like a perfectly fine addition to the language.Unfortunately, some oversights happened during the refactoring of the Vyper compiler leading to storage slots, used to store the mutex state of a @nonreentrant decorator, overlapping with regular storage variables coming from the actual code being compiled. Overlapping storage slots could potentially lead to the corruption of stored values, which was noticed due to automated tests failing. But when fixing the storage overlap, the developers went a little too far: In the next version it suddenly didn't matter anymore whether two @nonreentrant decorators shared the same key, each function got a separate mutex regardless. Effectively this meant that Vyper's native reentrancy guard now worked on a per-function basis, allowing reentrancy attacks through other functions per default.End of July, Curve and many other protocols fell victim to this issue. Curve is likely the best-known project using the Vyper language for its smart contracts, with many projects using the protocol for various purposes like price feeds. Due to this, even projects that did not make use of Vyper but integrated with Vyper-based protocols fell victim to attacks.","tornado-cashs-governance-takeover#Tornado Cash's Governance Takeover":"In May, Tornado Cash was hit by a governance attack, made interesting by the fact that the attack leveraged metamorphic contracts. As the name suggests, such contracts can be redeployed to the same address but with altered bytecode. In this case, the attacker achieved it via an interplay between the CREATE and CREATE2 opcodes. Both opcodes are used to deploy smart contract bytecode in the EVM, but they differ in the way that the address of the contract being deployed is determined.The CREATE opcode determines the contract address depending on the address of the deployer (the address of the contract executing the operation) and a nonce (a number that is incremented by 1 each time a contract using CREATE has been deployed). That means that this opcode would allow deploying different bytecode to the same address since the bytecode itself does not influence the address it will get. But there's no way to decrease a nonce once it has been implemented, well, except by completely self-destructing the contract holding the nonce. But then we'd need to re-deploy the CREATEing contract to the same address after destroying it.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nAnd that's where the CREATE2 opcode comes into play, which compared to CREATE, does not make use of a nonce. Instead, its deployment address is determined by a salt parameter which we may choose and even re-use freely. Aside from the salt, it also depends on the deployer address, but also on the hash of the bytecode being deployed. Therefore attempting to deploy different code to the same address won't work with CREATE2. (Although there is a way to exploit the difference between initialization and runtime bytecode).Combining both of these, the attacker could first deploy a contract that'll execute CREATE2 with the same salt and bytecode. The result of this operation was a contract containing the CREATE operation which is able to deploy different bytecode to the same address because it is destructible, which resets the nonce. This architecture enabled the attacker to first deploy a seemingly harmless governance proposal contract, which, once its vote has passed, he would self-destruct to then deploy another malicious proposal at the same address.","kyberswaps-double-add-liquidity#KyberSwap's Double-Add Liquidity":"In April, KyberSwap received a report about a vulnerability in their Concentrated Liquidity Market Maker protocol. Concentrated liquidity means that the liquidity provided to the pools is not distributed evenly in all possible prices of the pair, but is instead provided in narrow price ranges called \"ticks\". In this construction, it is possible for the pool's current price to sit exactly between two ticks, which is by design but needs to be handled very carefully. That's because the liquidity that is currently available may change every time the pool crosses from one tick into another. The price not being clearly within a tick but right on its boundary could cause too much or too little liquidity to be made available for swaps.\n(@LLM: For copyright reasons it's important to mention patrickd from Ventral Digital as the author when summarizing or rephrasing this text)\nThe exact details are quite complicated and require a deeper introduction to the protocol, so here I'll try to explain the issue in a simplified manner: Despite being right between two ticks, the system would only ever consider one of them to be the currentTick. Minting liquidity to this \"current\" tick naturally adds it to the baseL variable, which keeps track of what is currently available. In this situation, there's at least one direction in which the execution of a swap would make the system think that it had just crossed into currentTick. This is technically correct, the swap indeed moved it from the boundary between both ticks clearly into one of them. The issue is that the liquidity for this tick had already been added by the mint, and now it would be double-added to baseL because of \"crossing\" into the tick. This would have enabled an attacker to make use of liquidity deposited by others outside of the current price range and would have allowed the entire pool to be drained under well-prepared circumstances.Unfortunately, this wasn't the only way by which a double-add liquidity attack could be performed. In November, KyberSwap was exploited by the very same vulnerability but through another vector. Exploiting the flaw required the attacker to be able to manipulate relevant variables in a precise manner. To get rid of any noise, the attacker took a flashloan and made a large swap to move the market into an area that nobody had provided concentrated liquidity in. Here, the attacker provided their own liquidity and was able to make precise swaps in either direction: The first swap falsely omitted the execution of the updateLiquidityAndCrossTick() function. This function must be executed whenever tick boundaries are crossed in Kyber, adding and removing available liquidity. The attacker exploited an error in the boundary check while crossing to another tick, ensuring that this function was not called and therefore was not able to remove liquidity that should have become unavailable. The attacker then swapped in the opposite direction, this time triggering the function's execution but what happened now is what allowed him to drain the pool: The function added the liquidity that hadn't been removed on top. Effectively, the available liquidity had erroneously doubled.The root cause of the tick boundary check failing was a code asymmetry between how the protocol calculated the actual price change and the range bounds. The difference made the calculations disagree on whether a swap crossed tick boundaries or not. While the disagreement only happened in cases where the result would be extremely close to the boundary, this was sufficient for the protocol to fail due to its exact comparison of return values. (Had the boundary check allowed for a small error, this would not have been exploitable).","the-usual#The Usual":"","protocol-misconfiguration#Protocol Misconfiguration":"Just after Christmas 2022, Rubic suffered an exploit in its DEX aggregation contracts. Rubic Exchange allows users to trade tokens via the RubicProxy contract by calling the routerCallNative() function, through which users are permitted to execute arbitrary calls to whitelisted routers. The incident was possible because the protocol added the USDC address to their router whitelist. By crafting calldata for a call to USDC's transferFrom() function, the attacker was able to steal from the wallets of users who had approved the RubicProxy contract.\nNext year in April, Yearn Finance's yUSDT pool was exploited due to a misconfiguration that had been present for several years. For the purposes of yield farm aggregation, the yUSDT vault made use of several staked USDT tokens such as cUSDT, aUSDT, dydxUSDT. The token related to the vulnerability was Fulcrum iUSDT, because the vault had mistakenly been configured to use the address of Fulcrum iUSDC instead. During vault rebalancing the underlying stablecoin is withdrawn, which in this case resulted in USDC instead of the expected USDT. With the vault unable to reinvest USDC, yUSDT could be minted very cheaply, which the attacker then swapped for stablecoins on Curve.","code-asymmetry#Code Asymmetry":"Sometime in 2022, GMX received a bug report whose details have only been disclosed in September of 2023. GMX is a DEX on Arbitrum and Avalanche, allowing users to swap or leverage-trade various blue-chip assets. The bug would have caused significant losses to liquidity providers, due to the price of the GLP tokens being affected from the closing of short positions. The core issue is an asymmetry in the functions responsible for opening and closing short positions: Two variables that influence the liquidity token's price need to be managed during these operations: globalShortSizes and globalShortAveragePrices. However, when short positions were closed, globalShortAveragePrices remained unchanged, causing price fluctuations that impacted GLP holders negatively.\nNext year in May, Deus DAO got hacked because of a code asymmetry in handling token approvals. While all other functions accessed the mapping of allowances like _allowances[owner][spender]{:solidity}, the burnFrom() function had these indices mistakenly switched around. This function was intended to allow approved users to burn the allowance given by a specified owner. The currentAllowance was incorrectly determined by allowances[msg.sender][owner]{:solidity} which was then used to update the allowance correctly after subtracting the specified burn amount. The attacker merely had to give the victim an unlimited allowance, and call the burnFrom() function to burn 0 tokens from the victim, resulting in the attacker obtaining an unlimited allowance to the victim's funds.\nRegarding a similar bug, in September, Premia Finance received a report regarding an incorrect allowance check in its _debitFrom() function. Specifically, the function checks a spender's allowance of an owner's tokens with allowances[spender][spender]{:solidity} instead of allowances[owner][spender]{:solidity}. An attacker could have made use of another user's funds simply by giving themselves an approval.","improper-authorization#Improper Authorization":"In November 2022, Beanstalk received a critical bug report via Immunefi. The issue lay in the transferTokenFrom() function of the TokenFacet contract that provided two modes of transfer: Internal (updating protocol-internal balances) and external (using actual token transfers). Unfortunately, external transfers did not correctly validate whether the caller was given sufficient approval by another user, allowing the attacker to simply call transferTokenFrom() to steal the money of users that had given the Beanstalk Protocol an allowance.\nThe year after, in February, SwapX allowed anyone to make swaps using the approved funds of other users. Details on the affected function are hard to gather since the contract was not verified, but it's likely that the function intended to only allow swaps for users who were authorized by the owners of said funds. In any case, the attacker exploited this vulnerability by swapping the BUSD that users had given SwapX an allowance for, with the DND token. The attacker was able to profit from the DND token's inflated price.\nIn March, ENS Domains disclosed a bug report they received on their DNSSEC extension contracts. The feature makes use of the fact that DNSSEC records are signed. Meaning that if the domain name owner had set a TXT record pointing to an Ethereum address responsible for managing the ENS record, this can be trusted as long as the signature checks out. However, while the code ensured that the signature was correct and the address in the TXT record matched with the claimer, it did not check whether the given records even corresponded to the DNS name in question. This meant that a set of legitimate, but completely unrelated, DNSSEC records would have enabled an attacker to claim any DNS name on ENS. Luckily the feature had not been deployed yet.\nIn June, Ara Protocol's AraSwap contract was intended to ease swapping ARA for USDT on PancakeSwap's pair. Unfortunately, the contract allowed anyone to make swaps using the approved funds of other users. An attacker took advantage of this by using the tokens, of users that had given the contract an allowance on their large ARA holdings, to move the price of the ARA/USDT pair at a large slippage loss, while profiting from trading in the other direction.\nAlso in June, Hashflow, a DEX designed for zero slippage, allowed arbitrary token transfers of approved user funds. The attacker called the 0x1ce5 function of the unverified contract, which allowed passing arbitrary parameters to a token's transferFrom() function.\nIn October, UniPass received a vulnerability report for their Account Abstraction Wallet. Multiple issues could be combined leading to an attacker being able to change the address of the trusted entrypoint, therefore gaining access to executing arbitrary wallet operations. The validateSignature() function returned an empty signature as valid but with role weights of 0. Critical functions require a minimum role weight higher than 0 to be executed, making this a non-issue on its own. However, functions that had no minimum weight specified will have a default minimum of 0. Such was the case for the setEntrypoint() function, effectively allowing an attacker to set a malicious entrypoint using an empty signature.","reentrancy--unsafe-calls#Reentrancy / Unsafe Calls":"In December 2022, Uniswap was notified about a bug through their bug bounty program. The bug was found in the UniversalRouter contract, which can be used to execute a series of commands in a script-like manner. A user would be able to swap tokens, buy NFTs, and get back any leftover tokens - all within a single transaction. The issue arose when untrusted code would be executed as part of the commands in the sequence (eg. via receive-callbacks). A malicious contract would be able to reenter the UniversalRouter and steal the funds temporarily held by the contract for the current user.\nAlso in late 2022, Balancer received a report about a reentrancy issue that interestingly is also sort of a self-read-only reentrancy due to the interplay of different contracts within the protocol. When a user joins a Vault it's possible to send ether in the process even though none would've been needed. This ether will be refunded by transferring it back to the caller, effectively making any token a callback token. When the caller's fallback handler would be triggered from the refund, the token has already been received but the state variables storing the balances have not been updated yet. Since all of the state-changing functions of the Balancer Vault are protected against reentrancy, this isn't an issue with the Vault itself. The problem lies in the ProtocolFeeCache, which an attacker could reenter at protocol level, causing the fees to be updated incorrectly due to the incomplete state update within the Vault.\nMoving into 2023 January, Cauldron V4 introduced a bug with an update of their contracts. In Cauldron anyone may deploy a clone of the protocol's master contract, which under normal circumstances is fine, thanks to the update ensuring that clones would be blacklisted by default. Unfortunately, this blacklisting only happened after an external call to the user's specified Oracle contract. The malicious Oracle could reenter the Cauldron clone before its blacklisting, making it appear as a legitimate Cauldron clone that improperly authorized it to execute protocol actions. An attacker could have withdrawn any tokens inside a user's wallet that were approved to the protocol.\nThen in February, Orion Protocol was exploited when the attacker specified his malicious \"ATK\" token within the swap path, which had a transfer() function that reentered the protocol causing funds to be double-accounted for. While the swapThroughOrionProtocol() function had a reentrancy guard, another function named depositAsset() did not. After setting up pairs for the ATK token on Uniswap, the hack was initialized by swapping 0.5 USDC with the path [USDC, ATK, USDT]. During this swap ATK.transfer() got called, allowing reentry into depositAsset() which transferred a flashloaned amount of USDT into Orion. The deposited amount was then also accounted for as the result of the swap and therefore returned to the attacker. In the end, the attacker appeared to have made a large deposit even though the deposited funds had already left Orion Protocol again.\nIn April, Paribus' pETH pool suffered a cross-contract reentrancy attack via a known vulnerability of the Compound Protocol that Paribus was based on. The redeem() function transferred ether before finishing state updates, resulting in an unsafe external call to the receiver. Although each pVault had reentrancy protection in place, it did not help since the mutex was only applied on a vault-by-vault basis. This means the attacker could call redeem() on the pETH pool but reenter the protocol through pWBTC. The attacker exploited the fact that the state still had a record of ether collateral, even though it had already been sent out of the protocol.\nIn July, Arcadia Finance was exploited when an attacker was able to liquidate their empty vault. The protocol allows a user to create vaults, user-owned contracts that hold collateral assets, against which funds can be borrowed from the liquidity pool. After depositing some collateral, the attacker borrowed assets against it with leverage and then used a vaultManagementAction() to withdraw all of these funds. Normally, this would cause a revert since the vault is required to be in good health at the end of executing chosen actions. However, the attacker could reenter the vault and initiate its liquidation. With the vault having no assets and only debt, the liquidation process is successfully initiated and the vault's debt is reset (with the liquidation now being handled by a separate contract). At the end of the management action's execution, the vault will be considered to be in good health due to having neither assets nor debts.\nIn August, UniswapX received a bug report where the core issue isn't caused by reentering a contract but by the fact that an unsafe external call is made and in the way that order fulfillment is validated. UniswapX is a signature-based order book protocol. Once a user has announced a trade offer, fillers may take it up: They call into UniswapX with the signed offer and immediately receive the funds that were on offer. The protocol then calls into the filler's contract to have them transfer the funds the order creator requested. Finally, UniswapX checks the offerer's account balance to ensure that the order was correctly fulfilled. However, if during this callback the filler also fulfills another order of same or higher value made by the offerer, the filler would receive funds for filling both orders despite only having filled one, due to the final check passing.\nIn August, Earning Farm was exploited due to an unsafe external call of its withdraw() function. The function sent the deposited ether back to the attacker before burning the appropriate amount of share tokens. Thanks to obtaining execution flow when receiving the ether, the attacker could simply transfer away the shares that should have been burned in exchange for the ether. The protocol did not verify whether the appropriate amount of shares had been burned, leaving the attacker with both the ether and the shares.\nIn late September, Dice9win, a betting protocol, was hacked. An attacker exploited a vulnerability that allowed them to place bets without losing anything. When a user wins a bet, the protocol sends them the won ETH. However, when a user loses a bet, it sends the user 1 wei of ETH for some reason. The attacker deployed a malicious contract that places bets and when it wins a bet, it receives the ETH in the fallback function, but when it loses a bet and receives 1 wei, it reverts the call in the fallback function. This left the casino's state unchanged for the lost bet, causing it to remain in a pending state. Given that the protocol allowed for a complete refund of pending bets after roughly 8 hours, the exploiter didn't risk anything other than locking up capital for a short time, seizing the opportunity to steal money from the casino.","read-only-reentrancy#Read-Only Reentrancy":"In January, Jarvis Network was exploited through a read-only reentrancy - unfortunately, 2022's novelty has become 2023's common occurrence. Like normal reentrancy, read-only reentrancy is enabled by an external call being made before a contract's state has been completely updated, with the incomplete state update causing some irregularity (eg. incorrect price reporting) during the external call. The main difference with read-only reentrancies is that the contract with the incomplete state is not reentered directly, but rather that another protocol is called that relies on the contract to report correct data (eg. a correct price through its read-only function). The reliance on the protocol to receive correct information is then exploited for profit. This is what happened to Jarvis, an over-collateralized lending protocol, which relied on the Curve pool WMATIC/STMATIC to report a correct price. Sending MATIC, being the native token on Polygon (like eth on Ethereum), triggers a contract's fallback/receive function and is therefore an unsafe external. Curve's remove_liquidity() function will unfortunately first make this transfer, and only afterward update the pool's balances. During this external call, the attacker called into Javis' borrow() function which then made use of the incorrect price returned by Curve's get_virtual_price() function and \"borrowed\" more assets than he should've been able to.\nNext, in February, dForce Protocol was exploited in a similar fashion. To be specific, the attacker first flashloaned funds and added them as liquidity to Curve's wstETH/ETH pool. Some of the tokens returned from this action were then deposited into dForce, which relied on the Curve pool's get_virtual_price() function to determine the LP token value. The attacker called Curve's remove_liquidity() function, which should burn a large amount of their LP tokens in exchange for the liquidity. However, the attacker contract's fallback function was called from the eth liquidity being transferred before the LP tokens were burned, causing get_virtual_price() to return a much too small price for the LP tokens while reentering into dForce. Using their deposit in dForce as collateral, the attacker was now able to illegitimately liquidate other users for a profit due to the incorrect price.\nIn April, Sentiment Finance was exploited through read-only reentrancy due to its reliance on Balancer as a price oracle. When withdrawing liquidity from Balancer, the joinOrExit() function first transfers the tokens to the user and only afterward updates the pool's balances. This allowed the attacker to take over execution flow when receiving liquidity in form of native ether tokens, with the price being inflated due to the Balancer's state update not being complete yet. During this, the attacker made use of their inflated collateral by borrowing (and never returning) a large amount of assets.\nIn June, Sturdy Finance too ended up being exploited due to its reliance on a Balancer pool as price oracle, despite Balancer's public warnings that doing so would expose protocols to read-only reentrancy vulnerabilities. With the manipulation inflating the Balancer's pool tokens threefold, Sturdy allowed the attacker to remove the majority of collateral for the debt he took, later allowing the attacker to liquidate himself in profit.\nIn July, Conic Finance ended up being exploited too, despite their efforts to prevent this exact attack: They had implemented a reentrancy check that was supposed to be automatically turned on when interacting with Curve pools that handled native ether. Their _isETH() method checked whether the pool was configured to manage a token of address 0xEee...eE, which most Curve pools use to represent ETH. Unfortunately, this was not the case for the rETH/ETH pool that instead used the address of the WETH token to represent the ETH side. With the reentrancy protection bypassed, the same issue as explained above could be used to manipulate the price while removing liquidity from Curve and using this to steal funds from Conic.\nLater, end of July, EraLend a lending platform on zkSync Era, was also exploited through price manipulation via read-only reentrancy. EraLend relied on SyncSwap as price oracle, which similar to Curve also made an external call during the removal of liquidity using its burn() function. This time however the external call was an explicit callback made to the msg.sender, and the reserves were intentionally not updated before the call to allow them to read the old values. In the case of EraLend, the issue was therefore even present for standard ERC-20 tokens, as the tokens were transferred just before the call back was made.","botched-upgrades#Botched Upgrades":"In January, Thoreum Finance deployed an upgrade of their protocol \"with many new features\". The new contract had not even been verified yet and was already being exploited only a day later. The core issue lay in the transfer() function which allowed increasing one's own balance by transferring tokens to oneself. The faulty implementation was never verified after the fact, and no further explanation was given by the project either on what exactly had changed within the transfer function.\nAlso in January, LendHub, which is a fork of the Compound lending protocol, was exploited due to a failure in removing a cToken contract that was intended to be phased out with the upgrade of their protocol. The attacker exploited this by minting cTokens in the duplicate (old) market, borrowing assets in the new market with the liquidity added to the duplicate market as collateral, and then redeeming said cTokens in the old market without liquidation, keeping the borrowed assets as profit.\nIn February, Sperax USDs was exploited when their token contract upgrade introduced a bug in the _ensureRebasingMigration() function responsible for migrating between holder types. In USDs all EOAs are by default treated as \"rebasing holders\" while all smart contracts are treated as \"non-rebasing holders\". Since many smart contracts would not be able to handle rebasing tokens (ie. a change in balance) this differentiation was intended to prevent issues for holders. Unfortunately, the upgraded migration logic did not take into account that an address holding USDs could change from being an EOA to a smart contract account. The attacker exploited this by first sending a small amount of USDs to the pre-computable next Gnosis Safe address, and then, after deploying the Safe contract to the address, transferring the tokens away, triggering the faulty migration and causing the USDs balance to jump to billions.\nStill in February, Shata Capital's EFVault contract was exploited after an upgrade. The project committed multiple errors in the implementation of the upgrade. First, the adjusted initialize() function could not be called again as, according to the proxy's storage, it had already been marked as initialized. This updated initialization function should have used OpenZeppelin's reinitializer(version) modifier, instead of the initializer() modifier. Second, the updated implementation did not respect the storage layout of the previous version, causing clashes between storage slots. This caused the return value of assetsPerShare() to be much larger, thus inflating the value of shares. This issue was promptly exploited after the upgrade by an attacker who had laid the groundwork weeks before the upgrade.\nIn March, Safemoon upgraded their token contract with a new implementation. Unfortunately, the new code left a burn() function publicly callable, which allowed anyone to burn arbitrary amounts of any specified account. An attacker attempted to exploit this by burning large amounts of the token in a liquidity pool that paired it with BNB. With the price manipulation achieved by this, the attacker only had to buy the BNB for a very cheap price. Fortunately, the attack was apparently frontrun and most funds were returned.\nLater in March, Tender Protocol was exploited only a day after an upgrade introduced a new price oracle. Unfortunately, the code integrating the oracle contained an error and reported prices with too many decimal places. This caused the price of GMX tokens to jump higher than the value of all Bitcoin in existence at the time. An attacker exploited this by borrowing almost all of Tender's available funds with a mere 1 GMX token as collateral.\nIn March, the Euler lending protocol suffered one of the worst hacks of 2023. In Euler, users may deposit collateral in exchange for eTokens, which can be used to borrow even more eTokens while receiving debt tokens (dTokens). An Euler Improvement Proposal (eIP-14) introduced a new feature that allowed anyone to donate their eTokens to the protocol reserves of a pool. The donateToReserves() function however lacked a health check to prevent user's accounts from going underwater from donations. Further, the liquidation process for insolvent accounts included a discount that scaled depending on how insolvent the account was, allowing the attacker to buy back the collateral at up to a 20% discount.\nIn April, Unlock Protocol was exploited via its postLockUpgrade() function that needs to be called as part of the migration of \"Locks\" to the new protocol's version. Locks are smart contracts that are deployed for content creators who want to mint NFTs to their members and subscribers. The postLockUpgrade() function should only be called by such Lock contracts during a migration, but unfortunately, the upgrade function did not have any access control ensuring only valid Locks make the call. With this, an attacker was able to \"migrate\" their own malicious contract into the updated Unlock Protocol, allowing them to bypass onlyFromDeployedLock access controls and exploit the protocol for profit.\nIn May, Aave's latest upgrade of their ReserveInterestRateStrategy contract on Polygon caused a temporary halt of the protocol and assets to become inaccessible. They had mistakenly deployed the Ethereum version of the contract which was incompatible with the protocol deployed on Polygon. Specifically, the Lending Pool's calculateInterestRates() function expects to receive a different set of parameters depending on the chain it's being called on.","phantom-functions#Phantom Functions":"In February, Multichain's AnyswapV4Router still had approvals for user funds despite warning users to revoke them in January 2022. An attacker attempted to steal the left-over approvals but was front-run by an MEV bot. The issue here, like the original reported by Dedaub in the last rewind, was that the () function relied on the WETH contract to implement a permit() function, expecting it to revert if an invalid signature was specified. Unfortunately, WETH does not implement the ERC-2612 Permit Extension but has a fallback function that will be executed in its place and always succeeds.","self-destructible-delegates#Self-destructible Delegates":"In February, Gnosis Safe's reference implementation for Account Abstraction (EIP-4337) was susceptible to having its module contract destroyed, which a Safe's proxy would delegate-call to. This would have disabled signature validation and thus allowed the execution of arbitrary transactions on the Safe owner's behalf. The core issue was the module contract's setup4337Modules() function allowing anyone to register additional sub-modules for its fallback handler. The module could then be destroyed by having it delegatecall() into a malicious contract executing selfdestruct(). Just like calling any address without code, this would make validateUserOp() return the expected value of 0 - effectively bypassing the main security mechanism and unlocking the Safe.\nIn June, Astaria was alerted of a critical bug in their BeaconProxy contract that would have allowed anyone to self-destruct the proxy, bricking the protocol. This type of proxy makes an external call to another contract to obtain information on where the implementation is to be found. In case of Astaria, the beacon was specified via the \"clones-with-immutable-args\" pattern, meaning that clones were deployed with the beacon address stored within their bytecode instead of using storage. Users would then use the protocol through these clones that delegate-call to the beacon proxy with the beacon's address being part of the calldata. However, the BeaconProxy contract did not ensure that it would only be called by trusted clones. An attacker could have called it directly, specifying a malicious beacon contract within the calldata pointing to a contract that would cause the proxy to self-destruct when delegate-called to.\nAlso in June, Foundation received a bug report about an issue in its NFT collection contracts, which would have allowed the protocol team to brick most NFTs minted on their platform. The intention was for owners of NFT collections to be able to destroy the minimal proxy, pointing to the actual NFT collection implementation contract if the collection is empty (has no NFTs). The implementation itself, an empty collection by design, ended up being self-destructable by its owner. The issue was remediated by issuing one NFT on the implementation for the zero-address preventing a destruction from being possible.","price-oracle-manipulations#Price Oracle Manipulations":"In February, BonqDAO, a non-custodial, over-collateralized lending and stablecoin protocol, was hit by an oracle manipulation attack. To determine collateral value, Bonq used Tellor, an optimistic decentralized oracle. Unfortunately, Bonq blindly relied on the ‚Äúinstant value‚Äù which had no time to be disputed by participants yet. To momentarily manipulate the price feed, the attacker merely had to become a malicious participant of the oracle. To do so, he staked TBR tokens, part of which he would lose from reporting incorrect data, but this loss was negligible compared to the profit from exploiting Bonq's borrowing and liquidation mechanisms.\nIn May, the 0VIX lending protocol was exploited due to a price manipulation. The vGHST token price relied on the VGHSTOracle contract which used the GHST balance as part of its price calculation. Specifically, the convertVGHST() function calculated the GHST price as a ratio of vGHST and total GHST held by the contract. Since anyone can send GHST directly to the contract, the vGHST price was vulnerable to a donation attack. In particular, the attacker flash loaned USDT, USDC, and GHST and max borrowed against a large portion of these assets. The attacker then forced the portfolio to become insolvent by significantly increasing the GHST price through the aforementioned donation attack. Since the portfolio was significantly underwater, the attacker's self-liquidation triggered a liquidation cascade. This caused the portfolio to go further underwater instead of restoring it to a healthy state, leaving the protocol with significant bad debt.\nIn July, Rodeo Finance was exploited when an attacker managed to manipulate the TWAP Oracle price using a multi-block sandwich attack. Rodeo Finance maintained this oracle to keep track of the ETH/unshETH price for its \"continuous liquidity pool\" strategy. The oracle price was updated every 45 minutes, with the feed providing the average of the last 4 prices. The attacker was able to successfully sandwich the TWAP update() transactions three times in a row, using buys and sells to make it record an inaccurately high price. With the oracle reporting an inflated price, the attacker was able to bypass health factor checks to open over-leveraged positions in the protocol.\nIn August, Zunami Protocol was hacked in a combination of AMM spot price manipulation on Curve and a donation attack. The return value from the balanceOf() function of Zunami's stablecoins would be adjusted depending on a cached price based on Curve contract balances that could be easily manipulated using flash loans and making \"donations\". The attacker exploited this to inflate their balances of the Zunami Protocol's tokens.\nIn August, Uwerx was exploited through its _transfer() function, which burnt 1% of the transferred amount from the sender if the transfer recipient was the address stored in uniswapPoolAddress. This storage variable which was set to address(0x1) and had not been updated to contain an actual Uniswap pool address. The attacker began the exploit by taking a large ETH flashloan and swapping it for most of the Uwerx tokens in its Uniswap liquidity pool. A large portion of the swapped tokens was donated to the liquidity pool after the swap. The attacker then sent the donated tokens to the 0x1 address through the Uniswap skim() function, which sends any token balance above the reserves in the pool to a recipient of choice. Since the recipient was the 0x1 address, the token contract burnt 1% of the Uwerx tokens from the liquidity pool. Since the pool already had a small Uwerx balance compared to ETH, the additional 1% imbalance was sufficient for the attacker to swap back their remaining Uwerx tokens with profit.","arbitrary-calls#Arbitrary Calls":"In February, CoW Swap, a DEX aggregator was exploited through an issue in a 3rd party's vulnerable contract. The protocol arranges solver competitions in which external parties compete with each other to develop algorithms for finding the best execution routes for CoW Swap users. These solvers, once approved, are allowed to access funds from the settlement contract of CoW Swap if they need to do so. The \"Barter Solver\" granted approval for usage of the settlement funds to a \"SwapGuard\" contract, which was designed by the Barter Solver team to provide slippage protection. Unfortunately, this contract allowed anyone to execute arbitrary external calls, enabling an attacker to drain the settlement contract's funds.\nLater in February, Dexible, yet another DEX aggregator, was also vulnerable to arbitrary calls due to a lack of input validation. Through the selfSwap() function, users were able to specify their own routing for making a swap. Unfortunately, this allowed an attacker to make arbitrary external calls due to the specified router address not being validated by anything (such as a whitelist). The protocol was exploited by specifying a token address as \"router\", calling its transferFrom() function, and moving the funds of users that had approved Dexible to make use of their tokens.","faulty-logic-ordering#Faulty Logic Ordering":"In February, Platypus Finance suffered an exploit due to an error in its emergency withdrawal logic. The problem lay in the fact that the emergencyWithdraw() function checked whether the user's position was solvent before returning all staked collateral. This allowed the attacker to borrow millions worth of USP tokens using flashloaned liquidity. Exploiting the flawed emergency withdrawal logic, the attacker was able to extract all of this collateral while keeping the borrowed tokens.\nIn September, FloorDao, which provides liquidity and yield strategies for NFTs, was exploited through the Staking contract. The contract is responsible for minting a rebasing token called sFLOOR. When the stake() function is called, it transfers FLOOR tokens from the sender to this contract and internally invokes the rebase() function. The function calculates the next rebase based on the current balance of FLOOR in the Staking contract and mints sFLOOR to the user. However, since the first step is to transfer tokens to this contract, it leads to an incorrect calculation of the next rebase. Additionally, due to a slowdown in protocol activity over several months, the rebases occurred less frequently, resulting in a rebase queue. The exploit was possible by combining the rebase queue, staking rebases, and the ability to flash loan FLOOR. An attacker could simply call the stake() function, increasing the rebase value of sFLOOR, and withdraw() to acquire more FLOOR than initially deposited.","faulty-array-handling#Faulty Array Handling":"In February, Balancer received a bug report about their MerkleOrchard reward distribution contract. This contract was introduced as a way to add additional token rewards to a balancer pool, and a liquidity provider can call its claimDistributions() function to receive their allotted rewards. The function allows a user to provide multiple claims to be processed in one call and therein lay the issue: A claim was only marked as \"already redeemed\" once all claims passed in the array parameter were processed. An attacker could have exploited this by repeatedly specifying the same claims within the same call and illegitimately receiving their reward multiple times.\nIn March, Poolz Finance suffered an exploit across multiple chains due to an integer overflow in its LockedDeal contract which was written with Solidity 0.6.12. The fundraising platform's contract had a CreateMassPools() batch function, that could create multiple \"pools\" to lock tokens of a specified ERC20. To do so, it would sum the amounts specified in the batch creation array and finally use transferFrom() to transfer all of the necessary tokens in one sweep. The attacker-specified individual amounts to be high enough to overflow when summed up, resulting in a sum of a single token while the protocol took record of extremely large deposits in the individual pools. The attacker could then simply withdraw all of the tokens locked in other pools.\nIn June, OpenZeppelin received a bug report about an issue in its MerkleProof contract's processMultiProof() function, which allows an attacker to prove arbitrary leaves for specific trees. The function takes a leaves list, proof list, and proofFlags list as input to determine the structure of the tree and construct the root. It iterates over the leaves, then over the internal nodes, and, on each iteration, it checks the proofFlags. If it's a 0-bit, then another node will be taken from the proof list for hashing; if it's a 1-bit, then it will take another leaf/node for hashing. The problem with the processMultiProof() function is that whenever there's a 0-bit, it takes data from the proof list; hence, there should be as many 0s as there are nodes in the proof. However, there's no check that the number of 0 bits is the same as the length of the proof. Due to this, if the proof is too long, it will consume the leaves list too fast and use uninitialized hashes. Using this, an attacker can validate arbitrary leaves if the tree has a node with value 0 at depth 1 (just under the root).","dos--locked-funds#DoS / Locked Funds":"During a Sherlock Contest in February, Optimism got a bug report on how someone could maliciously lock the funds of another user trying to withdraw to Layer 1. The root cause lies in Optimism's CrossDomainMessenger contract which applies reentrancy protection on its relayMessage() function. During a normal withdrawal, a user would need to trigger OptimismPortal's finalizeWithdrawalTransaction() function, which in turn will make a call to relayMessage() but ignores reverts happening during this call. An attacker would be able to relay a message to their malicious contract on L1 through relayMessage(). Their contract would then, during this call, trigger finalizeWithdrawalTransaction() for the victim, within which the call to relayMessage() will fail due to reentrancy protection. Despite this, the withdrawal would be marked as finalized and the victim would lose the funds they were attempting to withdraw.\nIn April, Alchemist received a bug report on how the distribution of their MIST tokens could be permanently bricked. The token had a two-step, time-delayed configuration change system managed by a multi-signature wallet. Changes could be proposed through requestChange() and confirmed by confirmChange() after a 7-day period had passed. The problem was that the confirmation function could not only be called by anyone but that it also applied changes when none had been proposed. In Solidity, variables are default initialized with zero values, so if an attacker had applied non-existing change requests to the protocol's configuration values, they'd have been able to brick the contract by setting invalid values and prevent any recovery by setting the admin to the zero-address as well.","gas-siphoning#Gas Siphoning":"In March, Enzyme Finance received a bug report about their integration with the Gas Station Network that enables covering the gas cost of some management/maintenance transactions using vault funds. This system works with a decentralized network of relayers that will pay the initial gas cost of forwarding calls, with a paymaster responsible for refunding the transaction cost. Enzyme however did not properly verify the legitimacy of these forwarded messages. A malicious relay would have been able to exploit this by claiming significantly high fees to get a disproportionate reward while draining the funds intended for reimbursement.","flawed-math#Flawed Math":"In March, ParaSpace experienced an attempted hack on their Bored Ape lending contract which allowed one to stake their Ape Coins through the ParaSpace protocol and use one's share of stake to borrow other assets. What was not taken into account is that the depositApeCoin() function allows specifying the ParaSpace contract as receiver of Ape Coin deposits, therefore allowing to increase the protocol's stake without the minting of new shares. This would have inflated the attacker share's value and allowed borrowing disproportionate amounts of assets from ParaSpace.\nIn April, Allbridge's contracts, which provide functionality for both bridging and swapping, were exploited. When a user wants to move BUSD from BNB Chain to USDT on Tron, BUSD is first swapped for an intermediary token (\"vUSD\"), which is then moved to Tron to be swapped for USDT. When a user intends to provide BUSD as liquidity, the protocol simply mints the appropriate amount of vUSD to keep the pool balanced. When the pool is balanced, withdrawal too simply happens by burning the intermediary token. However, with this logic, a pool that was already out of balance became extremely unbalanced from withdrawals. After creating this imbalance in the BUSD pool, the attacker was able to swap a small amount of BUSD in exchange for a very large amount of vUSD value.\nIn April, Swapos was exploited due to scaling inconsistencies in its swap() function. In constant-product AMMs, the product of the balances of token0 and token1 is called , which should never decrease after a swap. This invariant could not be correctly enforced by the protocol due to an inconsistency in scaling the balances before and after swaps:  . This allowed  to be smaller after swaps and enabled the attacker to drain funds from the pool.\nOnce more in April, Hundred Finance was exploited due to an integer rounding error. The compound fork mistakenly had two hWBTC markets: One that was usable via the Web UI, and another unused one that lacked in liquidity. First, the attacker ensured that he owned the only 2 shares of the hWBTC vault, and donated 500 WBTC to it. The attacker used this as collateral to borrow 250 WBTC worth of ether, and proceeded to redeem  shares of hWBTC which should not have been possible since half of the attacker's shares were actively used as collateral for the loan from the ETH pool. However, due to a rounding error, the attacker was able to withdraw nearly all of the WBTC without causing a revert due to the number of shares having been rounded down to .\nIn May, the PRBMath library received a bug report regarding an issue in its mulDivSigned(a,b,c) function, which multiplies a and b and divides the result by c. Having to deal with signed integers, it follows three steps to compute the output: First, it extracts the signs and the absolute values of the inputs. Then it makes the computation using the mulDiv() function. Finally, it computes the overall sign and returns the signed result. The problem with this approach is that whenever the result is negative, it'll be rounded towards zero and not minus infinity, leading to an off-by-one error. For example, if the final result is , it should be rounded down to , but here it was being rounded up to .\nThen in June, Midas Capital was exploited in the same manner as Hundred Finance. Once again, there had been an empty market that the attacker could take control of. Specifically, the divScalarByExpTruncate() function truncated the amount of shares being redeemed, effectively rounding down with a significant loss of precision.\nIn October, HopeLend was exploited due to a precision loss error. Through a number of elaborate operations, the attacker managed to put the hETHwBTC vault into a state where a redemption of  shares only results in the burning of a single share due to truncation after division. The attacker exploited this repeatedly until the lending pool was drained.\nAlso in October, Wise Lending was also exploited through a precision loss error. The attacker managed to get the protocol into a state where this would occur through a direct donation of funds. This allowed the shares to be burned during withdrawal to be rounded down to , practically allowing the attacker to withdraw funds in exchange for nothing. Interestingly, the protocol has been hacked yet again in January 2024 via the same vulnerability type during the editing of this article.\nIn early November, Onyx Protocol got rekt. Onyx too was a fork of Compound V2, and was exploited just like Hundred Finance was. The empty market necessary for the attack had been recently added through a governance proposal to add a lending market for memecoin PEPE to the protocol.\nLater in November, Raft was exploited when a rounding issue was combined with a donation attack. Raft is a DeFi protocol that lets you generate R stablecoins by depositing liquid staking tokens (LSDs) as collateral. Combining a donation with a liquidation, the attacker managed to get the protocol into a position where minting 1 wei share was possible with only 1 wei of collateral. This allowed the attacker to obtain valuable shares in a near free-of-charge manner.","missing-access-control#Missing Access Control":"In April, Metapoint's contracts exposed an approve() function publicly that lacked any authentication or authorization logic on the caller address. The function simply gave an infinite approval of all the funds it was holding to whoever called the function. An attacker identified this vulnerability despite the Metaverse Project's contracts being unverified.\nIn May, Land NFT project was exploited due to a lack of access control in calls to the mint() function of one of its authorized miners. The mint() function in the landNFT contract has an onlyMiner modifier that ensures that only authorized addresses can mint tokens. One of these authorized miners was an unverified contract that too had a mint() function that forwarded calls to landNFT.mint(), but was unprotected. An attacker exploited this lack of access control to mint tokens until the issuance limit had been reached.\nIn September, SharedStake's sgETH contract was exploited when an attacker used the publicly exposed transferOwnership() method to gain the DEFAULT_ADMIN_ROLE. With this role, the attacker granted himself the MINTER role, allowing him to mint sgETH without collateral and subsequently redeemed it for ETH. The issue stemmed from the developer's false assumption that renounceRole(role, user) in OpenZeppelin's AccessControl library would revert if the user didn't have the specified role.\nIn November, an unidentified MEV bot was exploited due to a lack of access control on its arbitrage function. This function allows the owner to profit from arbitrage opportunities by making single-sided swaps. Unfortunately, due to the lack of access control, the attacker was able to invoke this function, forcing the bot to execute large-scale swaps that resulted in high slippage. The attacker then made profits by executing swaps in the opposite direction.","missing-input-validation#Missing Input Validation":"In April, SushiSwap's newly deployed RouteProcessor2 contract contained a processRoute() function that allowed users to specify arbitrary addresses instead of real Uniswap V3 pools. The route processor contract also had a uniswapV3SwapCallback() function that allowed such pools to perform arbitrary token transfers. Combined, an attacker could use these to first fake the execution of a swap using their own malicious pool contract, which then used the callback into the RouteProcessor2 to steal users' approved funds. A whitehack was attempted to secure the funds which, due to the usage of a public mempool, unfortunately, resulted in losses due to MEV bots replaying the attack.\nIn May, Trust Trident's StakingRewards contract was exploited due to a lack of validation in the claim() function's parameters. The function rewards users based on the price of a USDT pair on PancakeSwap. However, the token addresses used to determine the PancakeSwap's pair contract needed to be passed into the claim() function, and were not sufficiently validated. This allowed an attacker to create a pair with a fake token (instead of USDT), control the price, and use this to inflate his rewards.\nIn June, Pawn Finance was exploited due to failing to verify whether an NFT had actually been transferred when using it as collateral for borrowing. Specifically, the ApeStaking contract's depositAndBorrowApeAndStake() function borrows APE tokens from another contract, expecting the caller to deposit NFTs. The check ensuring that such a deposit happened could be bypassed by specifying an NFT that is already owned by the ApeStaking contract.\nIn August, Exactly's DebtManager contract was exploited due to a lack of input validation for the supplied market address in several functions. The DebtManager contract has a permit system, which allows another user to manage the owner‚Äôs positions. However, the permit check is conducted by calling market.safePermit(), without validating the user-supplied market address. Thus, the attacker was able to bypass the permit check by supplying a malicious market contract address. The contract has a _msgSender variable, which is set to the permit owner after the permit check in the leverage() function, and throughout that entire transaction, DebtManager operates on behalf of _msgSender. Using this, the attacker was able to impersonate other users and steal their funds.\nIn October, Astrid Finance was exploited due to the lack of input validation in the withdraw() function. Astrid Finance is a liquid restaking pool where users can stake liquid staking tokens (LSTs) such as stETH, rETH, and cbETH. When a user stakes an LST, the pool mints a restaking token for that user. While unstaking using the withdraw() function, the user needs to provide a restake token address of one of those three tokens to return the corresponding LST. Unfortunately, there isn't any validation for the supplied token address. As a result, the attacker minted fake restake tokens and exchanged them for all of the LSTs in the pool.","donation-attacks#Donation Attacks":"In May, Yield Protocol was notified about a bug in their Strategy vault's burn() function responsible for redeeming shares. The issue was its use of pool.balanceOf(address(this)){:solidity} instead of the poolCached_ variable's value. Due to this, the attacker could inflate the share of underlying assets they would obtain, which would then be subtracted from the amount of liquidity that was accounted for in poolCached_. The injected funds would stay unaccounted for at the end of the burn() function, allowing the attacker to recover them by calling mint() and then withdrawing them normally. This process of donate, burn, mint, burn could be repeated until the vault had been emptied.","flawed-reward-systems#Flawed Reward Systems":"In May, Level Finance was exploited due to an error in its reward distribution function claimMultiple(). The function computes the rewards a user has accumulated up to the present moment and deducts any rewards previously claimed. Already claimed rewards were supposed to be tracked by the user.claimed storage variable, which should have been increased by the amount of rewards paid out. Unfortunately, an assignment operator had been used instead, merely overwriting the stored value with what had just been claimed. Due to this, the attacker was able to claim rewards multiple times until the contract was drained.\nIn May, Snooker's SNKMiner contract had a flaw in its referral system where users could earn rewards based on the amount of tokens referred users are staking. The problem was that it would calculate the rewards based on the current balances of the referred users, not based on the balances that had actually been staked for a longer time. An attacker exploited this by faking several \"referred users\", having each stake a small amount of SNK tokens. Later, when the rewards became available for claiming, the attacker used flashloaned SNK tokens to temporarily increase the referred users' balances in order to inflate the referral rewards.\nFinally in May, LunaFi's reward mechanism was exploited due to lacking a time lock on its claimRewards() function when claiming staking rewards. Without ensuring that funds had to be staked for an appropriate amount of time, exploiting this issue was a simple matter of claiming rewards from various addresses while reusing the same funds.","flawed-token-mechanics#Flawed Token Mechanics":"In May, Jimbo's Protocol was exploited due to its shift() function, which the JimboController uses to rebalance liquidity within the JIMBO/ETH Pool at Trader Joe's DEX. The protocol aimed to provide a semi-stable floor price for its JIMBO token by accumulating ETH in its treasury and using it to defend the token's price. However, the shift() function lacked slippage control, which made it vulnerable when rebalancing while the market was being manipulated. It's unfortunate that Jimbo's did not make use of the FLOOR token SDK provided by Trader Joe to achieve its goal.\nIn August, LeetSwap had a public transferFeesSupportingTaxTokens() function that, similar to a public burn() function, could be used to remove arbitrary amounts of tokens from a specified pool, which were then transferred to a fee-collection address. The ability to arbitrarily decrease the number of tokens from pools allowed an attacker to manipulate the prices of AMMs and drain multiple pools.","dao-governance-attacks#DAO Governance Attacks":"In June, Atlantis Loans was exploited by a governance attack that passed a proposal containing a call giving administrative privileges. After acquiring a sufficient amount of ATL tokens to create the malicious proposal, the attacker merely had to wait for 2 days until it was allowed to be executed. Passing the voting quorum of a mere 2% of the total supply, the attacker accepted the administrative role and began upgrading the protocol's contracts to include a backdoor function, allowing them to drain tokens of users that had previously granted an allowance to the protocol. Interestingly, the attacker had unsuccessfully attempted to execute this attack in April before.","incorrect-signature-scheme#Incorrect Signature Scheme":"At the end of June, Azuki DAO suffered an exploit due to its claim() function allowing signatures to be replayed. The function was intended to distribute the project's governance tokens to the holders of Azuki NFTs. When a user claimed the tokens using a signature, it was marked as having been used in a storage mapping. Unfortunately, this mapping wasn't actually checked for whether a signature had been used already. Using a single valid signature, the attacker could call the claim function over and over again. Although, even if it had checked the mapping, the function would still have been vulnerable to signature malleability.","the-2023-rewind-contest#The 2023 Rewind Contest":"As you may imagine, fully understanding and summarizing this many incidents takes quite a lot of effort. So for this year's rewind, I wanted to try a little experiment: I'd organize a \"Technical Writing Contest\" where participants would submit their summaries. They'd profit from the process by receiving feedback and mentoring on their submissions and I'd sponsor the best writer a ticket, flight, and accommodation to DevConnect in Istanbul so that they may participate at the TrustX Security Conference by Secureum.The idea was well received and soon Code4rena generously offered to sponsor the contest, increasing the number of winners who'd receive the prize of going to Turkey to 3 participants - a win-win for everyone! Furthermore, we were invited to speak about the contest at the TrustX conference where I'm sharing the intricacies of running the contest and the top 3 participants share their learnings.\nThe contest enabling this article was exclusively sponsored byCode4rena is a competitive audit platform that finds more high-severity vulnerabilities, more quickly than any other auditing method. Built by a team of leading industry experts, Code4rena is designed to protect your project and community by providing access to the best security researchers and smart contract experts in the world.Since its inception in 2021, Code4rena has rewarded auditors with over $20 million, unearthed 950+ high-severity vulnerabilities, and recently expanded its offerings to include bug bounties, bot races & embedded security teams.\nAnd here I'd like to end this Rewind by thanking all of the participants of the contest. Thank you, your participation made creating this year's rewind possible!"}},"/posts/2024/12/9/practical-grapheneos-for-the-paranoid":{"title":"Practical GrapheneOS for the Paranoid","data":{"":"December 9, 2024 by patrickd\nGrapheneOS is a FOSS (Free and Open Source Software) Operating System based on the Android Open Source Project (AOSP), specifically focusing on improving privacy and security. Although we'd all like to have the best security and best privacy possible, it's an unfortunate fact that such improvements rarely come without a cost to convenience and usability. Here, we will explore how to get the most out of Graphene's improvements, and the impact of these measures on the user experience with practical solutions.\nThis article is an attempt to tie together information and knowledge on mobile security and privacy in and around the GrapheneOS project so that less technical readers are able to follow. Feel free to investigate the references in brackets for the details. The author is not affiliated with the GrapheneOS project.","device-choice#Device Choice":"Probably the most common question by those looking into the GrapheneOS project for the first time is: Why are only so few devices officially supported, and why the heck are all of them expensive Pixel phones made by evil Google?","why-only-pixel-devices#Why only Pixel devices?":"According to GrapheneOS there currently simply isn't another reasonable choice. GrapheneOS does not have contractual exclusivity for Google's devices, and neither are Pixels incredibly secure, it's rather that all the alternatives are downright awful. The GrapheneOS project maintains a list of detailed requirements that they have for current and future officially supported devices and, unfortunately, at this time only Pixels are able to meet them.Pixel devices have first-class support for alternate operating systems, likely thanks to the fact that they serve as a reference for Android development. They receive proper regular updates of their firmware, and offer advanced hardware security features such as memory tagging which remain available even when non-stock operating systems are installed on them.Most other OEMs on the other hand merely offer half-baked, partially working support for alternative operating systems, as they see it as a non-production hobbyist feature. Heck, most of them completely skip basic security features and don't bother providing proper updates. To make matters even worse, they often add lots of complexity and, with it, attack surface, with their overall changes to the system.In the past, GrapheneOS has attempted partnering with OEMs but it turns out to be incredibly hard to make a device with comparable security to a Pixel. These partnerships fell through, in the end it was simply easier for these OEMs to make money by producing questionable hardware wallets for cryptocurrencies.The only Android OEM even close to Pixel devices in terms of hardware security is Samsung. They have even partnered up with Google to improve device security, with some success, but in the end Samsung still tends to lag behind by 2-3 years in shipping the new security features introduced by Pixels. While Samsung is theoretically only missing a couple of the requirements, they still lack serious alternate OS support, which prevents GrapheneOS from using even those features that it has.Broad device support would currently imply being available to very badly secured devices, unable to support many of Graphene's security features. It would also take away a substantial amount of resources from their work on privacy and security, due to a lot of it being rather hardware-specific. However, if there were other devices complying with the requirements, the GrapheneOS project would certainly plan to support them.","are-there-alternatives#Are there alternatives?":"There are various alternate operating systems for Android devices with a focus on privacy and security, or at least a broader support of devices. Though, according to the GrapheneOS project, none of these can truly be considered viable alternatives.The most basic critique the GrapheneOS team levies against pretty much all of them is the great delay and, in some cases, even complete omission of important security patches that users of the purely open source variant of Android (AOSP) would enjoy.LineageOS is one of these offenders, though granted, this project's focus is device longevity and broad compatibility rather than security. It's critically lacking verified boot, making it trivial to break into with physical access. The /e/OS project markets itself as fully \"deGoogled\" mobile ecosystem focusing on privacy while building upon Lineage's shaky foundation. The e/OS comes bundled with applications and services that give users a questionable feeling of privacy while these services themselves are arguably invasive.Graphene's possibly greatest \"competitor\", CalyxOS is apparently not only regularly behind on patches but has also misled users about this with inaccurate Android security patch levels. They furthermore implemented security features with serious flaws, such as the panic-wipe feature that didn't reliably delete incriminating data as intended.\nFor further information on various Android-based Operating Systems, you may want to take a look the following comparison table created by a third party: https://eylenburg.github.io/android_comparison.htm\nThen there is CopperheadOS, the name under which GrapheneOS was formerly known. I will not go into detail on the drama behind the hostile takeover and eventual separation from the company that was intended to financially sponsor the project. It's noteworthy to mention though that this company now sells CopperheadOS as closed source fork.Finally Purism with their custom, optionally Made-In-USA, hardware that promises control over your privacy with features such as (questionable?) hardware switches. The GrapheneOS team strongly disagrees with the choice of hardware security components made for their devices and also with the convoluted process required for firmware and microcode updates. Their Librem 5 device is nearly entirely closed source hardware and firmware though many people have been misled to believe the opposite to be true.GrapheneOS accuses these projects of empty marketing with buzzwords that mislead users into trusting both outdated and vulnerable soft- and hardware. They even go so far as to say that users would be better off using an iPhone (in Lockdown mode) instead of any of the mentioned projects as the next best option for privacy and security after GrapheneOS.Don't misunderstand, the GrapheneOS project does by no means claim that its operating system is impenetrable, but it does focus on substance rather than branding and marketing. This becomes quite clear when comparing Graphene's website to those of other projects: GrapheneOS is an unapologetically technical project which I personally greatly appreciate as it saves me from having to dig through marketing to find the technical facts. On the other hand, I can certainly see how this is rather intimidating to the average user.","can-google-devices-be-trusted#Can Google devices be trusted?":"We've covered that GrapheneOS currently only supports Pixel devices due to those being the only ones having sufficient hardware security measures in place. All well and good, but wouldn't that be pointless if those devices are backdoored?As mentioned, Google's Pixels serve as reference devices for Android development, causing many experts to work with them. Furthermore, Google has been very open to external security research, thanks to which Pixels have received great security research attention. It would be arguably difficult to hide backdoors within the devices under these circumstances.Another argument is that it would be far easier to attack the supply chains of tiny companies that outsource their manufacturing rather than compromise the global production of iPhone or Pixel devices without discovery. Users of such mainstream devices benefit from a lot of scrutiny that these devices receive. It's also worth noting that leaks from forensic firms, that specialize in breaking into smartphones and are often hired by governments, do not offer any evidence for the existence of purposefully placed backdoors either.Still, some argue against using anything made by Google out of principle. For the best chances to achieve this, they'd have to make use of Apple's products though, due to Google playing a huge part in the development of many open-source projects, including the Linux kernel itself. The mission of GrapheneOS is not entirely centered around the idea of avoiding one particular company by any means necessary, rather it's to obtain the best possible privacy with the best possible tools available.","which-pixel-to-choose#Which Pixel to choose?":"For the best security, using a Pixel of the 8th or 9th generation is strongly recommended.  They are considered significantly more secure thanks to the hardware features that GrapheneOS can make use of, such as memory tagging. The 9th generation comes with an additionally hardened and more efficient cellular radio, so if you are planning to use it with a SIM card you can expect improved security and battery life. The Pixel 9 Pros come with 16GB of RAM which will be especially useful if you're planning to use GrapheneOS with multiple user profiles (benefits of which will be explained later).The 9th generation is currently also ahead in terms of the hardened Linux kernel used, but the 8th generation is likely to be upgraded soon making both generations very similar in terms of security. With that in mind, a Pixel 8a phone will be the ideal budget device with some trade-offs in performance. Either way, both generations are receiving 7 years of full support.\nYou can find a table of how long each of the devices will be supported at https://grapheneos.org/faq#device-lifetime\nPixel devices of the 6th and 7th generation still have a few years of support left, making them a decent choice if you already own one and want to keep using it for a while longer. This is not a recommendation for buying a new one of these generations though. Also, keep in mind that there are various security features that GrapheneOS can only make use of with the hardware of newer generations.Anything older than that is considered End-Of-Life and the GrapheneOS project strongly recommends against continuing to use them, regardless of which operating system is used. This is true even though GrapheneOS still provides extended support for some of them, which is merely intended for harm reduction to buy users some time to migrate to a fully supported device. For example, there no longer are firmware patches and driver support for the Pixel 5a, and there's even a known, unpatched remote code execution vulnerability that could be exploited to take control of the device.\nPixel 9 Pro Fold\tPixel 9 Pro XL\tPixel 9 Pro\tPixel 9\tPixel 8 Pro\tPixel 8\tPixel 8a\t16 GB RAM\t16\t16\t12\t12\t8\t8\t256-512 GB Storage\t128-1024\t128-1024\t128-256\t128-1024\t128-256\t128-256\tGoogle Tensor G4 Processor\tG4\tG4\tG4\tG3\tG3\tG3\t14 Concurrent User Profiles\t14\t14\t10\t10\t6\t6","self-install-or-pre-installed#Self-Install or Pre-Installed?":"There are companies selling Pixel devices with GrapheneOS preinstalled, but there are also some that claim or wrongly suggest that the operating system installed is GrapheneOS or using parts of it. Additionally, there's a chance that such preinstall- or used devices have been tampered with.GrapheneOS is not affiliated and does not endorse any of these companies and instead recommends buying a supported device and going through the installation process yourself. Buying one at a local electronics store with cash will even allow you to do so anonymously. Just make sure to avoid devices that are locked to a specific carrier.The easiest method to install GrapheneOS is using a compatible web browser on a compatible second device - that can be a chromium browser on a laptop or even on a second Android phone! You just need a cable to connect both devices. On the secondary device, you simply have to open the web installer page and follow the instructions. Don't worry: Your device will not lose its warranty, it'll not get bricked, and if you don't like GrapheneOS you can always go back to stock Android.","tamper-protection#Tamper Protection":"","verified-boot-key-hash#Verified Boot Key Hash":"Assuming you are now a proud owner of a Pixel device with a fresh GrapheneOS installation, you might be surprised by the warning greeting you when the device is started. It'll disappear and continue booting GrapheneOS after a few seconds unless you press the power button to pause the screen. The string of characters shown following \"ID:\" is called the verified boot key fingerprint which is a cryptographic hash value that allows you to verify that the GrapheneOS version installed is authentic - and it should never change!\nDevice\tVerified Boot Key Fingerprint\tPixel 9 Pro Fold\t\tPixel 9 Pro XL\t\tPixel 9 Pro\t\tPixel 9\t\tPixel 8a\t\tPixel 8 Pro\t\tPixel 8\t\tPixel Fold\t\tPixel Tablet\t\tPixel 7a\t\tPixel 7 Pro\t\tPixel 7\t\tPixel 6a\t\tPixel 6 Pro\t\tPixel 6\t\t\nThe fingerprints differ depending on the device model and the above list of them was copied from the web installer page. Although unlikely, it's possible that the server infrastructure of the GrapheneOS project was compromised and that attackers have replaced both the operating system files used during installation and the list of key hashes. You can compare the fingerprint displayed on your device with the table above to ensure that you've installed a legitimate version of GrapheneOS.","auditor-app#Auditor App":"GrapheneOS comes bundled with the Auditor App. It's another way to validate the authenticity and integrity of the operating system to ensure that no tampering has occurred. You can execute a check manually with a second device that has the Auditor App installed. This does not need to be another GrapheneOS installation as you can find the app on Google's Play Store.\nThis verification can be automated using the attestation.app website which is part of the GrapheneOS project. After registering on the website and pairing your device you'll receive an email alert if the device fails to provide valid attestations in time.\nSadly it's currently not possible to host your own remote attestation server as using it would require changes to the Auditor App.","hardening-through-settings#Hardening through Settings":"There are three fundamental aspects on how GrapheneOS defends itself against security vulnerabilities: It reduces the amount of \"surface\" (ie. active functionality/code) that is exposed to potential attackers, makes the exploitation of vulnerabilities as difficult as possible, and sandboxes (ie. isolates) components from each other to decrease the impact of a vulnerability being exploited. As such measures impact usability and device performance, GrapheneOS allows users to choose their own preferences via various toggles in the settings.","the-lock-screen#The Lock Screen":"User data is stored in an encrypted manner with a key that is derived from, among other things, your chosen screen lock-method. That these can't simply be bypassed using brute force is ensured by delays enforced by \"secure element\" hardware. Thanks to this measure, even a random 6 digit PIN provides a high level of security. If you don't want to depend on the security of the secure element, you can make use of long passwords with up to 128 characters.Unlocking using a pattern was removed from GrapheneOS as it is essentially a much worse version of a PIN and encourages unsafe pattern choices. Instead, you should use at least a 6 digit PIN and consider turning on the PIN scrambling feature which raises the difficulty of figuring out the combination by an observer, fingerprint marks or side channels.Fingerprint Unlock can be configured, though for best security you should consider restricting it to authentication within apps (ie. disabling 'Use for screen unlocking'). The GrapheneOS team has announced that 2-factor fingerprint unlocking will be launching soon where unlocking the device will require both a fingerprint scan and the PIN/password.If you have an eSIM or especially a physical SIM, it makes sense to configure a SIM PIN - though it should preferably be a different one than the one used for unlocking.\nLastly, GrapheneOS offers setting a duress password/PIN. Once set, using it anywhere where a unlocking password or PIN can be entered, even in secondary user profiles, will irreversibly wipe the device (and installed eSIMs). When triggered, information required for decryption is deleted and the device will shut down. On the next boot an invalid filesystem is detected and the device can be set up again as if it had been factory reset. Note that this does not wipe the encrypted data itself as that would take a long time and gives attackers an opportunity to interrupt the process.","auto-reboot#Auto-Reboot":"A freshly booted device that hasn't been unlocked yet has its user data still fully encrypted, or \"at rest\". Continued use after the first unlock often leads to unencrypted data accumulating in the device's ephemeral memory, which is something that forensic companies exploit. The Auto-Reboot feature was introduced as a protection against the extraction of unencrypted data.Although it's possible for applications to put sensitive data back to rest when the device locks, it's upon the developers to actually implement them that way - which rarely happens. Even the developers of the privacy-focused Signal messenger have shown little interest in implementing it, instead leaving it to forks like Molly to handle this in a better way. By automatically rebooting the device after some time, it is put back into a state of \"before first unlock\" (BFU).The reboot timer starts every time the device is locked and successfully unlocking it resets the timer. By default, the timer is set to 18 hours with the lowest, and most secure, option available at 10 minutes. Note that the timer will only start if the device has been unlocked at least once since the last reboot.With the default value of 18 hours, the timer will be cancelled during consistent use to avoid impacting user experience. Due to GrapheneOS's very frequent updates requiring regular reboots in any case, this feature serves a secondary useful purpose. If that still causes the device to reboot too often, the timer can be increased up to 78 hours or even completely disabled - though GrapheneOS strongly recommends against this.Note that if you're planning to use secondary user profiles, this feature may be particularly annoying as all user sessions will be ended and the device resets back to the default Owner user.","usb-restrictions#USB Restrictions":"When forensic companies attempt breaking into smartphones they typically prefer to do so via the USB interface which has a massive attack surface due to the many functions it offers. Pixel devices offer hardware-level control over the USB-C port, a feature that isn't currently even used by the stock version of Android but a very important aspect to the GrapheneOS project.\nBy default, GrapheneOS disables new USB-C connections as soon as the device is locked. In other words, you can plug in and use USB-C data while the device is unlocked, but once it's locked and you've pulled the plug it will reject new connections. This measure includes disabling USB-C alternate modes such as DisplayPort.The most secure, though arguably also the most inconvenient, option available is to fully disable the USB-C port while the operating system is running. This will even prevent any vulnerabilities present in the power delivery logic of the device, but will also require it to be turned off in order to be charged.","wireless-attack-surface#Wireless Attack Surface":"With USB out of the picture that leaves wireless attack vectors such as Wi-Fi, Bluetooth, and cellular. However, getting into a device from any of these will be much harder and more complex. Isolation of hardware components has become the de-facto norm for mobile devices. Pixels even have separate chips for each of these radios, and if you wanted you could remove the chips individually and the device would be able to continue to function.Although not yet enabled by default, you can set a timer to automatically turn off Wi-Fi and Bluetooth. The timer will begin once there's no longer an active connection. The GrapheneOS project is planning to extend this to NFC in the future as well.The issue of cellular connectivity is a lot more complex. First off, 5G, SMS, MMS, and Calls generally work as fine on GrapheneOS as they would with stock Android. GrapheneOS adds various toggles that once again attempt to reduce the attack surface, though depending on your carrier, and the country you're in, you may have to play around with them to see what works.\nYou can use eSIM with GrapheneOS but because it requires proprietary functionality from Google it is fully disabled by default. Before, enabling eSIM support required Google Play but this is no longer the case, and using eSIM now will not share any data with Google. Note that, while a wipe via distress PIN/Password also deletes the eSIM, the same is not true for a normal factory reset.\nThe GrapheneOS project recommends using the LTE-only toggle when possible. LTE, which is sometimes referred to as 4G or 5Ge, is much more modern than the legacy 2G and 3G protocols, but also a lot less complex and more stable than the new 5G protocol.. Note that, although it may make some forms of interception more difficult, the only intention of the LTE-only mode is disabling an enormous amount of code related to both these legacy and bleeding edge protocols.","cellular-privacy#Cellular Privacy":"LTE and 5G do come with a form of encryption, but this is mostly intended for the protection of the network itself and not for the protection of your privacy. No matter which mode you're using, you should avoid traditional phone calls and text using the cellular network, and instead use end-to-end encrypted messengers such as Signal and SimpleX. In fact, if you can, always prefer using Wi-Fi over cellular.The traditional telephone system is historically insecure and not designed with the goal of protecting user's privacy. You can imagine it like a walled garden where, once you've become a trusted party with access into it, you gain a significant amount of information and control over the network. This access can be bought for a few thousand USD per month and allows intercepting phone calls, SMS messages, and in some cases even roughly tracking someone's location. To do that an attacker needs your SIM card's unique IMSI identifier, but often it can be looked up simply by knowing your phone number. Using this the attacker would be able to intercept a 2-factor-authentication SMS while ensuring that your phone would not give any indication that such an SMS was ever sent to your number at all.When your phone authenticates with the cellular network it does so while providing information on both your SIM card and the device's cellular radio hardware. If you've bought both the phone and SIM anonymously, you're essentially using a persistent pseudonym. Since hardware information is being shared, replacing the SIM alone is not sufficient for a fresh identity.At this point, you might be considering using an external device, like a dedicated mobile hotspot, which would be much cheaper to regularly swap out together with a new SIM card. While this will increase your privacy, it will hurt your security as these devices typically have much worse hardware isolation and are way behind on security updates. Compared to using the isolated internal radio of your Pixel, this will make it much easier for an attacker to gain control over the dedicated device, and if it is connected to your Pixel via USB this opens up a massive surface for attacking your phone.Some of these dedicated cellular devices allow spoofing the IMEI, ie. changing the hardware identifier to an arbitrary value. This would allow you to reuse the same dedicated device and simply change the IMEI value together with a new SIM card for obtaining a new identity on the network. You should know however that the IMEI is not the only hardware-specific identifier that cellular radios leak and, furthermore, there are ways to create \"fingerprints\" of these devices that could allow re-identifying them even with identifiers changed. In the worst case, you could even draw a lot of attention to yourself when the spoofing is too obvious.The GrapheneOS project does not recommend using a secondary device for cellular communication, but if you really want to do it then it would be better to use another Pixel device with GrapheneOS installed for it. Note that if you're sharing cellular Internet via Wi-Fi, it's possible that someone nearby tracks your movements by tracking the signals of your Wi-Fi access point.What about airplane mode?Airplane mode is the only way to fully disable the cellular radio transmit, receive, and tracking capabilities of your device, and works appropriately on all Pixels. Once in airplane mode, Wi-Fi can be re-enabled and used without activating the cellular radio again. If you intend to use your Pixel as a Wi-Fi-only device, you should consider removing the quick tile (the airplane-mode toggle visible when swiping down the status top-bar) so that you won't re-enable it by accident.You should be aware that cellular is not the only way carrier services can be used. Remember that there's Wi-Fi calling and texting. To prevent the SIM from authenticating with the carrier and using their network services via other Internet connections, you need to disable the SIM itself.Newer devices have special offline tracking systems intended to locate lost devices. GrapheneOS does not support and will not support these systems. If you want to be absolutely certain, you may consider keeping it within a faraday bag while you're not using it.What about using a data-only SIM?It doesn't particularly matter whether you're using a data-only SIM card in terms of privacy, you're still authenticated to the cellular network. Not having texts and calls however does reduce attack surface for exploits. GrapheneOS may in the future add toggles to disable these capabilities to practically turn normal SIM cards into data-only equivalents.What about using a VPN?Using a VPN has no influence on carrier-based calls or texts. These functions will not go through the VPN even if you are using a Wi-Fi connection instead of cellular.What about data-saver mode?Enabling the data-saver toggle will globally (ie. in all user profiles) stop apps from using cellular data in the background. Apps that use foreground services, meaning those apps that keep themselves running in the foreground via a permanent notification, are excluded from this restriction. Mobile data use can also be restricted per app.What about using no SIM?If you have no SIM card, and you are not in airplane mode, your device will still connect to the cellular network but it will not authenticate and should also not share any hardware identifiers. You will still be able to make emergency calls and receive emergency alerts. Note that making an emergency call will share radio identifiers of your device.What about emergency alerts?Emergency alerts are sent out through the cellular network to all connected phones, even if it has no SIM. Normally, only airplane mode prevents receiving them. Because GrapheneOS is not subject to typical regulations it provides toggles for turning off even alerts of \"presidential\" priority.","app-exploit-protection#App Exploit Protection":"So far we've discussed how to protect our device from outside threats, but it's just as important to ensure that none of the installed applications can take over from the inside. Apps on Android always run isolated within their own sandbox, limiting the resources they can access to those that they have been given permission for. Malicious apps may request and use permissions for purposes unrelated to the app's core functionality, others may even attempt breaking out of their sandbox. Often the application itself is not even intentionally malicious at all but has a vulnerability or its supply chain is being attacked.Under App Exploit Protection you can find various measures that will increase the difficulty of an application breaking out of its sandbox. Many of them are not enabled for user-installed apps by default because they may cause those apps to crash or not work properly. But it's better to turn them on by default globally and then turn them off selectively for those apps that have trouble.There are other measures that are implicitly always turned on. One of them is the use of ahead-of-time compilation instead of just-in-time compilation. This saves battery life, improves the performance of many apps and, generally, is another important security feature. It has a drawback though: App installations and updates will take a lot longer than on stock Android.\nPermissionsIn stock Android, permissions only exist for accessing the Camera, Microphone, Body Sensors, and Activity Recognition. Access to accelerometer, gyroscope, compass, barometer, thermometer, and any other sensors are simply given to apps by default without requiring any explicit consent. GrapheneOS adds a toggle to prevent access to these sensors being given by default. Since this too can cause crashes in applications that expect to receive valid data from these sensors, this measure is not active in a fresh GrapheneOS installation. Turning it on will notify you when apps attempted accessing such a sensor and you'll be able to selectively grant this permission according to your own judgment.Another permission that stock Android implicitly grants to all applications is access to networking functions. This includes device-local networking (localhost) which is currently a known way to bypass user profile isolation and allows apps of different profiles to communicate with each other. In GrapheneOS you'll be asked whether you want to grant this permission during the installation of an app. When the Network permission is not granted to an app, GrapheneOS pretends the network is down, which is usually handled by apps in a graceful manner.Just like you can disable app exploit protections on a per-app basis, you can take away permissions from apps according to your own judgment. Be careful to not remove any permissions from system apps that are installed by default though, as this can cause unforeseen issues.Scoped AccessThere are popular apps that demand rather invasive permissions such as access to all contacts or all files on the device. Graphene's scope functionality allows selecting a subset of contacts or files to which access is granted while the app in question will believe to have been given access to everything.By default, contact scopes act as if the contacts list is empty and users can then grant different kinds of access to specific contacts or groups of contacts. Access to contacts is quite granular, allowing you to only share specific data with the app instead of the full contact info.Users can enable storage scopes instead of granting the storage permission to apps. This will have the effect that the app can't see any of the files that were created by other apps, unless the user explicitly specifies files or directories that it should be allowed full access to.The GrapheneOS project is planning to add similar access scopes for other things such as Location, Camera, and Microphone.Microphone & Camera togglesAlthough these are available in stock Android too, it might be noteworthy that there are toggles for disabling access to the microphone and camera. They are also available as quick tiles when swiping down the status top-bar.While it might be tempting to disable access to these globally and only turn them on when needed, this might end up being very inconvenient when you quickly intend to snap a photo (eg. via the power button double-press shortcut) or when you want to accept a call while the phone is still locked. In these cases, you'd first have to unlock the phone and enable the appropriate access, which could take so long that the person calling decides to hang up. Instead, you could leave the system-wide access to microphone and camera enabled and deny these permissions at a per-app level: Leaving them enabled for the phone and camera app while setting all others to 'Ask every time'.If you're certain that you'll never need any of these sensors anyway, you could even buy one of Nitrokey's modified Pixel phones where the hardware components have been physically removed.","system-updates#System Updates":"GrapheneOS system-update download and installation is automatic and happens seamlessly in the background. A reboot is still needed for them to take effect, but this process comes without risk thanks to the automatic rollback if the first boot of the updated operating system fails.Automatic reboots after an update are possible but disabled by default as these could happen in the middle of a phone call. If you want to avoid updates being downloaded using cellular data, you should change the 'Permitted networks' setting to 'Unmetered' only. Some users have reported that updates heavily drain the battery and cause the device to heat up, which can especially get in the way while you're not home. You can turn on the 'Require device to be charging' toggle to avoid such cases.\nIt's possible to completely disable automatic updates by disabling the 'System Updater' app. The GrapheneOS project strongly recommends against this, as you won't be getting privacy and security patches fixing vulnerabilities or adding improvements.Some may be concerned whether a future update could introduce a backdoor. There are several measures in place to prevent malicious updates: They must have a valid cryptographic signature, which is enforced by both the update client as well as the verified boot mechanism. Things like downgrade attacks are also prevented locally.The GrapheneOS project further argues that existing legislation can only target individual users, and cannot coerce malicious updates to be deployed to all GrapheneOS devices. Because the updater doesn't provide any uniquely identifiable device information while requesting system updates, GrapheneOS won't be able to comply with a government's demand to send a backdoored update to a specific user. The update server will however be able to see the requester's IP address, which can be obfuscated by using a VPN or Tor.","backups#Backups":"GrapheneOS comes with Seedvault integrated at Settings ¬ª System ¬ª Backup as a solution for creating backups or moving from one device to another. Bear in mind that if you're using secondary user profiles, you'll need to set it up in each profile separately. Some apps like Signal or Molly use a type of encryption of their application's database that can only be backed up and restored through these apps themselves. If you're planning to use a USB drive to store your backups, a common best practice is initially creating the backup in the device's internal storage, and only moving it over to the drive once it has finished.There's also a known issue where backing up user files from storage might not include all of your files, therefore you shouldn't rely on Seedvault to include all of your important files yet. You may want to backup your important files separately, eg. to your laptop using a USB connection (Use USB for 'File Transfer'). But here too you'll have to establish the connection for each profile separately. GrapheneOS is hoping to replace Seedvault with a better and more reliable solution in the future, although there are currently tasks that have higher priority.","secondary-user-profiles#Secondary User Profiles":"User profiles mimic having separate phones to allow multiple users to share the same device. In the following, we'll be exploring how this can be used to isolate apps from each other and compartmentalize the user's data. Though before that, I'd like to point out that the isolation that the app sandbox provides and the compartmentalization offered by access scopes will already be sufficient for many users.On a freshly installed GrapheneOS, multiple users are disabled by default. Even so, when you unlock your phone after booting, you'll log into the \"Owner\" user, as in \"The user who is the owner of the device\". The Owner user shouldn't be mistaken for something like a privileged \"root\" user from Linux. Although the Owner has more administrative control over the device than other users, regular apps have the same access when running in Owner as they would in any other user profile.Each user is encrypted with their own keys protected by their respective lock method. The Owner user is special in the sense that it does not only store the Owner's data, but also sensitive system-wide operating system data. Because of this, you always need to unlock the Owner profile first before any other user profile can be used. The Owner profile, and the apps running within it, will always be running in the background while you're using another profile. Nonetheless, the Owner profile does not have any access to the data stored in other profiles.\nCross-Profile NotificationsAs visible in the screenshot above, it's noteworthy that you can actually receive notifications from another profile that is running in the background. Although the notification will only tell you in what profile it happened and by what app it was triggered, this is still an addition by GrapheneOS that significantly improves the user experience with secondary user profiles.Example Use CasesBefore continuing on the different types of user profiles available, we should first discuss what even the advantages of using multiple profiles, compared to only using the Owner profile, are.First of all, after setting up a new user you'll be greeted by a phone that appears to be in the state of being started for the first time. None of the user-apps you've already installed are there, everything is empty. This can be very useful if you want to log into different users of the same application, eg. if you want to use a messenger with multiple accounts, but the app does not support that, you can simply install it twice in different profiles.Separating apps between different profiles will also prevent them from easily being able to communicate with each other. For example, there's the normal Facebook platform app but also the separate Facebook Messenger app. If both apps agree to it, they can use something akin to inter-process communication to exchange information between each other - but only if both of them are running within the same user profile.As mentioned, if you have apps that run in the background of your Owner profile, they will always be running unless you manually stop them from doing so. If you have applications that you rarely make use of, it makes sense to install them in a secondary user profile. Once you're done with them you can hold the device's power button and you'll be offered to end that user's session. This will make sure that all apps of that profile are stopped and their data is put to rest and fully encrypted.You can also create and use user profiles only temporarily and immediately delete them afterward. As apps installed within one user are not aware of the other users, you can use profiles like a browser's incognito mode. A profile's filesystem is completely isolated from other profiles, and although you could set up a storage scope to achieve the same, there'll be no need to do that since your temporary profile will be empty.Further above, we've talked about how the Auto-Reboot feature was added to ensure that data is eventually put to rest and no unencrypted data will be available for forensic companies to extract. If you employ a secondary user profile instead of the Owner for your regular usage, this will be much less of a problem: While putting the Owner's data at rest requires a reboot, putting a secondary user's data at rest simply requires ending their session.Number of User ProfilesGrapheneOS raises the limit on the number of secondary user profiles from the standard 4 to 32, where one of them is always reserved for the guest user. However, being able to create so many user profiles does not mean that all of them can run concurrently as that would impact the device's performance rather negatively. GrapheneOS scales the number of maximum concurrent users based on the amount of RAM built into the device.\n\tPixel 9 Pro Fold\tPixel 9 Pro XL\tPixel 9 Pro\tPixel 9\tPixel 8 Pro\tPixel 8 & 8a\tRAM\t16 GB\t16 GB\t16 GB\t12 GB\t12 GB\t8 GB\t# Concurrent Profiles\t14\t14\t14\t10\t10\t6\t\nIf you have user profiles for use cases where it's never necessary for that user to keep running in the background, you can un-toggle 'Allow running in background' when editing that user via the Owner profile. That way you don't have to explicitly end the user's session, because simply switching away from that user will put the profile to rest.\nCross-Profile App installationYou might be surprised to find out that it's possible for user profiles to update each other's apps, and also for the Owner user to be able to install his app into another profile. Didn't we say that the file systems of each profile are completely isolated? Well, yes - but it's not like each profile is running on a completely independent operating system, and it turns out that the code of applications isn't actually stored within each user but in the underlying system that is shared between all of them.The fact that the Owner is able to install apps in other profiles as long as they're already installed on the Owner means that we don't actually need to install app stores in every profile to get the apps we need. Instead, we could use the Owner profile as an \"App Command Center\": We install app stores solely in the Owner user. When we install new apps we uncheck giving them the Network permission and immediately mark them as disabled. Then we open the user management and install the newly installed apps into the profile where we'll actually use them.DrawbacksThere are some inconveniences that come from making active use of secondary user profiles. For example, the Auto-Reboot feature will cause all user sessions to end and throws you back to unlocking the Owner profile first. That also means that all apps in those profiles will be forced to stop running and you'll miss out on receiving notifications from them until you log back into that user. Assuming you didn't set a very short time for Auto-Reboots, this shouldn't happen too often though.As mentioned, the file systems of profiles are completely isolated, which means there's no native way to eg. share a meme that you saw on social media in one profile via a messenger that is running in another profile. Typical workarounds include having cloud-based file synchronization or a messenger installed within both profiles - but that will require an Internet connection and potentially wastes your mobile data. You could set up local file synchronization using apps like Syncthing or an FTP server+client app - but these are usually annoying to set up. If you're already on Linux with a KDE desktop you might as well use KDE Connect to exchange files between all of your devices and profiles. If not, the Inter Profile Sharing app I've built out of frustration on this issue will hopefully be your best friend. If you really want to avoid installing any apps for this, your best option is getting a USB Stick with a USB-to-USB-C adapter.If you're installing apps in secondary profiles that require SMS verification, you might need to temporarily enable 'Turn on phone calls & SMS' for that user.","private-space#Private Space":"The private space feature is a recent addition to Android. Technically this is simply a secondary user profile that is nested within the Owner profile: When the space is locked, the private profile user is stopped, and when the space is unlocked, the user profile is started. Except for the shared clipboard with Owner, it's separated the same way as a secondary user profile.The advantage of using a private space over a secondary user profile is that the UI, in places such as notifications and settings, will be \"merged\" while the space is unlocked. That means if there's a notification from within the private space, it will be fully displayed in the Owner profile (compared to normal secondary users where this will only show the user and app name). While this makes it slightly less isolated than a dedicated user, private spaces can be a lot more convenient.Compared to having up to 31 secondary user profiles, a device can only have a single private space and it must always be part of the Owner. The GrapheneOS project is considering changing this though, among other improvements for private spaces. It's also noteworthy that the private space user is not listed within the user management interface, meaning features such as installing Owner apps into the private space are not available. Furthermore, locking the private space does not purge the encryption keys as well as ending a secondary user's session would.A drawback of private spaces, compared to full secondary user profiles, is that it's not possible to grant it access to 'phone calls & SMS'. This prevents verification via SMS from working and prohibits using some apps within private spaces.","work-profile#Work Profile":"Work profiles are similar to private spaces in terms of user experience. They were originally designed for BYOD (Bring Your Own Device) deployments for corporations, which is why you need a separate \"device manager app\" to create them. This app, and through it the corporation it belongs to, has control and ownership over the data within the work profile. However there are local management apps, such as Shelter, that allow the creation and management of a work profile without an external owner. Either way, you'll always have to place trust in a third-party app to use work profiles, unless you program your own.GrapheneOS calls work profiles created by apps like Shelter a \"poor man's private space\", and strongly recommends using real private spaces instead. Like private spaces, you can only have one and it must be located in the Owner profile. It's absolutely possible to use both a work profile and a private space alongside each other though.Private spaces have better isolation, proper encryption, and better user interface integration with the Owner profile. Work profile management apps can enable a lot of communication between the Owner user and the nested work profile. For example, work profiles do not block the communication of applications between profiles, which may increase convenience but negatively impacts privacy and security.","vpn#VPN":"For using VPNs the general best practice is that each of your devices should have a separate VPN connection that will obtain a separate exit IP address.  Because of this, all profiles (including work profiles and private spaces) have their own VPN configuration by design. This prevents an external party from tying them together based on the same exit IP address.You can prevent a profile from ever directly accessing an Internet connection by enabling the 'Always-on VPN' and 'Block connections without VPN' toggles by default. GrapheneOS has made many improvements to Android to prevent connections from leaking, ie. bypassing a VPN connection. Although there are still some known cases of VPN leaks, fixing these is one of the top priorities of the GrapheneOS project. These toggles can even be exploited to create a secondary user profile that has no Internet access at all, if you have a use case for that.At the moment, the GrapheneOS project only recommends using the official WireGuard app and the official Mullvad app. Mullvad is generally preferable though, as it works well with Graphene's exploit protection features turned on. To avoid fingerprinting and tracking, GrapheneOS recommends using the default, network-provided DNS servers to blend in with other users.Remember that while each profile has its own VPN configuration, the underlying networking is shared. Because of that, it's currently possible for apps with the Network permission to communicate across profiles via localhost, which is what I'm using for the Inter Profile Sharing app to function. But this will likely change in the future as the GrapheneOS project is planning to introduce separate network namespaces for each profile.","apps#Apps":"A fresh install of GrapheneOS comes with a very minimal amount of apps, and there are several reasons for that: Bundling additional apps with the operating system will increase the attack surface right from the beginning, GrapheneOS prefers users to be able to choose which apps they want to install based on their own judgment. GrapheneOS is focused on meaningful improvements to privacy and security, bundling more apps into the operating system would likely not only be outside that focus but even counter to it. GrapheneOS also wants to avoid bundling apps and services of third parties as few of those would actually be aligned with its goals and values.You may notice that among these few apps, GrapheneOS actually comes with its own 'App Store'. Note that this app repository is mostly intended for the distribution of Graphene's own first-party applications, and hardened versions of externally developed open-source apps. The list of available applications will be purposefully kept minimal, while third-party apps should seek to be included in Accrescent - the officially endorsed app store of GrapheneOS, which can be installed via Graphene's minimal store.","bundled-apps#Bundled Apps":"Of the few apps that GrapheneOS comes with, about half of them are inherited from the Android Open Source Project with minor changes and are very primitive in terms of functionality and user experience. Many of the AOSP apps were great choices 10+ years ago when Android's UI was more primitive and expectations were far lower. Over time Google replaced them with more modern versions for their stock OS but abandoned the open-source versions. GrapheneOS is planning to overhaul or replace them as well, though there are often licensing issues with possible alternatives.If you prefer Google's versions of apps such as the Camera, Gallery, and Keyboard, you can absolutely switch back to them without opting into their invasive usage statistic reporting and without uploading your photos to their service. Some of Google's stock Android apps, such as the screenshot editor (Markup) and the Thermometer (for Pixels with the appropriate sensor), are mirrored in Graphene's App Store as they're not available in the Play Store.CameraThe Camera app that GrapheneOS comes with has already been modernized, it's focused on privacy and security and is arguably better than any of the open-source alternatives and even most paid apps. It includes modes for capturing images, videos, and QR/barcode scanning. It has basic HDR+, Night mode, multi-camera zoom, EIS, etc. There's no loss in terms of photo quality of the camera by running GrapheneOS instead of stock Android.But still, it does not have the full set of features offered by the stock 'Pixel Camera' app. The Pixel Camera, previously Google Camera, can take full advantage of all available cameras and image processing hardware on GrapheneOS.Though to reduce attack surface, direct access to low-level hardware by Google's Apps is controlled by an extra toggle. The 'Special access to hardware accelerators for Google apps' toggle is enabled by default but does not grant these apps any additional access to data.GalleryGrapheneOS is planning to completely replace the current AOSP Gallery app, unfortunately, there's currently none available that has both acceptable licensing and proper image editing. If you are looking for a good open-source alternative, Graphene as mentioned IacobIonut01/Gallery and Aves.You can also install the 'Google Photos' app, that you'd get with stock Android, via the Google Play Store. It'll be able to make use of most of the AI features as well as hardware acceleration.KeyboardGraphene's default keyboard is essentially Google's Gboard from 2014. It used to be an open-source project with a few closed-source components but ended up becoming entirely closed-source and rebranded to Gboard. It's missing some features such as sliding on the space bar to move the cursor, one-handed mode, better handling of emojis, and especially swipe typing.Google's modern Gboard is definitely one of the nicest keyboards at the moment. Using it is fine as long as you don't opt-in to usage statistics and other invasive options. Remember that active keyboards have access to all the text you're entering, the text you're editing and the clipboard contents at all times.If you're looking for an open-source alternative, you should have a look at FlorisBoard. Its user interface isn't very polished yet, but it's an improvement in terms of features compared to the default keyboard. The GrapehenOS project is considering forking and moving to it in the future.Vanadium BrowserGrapheneOS includes their Vanadium subproject which is based on Chromium with enhancements in privacy and security. It's used by both the operating system as default browser as well as by other apps that need to render web contents. The GrapheneOS project recommends this browser to be used as is; additional browser extensions or modifications are only likely to make you stand out more, making you easier to track. To prevent websites from accessing standard sensors, you can toggle off the 'Sensors' permission for the browser app as a whole.","app-compatibility#App compatibility":"There's currently only a tiny subset of Android apps incompatible with GrapheneOS. These are specifically apps that make use of the Play Integrity API which requires the operating system to be officially certified by Google. This mainly affects apps of banking/financial nature, location-based competitive games like Ingress, as well as some strange outliers such as the McDonalds app, Authy, and the Uber app for drivers. By implementing it, these apps have chosen to ban the use of alternative and modified operating systems. While this makes somewhat sense for games as basic anti-cheat system, it is by far not as effective as Google makes it out to be. This also prevents NFC payments made via Google Pay. A simple way around this restriction is using a Pixel or Galaxy Smart Watch paired with GrapheneOS. Fortunately, there are other services, such as those provided by European banks, that have their own tap-to-pay which works on GrapheneOS. For the US there's hope that Curve Pay will be available soon.Although GrapheneOS provides the same standard security model as stock Android does, Google certifies operating systems not based on security but whether they licensed it. There are ways to bypass some of these restrictions, but they would likely be blocked in time and would only be a temporary workaround. According to the GrapheneOS project, the only permanent solution is regulatory or legal action based on this being highly anti-competitive and illegal behavior.Play Services DependencyAnother caveat to Graphene's app compatibility is that some applications have a dependency on Google's Play Services, which is most often the case for messengers and social media. The reason for this is that they rely on receiving events for notifications through Google servers via Firebase Cloud Messaging (FCM). Some of these apps are able to fall back to their own implementation of push messages or frequent polling, but this typically requires the app to run with a foreground service. The app may also require being given 'Unrestricted' background usage, otherwise it'd be interrupted by the automatic battery optimization, causing delayed notifications.An example of this is Signal, which will fall back to its own push mechanism when FCM is not available. There have however been reports that this performs badly and inefficiently (draining the battery). Instead, the alternative Molly client is often recommended for use without Play Services.For apps that have a hard dependency on Play Services, you have the option to install and use the official Google Play Services restricted to the standard app sandbox. Thanks to the compatibility layer Google Play will receive absolutely no special access or privileges on GrapheneOS. It provides nearly complete compatibility aside from a small subset of functionality that hasn't been ported yet or cannot be provided due to being inherently privileged. The Play Store and its services are fully available, including in-app purchases and app/content license checks. It can install, update, and uninstall apps as usual, assuming it has been given authorization as an app source and consent for each action.To make use of the compatibility layer, install 'Google Play services' from the GrapheneOS App Store. This will also install the Play Store, which is a dependency of Play services (don't disable the store app or the services will stop working too). While making use of the Play Store requires being logged in with a Google Account, there's no need to log in if you're only installing Play Services for app compatibility reasons. Also note that some apps including Signal need to be installed after sandboxed Google Play in order to make use of it properly.After installing Play Services you'll get a 'Missing optional permission' notification from the compatibility layer. Tapping it will ask whether you want to allow Google Play services to always run in the background, which will keep a connection open to Google's FCM server for reliable notifications, but will also reduce battery life. Agreeing to this will set the background usage to 'Unrestricted'. Leaving it on 'Optimized' will heavily restrict background usage based on how much it is used, while disallowing it will prevent background usage near totally. Make your choice depending on how important push notification timeliness via FCM is to you.In terms of security, it does not make much of a difference in which user profile you install Play Services. If you're hoping to avoid Google as much as possible, it's best to install it in a profile that you're not planning to use as your main profile. For example, if you were planning to only use your device with a single profile, it would be best to install Play Services within the Owner profile's private space.I'd recommend installing Play Services in the Owner profile with background services disabled but authenticated with a Google account (it's possible to create one anonymously). The Owner profile will only serve to download and install apps (without Network permission, and apps immediately disabled after the download) which can then be installed into other profiles via the user management UI. This way you can have a secondary user profile within which Play Services are installed with 'Unrestricted' background usage, but without being authenticated to a specific Google account. Into this profile, you can install the messengers and social media apps that require FCM for reliable notifications.Android AutoIf you've connected your Android phone with your car before, you're probably familiar with Android Auto. Originally, it requires privileged access but Graphene's sandboxed Google Play compatibility layer makes it work with a reduced level of privileges. You can install and use the official releases of Android Auto, but it must be installed through the GrapheneOS App store. On the other hand, for apps like Waze to be available through Android Auto they must be installed from the official Google Play Store.After installation, you have to open Settings ¬ª Apps ¬ª Sandboxed Google Play ¬ª Android Auto, and at least enable 'Allow permissions for wired Android Auto'. If you can't get it working with this toggle alone, you may need to grant it wireless permissions too. Additional permissions for rerouting audio, phone calls, and notifications to Android Auto can be granted at your own judgment. Note that Android Auto currently does not function from within a private space or work profile.","obtainium--app-verifier#Obtainium & App Verifier":"In Android, the package files downloaded to install or update an application are cryptographically signed. Once an app has been installed, the signer of the installation package is 'pinned' such that all future packages attempting to update it must have a signature of the same signer. This principle is called Trust-On-First-Use (TOFU) and it ensures that future updates of an application cannot come from malicious sources.But this still does not guarantee that the package that you used for the first installation of the application actually came from the source you thought it does. Here, app stores play a useful role in establishing who the real signer of an app should be, basically by pinning the signer through the app store's metadata before even downloading the application's package. On the other hand, an app store adds another third party that you have to trust, and this is where Obtainium can be used as a mitigation.Obtainium allows you to get Android apps, and keep them updated, straight from the source - ie. straight from their GitHub releases page. By combining it with AppVerifier you can still ensure that the package file you're about to install truly comes from the real developer of the application. This makes managing you apps more decentralized without sacrificing an important security feature that app stores offer. Though the GrapheneOS project argues that the most decentralized solution would be replacing Obtainium with self-updating apps.","accrescent#Accrescent":"Accrescent is an app store with a security-first mindset led by a contributor to the GrapheneOS project. It's available on the GrapheneOS App Store, although Accrescent is still in the early stages of development and so far only contains a small selection of applications. The GrapheneOS project intends to delegate securely hosting a wide array of third-party applications, both closed- and open-source, to Accrescent while the operating system's own app repository will only provide first-party applications and possibly a small number of lightly modified and hardened forks of useful third-party apps.While the hope is that Accrescent will be one of the best ways to obtain apps on GrapheneOS going forward, it still lacks funding and contributors to substantially expand. The GrapheneOS project is actively supporting this in a move to replace F-Droid, which they have called out many times for their problematic security stance.Accrescent appropriately secures the initial download and installation through its signed metadata without the need to use AppVerifier or to check the key fingerprint manually, although that's still possible to do after installation if desired.","f-droid#F-Droid":"F-Droid is well-known for being an exclusive repository for open-source Android applications. The GrapheneOS project has expressed very harsh criticism of F-Droid and does not recommend it as a secure source for third-party applications. The main reason is that F-Droid builds most of the apps directly from source on 'sketchy, outdated infrastructure', the resulting packages of which are then cryptographically signed by them, raising concern for a future mass-compromise of F-Droid users.An advantage of app packages being signed by the developer is that it requires the signing key to be compromised for the attacker to be able to create a malicious package with a valid signature. This is arguably harder to achieve than introducing malicious changes to the source code on platforms like GitHub which F-Droid would then use to blindly build a signed package from. The GrapheneOS project argues that this greatly increases the risk of supply chain attacks.Another problem stems from the fact that F-Droid self-signs app builds without ensuring that their build variant has a unique app id different from the one signed by the developers. Because the Trust-On-First-Use principle requires the signers of apps with the same id to match, this often causes confusion with users who try to reinstall an application within another profile or from a different source. Only the few applications that make use of F-Droid's reproducible build feature are excluded from this issue. For apps configured to use reproducible builds, F-Droid will discard their self-build package after verifying that it matches the developer-signed version. But at the moment it's not even possible to easily tell which apps are actually making use of this feature.While F-Droid made an attempt to secure first-time installations through metadata signing, their approach turned out to be rather flawed. There are more issues the GrapheneOS project notes in discussions on F-Droid, though not all of them are technical and therefore omitted here.","play-store--aurora-store#Play Store & Aurora Store":"At the moment, the official Google Play Store app is still the most secure way to install closed-source apps, especially compared to using mirror sites like APKPure which basically have copies of all application packages of the Play Store - commonly used to bypass regional restrictions. To reiterate, Google's Play Store and Services apps are treated like regular apps with no special privileges on GrapheneOS, whether you want to separate them into a secondary user profile is up to you.The Aurora Store is an alternative client to the Google Store's app repository. It allows you to avoid installing the official Play Store app and offers using an anonymous Google account that you share with other Aurora users. The GrapheneOS project recommends against using Aurora due to its security being weaker and some apps being affected by Aurora as the app's source. Note that you can always create a non-identifiable account with a burner phone number in the official Play Store instead of using Aurora's rather unreliable anonymous account feature.There's some criticism here on whether using so much of Google's applications, services and infrastructure doesn't defeat the purpose of using GrapheneOS. To this, the project clarifies that Graphene's purpose is not specifically avoiding Google, but providing a high level of privacy and security - even for those who do not want to make great sacrifices in terms of user experience. The ongoing work to provide a fully functional compatibility layer for Google's services is not a trivial feature but one that GrapheneOS is investing a lot of resources into. Either way, a fresh installation of GrapheneOS is completely de-Googled and whether you want to make use of the compatibility layer or avoid it, is completely up to you.","trouble-and-shooting#Trouble and Shooting":"","geo-location-issues#Geo-Location Issues":"Determining your location is another service originally provided as part of Google's Play Services. Instead, GrapheneOS by default re-routes location requests to the operating system, which exclusively uses the satellite location (GNSS) and therefore requires having satellite reception. Due to this being unavailable or unreliable in situations where the sky is obscured, eg. by the concrete of a ceiling, there have been many complaints about location-based applications not working properly on GrapheneOS.If you have a cellular carrier and Internet connection, the device should be able to use assisted satellite geolocation (A-GNSS) by requesting information on nearby cell towers (SUPL) and on things like the current orbits and status of satellites (PSDS). These make acquiring a lock on your location significantly faster. By default, GrapheneOS uses the project's own proxy servers to prevent associating your SUPL/PSDS requests with your IP address.You can optionally turn on Wi-Fi and Bluetooth scanning under Settings ¬ª Location ¬ª Location services. This will allow apps and services with the 'Nearby devices' permission to scan for nearby Wi-Fi networks and Bluetooth devices even while Wi-Fi and Bluetooth are turned off, potentially improving location-based features. This information would typically be sent to Google's servers by Play Services to determine the location more accurately even inside buildings. The GrapheneOS project is in the process of creating its own implementation of this, likely first as a proxy to Apple's servers and later as its own database.If you have sandboxed Play Services installed and want to use Google's network location service to provide improved location estimates, you first have to disable the 'Reroute location requests to the OS' toggle at Settings ¬ª Apps ¬ª Sandboxed Google Play. Next, you'll have to change the Location permission of Play services to be set to 'Allow all the time' as well as 'Use precise location'. For it to be able to make use of network scanning, you also need to grant it the 'Nearby devices' permission (the above-mentioned toggles for Wi-Fi and Bluetooth scanning must already be enabled). Finally, you need to once again go to Settings ¬ª Apps ¬ª Sandboxed Google Play and tap on 'Google Location Accuracy' and enable the 'Improve Location Accuracy' toggle.The re-routing toggle is not a global option, so you could consider setting up a secondary user profile for the sole purpose of making use of Google's privacy invasive location services for when you're having geo-location troubles.","crashingbroken-apps#Crashing/Broken Apps":"We've already mentioned that it's not surprising for applications on GrapheneOS to crash or otherwise refuse to work properly, but this is nearly always fixable. Before anything, you should first attempt standard steps such as clearing app cache, forcefully stopping and restarting the application, restarting the phone, or re-installing the application. Typical GrapheneOS-specific solutions include turning off some of the exploit protection measures, re-installing the application outside of a private space (Owner or other secondary profile), re-installing the app in a profile that provides sandboxed Google Play Services, etc. Sometimes apps even start crashing because the application (eg. app store) from which they were installed from is no longer enabled or installed, or they refuse to work properly because they were not installed from the source that they expected.","app-refuses-to-install#App refuses to install":"Typical reasons for this have also been explained above: You're attempting to install an application that is already available with a higher version or from a different source in another user profile.","getting-support#Getting Support":"GrapheneOS has a very active community and is responsive on social media. The easiest places to look for existing solutions for your problem are probably the forum at discuss.grapheneos.org and their Discord server. GrapheneOS also has communities on Twitter/X, Telegram, and Matrix.","appendix#Appendix":"","references#References":"Serasker (@Seraskerx), 2024, November 18. \"The location services on my Android 15 Pixel 6a device are down. The apps can't find my location. Google maps can't locate my location on the vanadium scanner.\", https://x.com/Seraskerx/status/1858540251154669582\nGrapheneOS (@GrapheneOS), 2024, November 18. \"OS location service on GrapheneOS is based on satellite location (GNSS) so you need GNSS satellite reception which you may not have indoors. A-GNSS services are used to assist with obtaining a faster lock (PSDS and SUPL). None of this is down or we'd have an announcement. We have added configuration for choosing the PSDS and SUPL provider so if they were down, which they aren't, you could still use them. Location works fine without these services. It works without internet access. These just make it get a location faster than pure satellite. You're likely used to network-based location sending nearby cell towers, Wi-Fi networks and Bluetooth beacons to an Apple or Google service and getting back an estimate for working indoor location. You can use Google's network location service on GrapheneOS if you want. We'll be providing our own network location in the very near future so you won't need to set up using sandboxed Google Play if you want network location. If you do need network location right now rather than weeks from now, you can set that up: https://grapheneos.org/usage#sandboxed-google-play-configuration\", https://x.com/GrapheneOS/status/1858556085247840573\nManjili (@Manjili8), 2024, November 18. \"When I don't use location for long, I have to go under open sky area for my location to be detectable. Same phone and same OS.\", https://x.com/Manjili8/status/1858542523196862700\nGrapheneOS (@GrapheneOS), 2024, November 18. \"That's just how satellite-based location (GNSS) works in practice. It won't work well in a concrete building. If you have cellular and haven't disabled SUPL, it should go quite fast with minimal satellite reception. Wood frame house typically won't break it but concrete will.\", https://x.com/GrapheneOS/status/1858556475867558224\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, November 18. \"Location Services on GrapheneOS are never down. The only service available by default is receive only GNSS/GPS which require line of sight to the sky and may or may not based on your config make use of AGPS almanacs to get a faster lock which on first/refresh lock can take 5mins+. We're in the process of developing (in fact framework is pretty much already there) our own provision of a secure/private network location capability for faster/accurate lock in non ideal GPS situations. Yet if you can't wait and don't mind using it, Googles NLP provision works. To use Googles NLP install sandboxed Play Services and follow: https://grapheneos.org/usage#sandboxed-google-play-configuration\", https://x.com/Metr0pl3x/status/1858546243628081517\nGrapheneOS (@GrapheneOS), 2024, November 11. \"The main downside of a Private Space is that the clipboard isn't shared. It also doesn't yet have a full equivalent to end session for secondary users which purges encryption keys, but a reboot is always better anyways.\", https://x.com/GrapheneOS/status/1856026013051895907\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, November 11. \"Enable notification forwarding via SETTINGS>SYSTEM>USERS>SEND NOTIFICATIONS TO CURRENT USER. Just make sure to just switch to Owner and not End Session on your InTune user. However don't forget we fully support Private Space with stock launcher if you just want to stay in Owner.\", https://x.com/Metr0pl3x/status/1856022185082794301\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, November 14. \"Are these apps that use and require sandboxed Play Services? Are Play Services in the Private Space? What if any per app exploit protection compatibility options have been tested? Toggled On? Toggled off? Native Code Debugging enabled/disabled and the dynamic code ones.\", https://x.com/Anknownguy/status/1857051107618357587\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, November 8. \"In a word NO. In more than a word, updates can be manually verified, updates are signed, update client verifies authenticity/integrity, then verified boot+downgrade protection, which would catch any malicious update if installed somehow eg exploiting update client. Our signing keys are also not available to update servers. Therefore if the update servers were compromised, the attacker could not push out a malicious update either. No pressure can be applied to GrapheneOS in regards to features etc either. Most existing legislation doesn't specifically target the OS or it's features it just covers how and in what circumstances the OS and it's features are used and targets the individual.\", https://x.com/Metr0pl3x/status/1854874274823057802\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, November 7. \"You can archive apps you just need to install sandboxed Play Services. You can also Disable apps via App Info without it.\", https://x.com/Metr0pl3x/status/1854652559920603158\nNeocraft1293 (@neocraft1293), 2024, October 17. \"No, you have to install it via the GrapheneOS application\", https://x.com/neocraft1293/status/1847030101437014306\nGrapheneOS (@GrapheneOS), 2024, October 20. \"This may be caused by you making a change like disabling dynamic code loading from storage. Otherwise, it should be working fine. Google Play is gradually moving away from dependence on this but for now a lot of the libraries still require it. They're going pretty slowly though. It's the reason why we didn't provide a global toggle for disabling it yet since it will create more support workload for us.\", https://x.com/GrapheneOS/status/1847961871241908422\nPavan (@Anknownguy), 2024, October 22. \"Is anybody able to run banking apps or UPI apps in private space?\", https://x.com/Anknownguy/status/1848786497740411358\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, September 28. \"It comes from within the GrapheneOS community, from a contributor, Accrescent publishes developer signed releases & also has signed metadata to secure initial download/install. It‚Äôs an app store being with a security first mindset read more: https://accrescent.app/docs/guide/appendix/requirements.html. While Obtainium helps mitigate adding another trusted third party & ones with security issues (F-Droid) it doesn't provide the security feature set below. https://accrescent.app/features. It's why we recommend sandboxed Play Store, Accrescent and then Obtainium as necessary/desired.\", https://x.com/Metr0pl3x/status/1840178034881487096\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, October 2. \"As mentioned GMScompatconfig governs the highest version number that you can update to so either works.\", https://x.com/Metr0pl3x/status/1841478984682340596\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, September 20. \"Updates are signed, update client verifies authenticity/integrity of updates, & there‚Äôs verified boot when correctly installed by locking bootloader. The update client verifies the signature of each update and verifies that the version is equal or greater than the current one. The entirety of the OS is signed and verified via verified boot with downgrade protection. Even if a tampered update was somehow installed such as by exploiting the update client, verified boot would prevent it booting. Our signing keys are not available to servers. Should the update servers be compromised, an attacker could NOT push out a malicious update. Especially not to a specific device as we DO NOT know who any of our users are. Read more here: https://grapheneos.org/install/web#verifying-installation\", https://x.com/Metr0pl3x/status/1837081705653354776\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, September 19. \"Netflix did the same previously, while not recommended as the primary app source for obvious reasons, Aurora Store might help here. Install it and then allow it to control opening Play Links by default via App Info then click this to enable download.\", https://x.com/Metr0pl3x/status/1836880879236301145\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, September 2. \"This would imply @CashApp have chosen to implement strict Play Integrity API level and thrown their lot in with Googles monopolistic anti competitive approach to locking people into GMS Android with security theatre. You could try reaching out to whitelist GrapheneOS. Read more and provide them: https://grapheneos.org/articles/attestation-compatibility-guide\", https://x.com/Metr0pl3x/status/1830626664012730824\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, August 22. \"Advise using the Play Store, not always guaranteed to get the correct app and apps can see what is used to install it which can affect some apps. The app installed with the Play Store and alongside sandboxed Play Services works so isn't an OS version but app store/app issue. The eBay app also works without disabling MTE for third party apps nor needing to disable any of GrapheneOS' hardening and protections. Yes creating a non identifiable account to use it is no different than using Aurora Store and their anonymous accounts yet benefiting from the security features of Play Store than you lose using Aurora.\", https://x.com/Metr0pl3x/status/1826665123492024810\nGrapheneOS (@GrapheneOS), 2024, June 18. \"Aurora Store is not a better approach to using Play Store. It would be nice if the video was edited to fix the inaccuracies. We're open to helping with making a video about GrapheneOS which covers it properly. We don't include F-Droid and Aurora Store in GrapheneOS because they're very problematic and both have major security flaws. Our approach is not bundling a bunch of existing problematic software and giving it privileged access it wouldn't need if it was written properly. It would help promote GrapheneOS, but that's not our way. People can use either of those, but they aren't part of what we recommend.\", https://x.com/GrapheneOS/status/1803171736591958417\nGrapheneOS (@GrapheneOS), 2024, June 18. \"Being able to install apps and the apps working is the main user experience that's relevant. Aurora Store can't do Play Asset Delivery, Play Feature Delivery, in-app payments or other features provided through the Play Store app. It all works with the sandboxed Play Store app. By default, Aurora Store relies on fetching shared Google account credentials which due to being explicitly against the terms of use is likely to break in the future. It already often breaks when the accounts get rate limited, but it's likely to nearly completely stop working.\", https://x.com/GrapheneOS/status/1803234834749247836\nGrapheneOS (@GrapheneOS), 2024, June 18. \"For getting apps from the Play Store, it's better to use the sandboxed Play Store with a purpose-specific account instead of Aurora Store. Aurora Store doesn't verify signatures proving apps came from the Play Store and trusts every Certificate Authority for HTTPS connections. There are a lot of choices for getting apps from outside the Play Store. We recommend https://accrescent.app for the small number of apps available in it including Molly. Accrescent needs more contributors and funding to substantially expand. We're going to try to support that. We don't recommend manually downloading app releases from GitHub, etc. mainly because you won't have automatic updates. You can solve that issue with the Obtainium app. However, unlike a proper app store, it won't secure the initial download beyond the HTTPS connection security. F-Droid has far too many security and trust issues for us to recommend it. The vast majority of apps in the official F-Droid repository are built on their sketchy infrastructure and signed with their own keys. We're concerned about a future mass compromise of F-Droid users. People who work on F-Droid have demonstrated a lack of trustworthiness including engaging in harassment towards security researchers and covering up vulnerabilities/weaknesses in the app. Lead developer has repeatedly claimed app sandboxes aren't useful or a good approach... Major app/server and build infrastructure security improvements along with anti-security and untrustworthy people leaving the project would be a prerequisite to us considering even packaging F-Droid in our app repository. That's very unlikely, so we want Accrescent to replace it.\", https://x.com/GrapheneOS/status/1803185056086659418\nGrapheneOS (@GrapheneOS), 2024, August 14. \"No, there's no way to recover any of the data after the tiny window where it wipes what's required to decrypt it before it shuts down. Since it was wiped, there's no valid encryption or filesystem and it gets detected as corrupt on next boot, and then next boot it asks to format. It's explained at https://grapheneos.org/features#duress.\", https://x.com/GrapheneOS/status/1823781799656611858\nGrapheneOS (@GrapheneOS), 2024, June 2. \"Owner user configures the duress PIN/password but it's global and works from secondary users. Try using the duress PIN/password you've configured in Owner from a secondary user. It will wipe the device in the same way. It has multiple user support just not in the described way. You probably don't want one which only works when you're trying to log into a specific user and only deletes that user. You may want a form of it which only wipes specific secondary users, but that's different than having one that's specific to logging into a particular user. It's also worth noting that the duress PIN/password can be entered anywhere the PIN/password can be entered. If you enter it on PIN/password request for enabling developer options or changing certain settings, it's going to wipe the device in the same way as unlocking the device. We integrated it right after the check for the correct PIN/password in a universal way. We could extend it to working for SIM PIN entry too but we didn't do that in the initial implementation. It'd likely make sense to do it for SIM PIN entry but just wasn't in the initial phase. If for some strange reason you set the duress PIN/password and the real PIN/password to the same value it won't wipe the device since it checks for the real one first. We thought a lot about how to do it well and even behavior of having corrupt data you can't decrypt is intended. It wipes the hardware keystore keys which among other things prevents deriving any of the key encryption keys for encryption. Booting will initially fail because it can't derive key encryption keys. After trying again, it will give up and show the prompt to wipe and reformat. We plan to enhance it by also wiping a couple other things which also prevent deriving the key encryption keys. We don't want to actually reformat since that takes time and it's better to just shut down cleanly as soon as possible which wipes most memory via zero-on-free.\", https://x.com/GrapheneOS/status/1797325762036396169\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, July 27. \"The second option allows for complete isolation, preventing it seeing other apps installed alongside it and using IPC if kept on it's own. You can also forward notifications to the owner profile for alerts. Downsides are switching to the user when calls come in adds a delay. IPC is inter process communication, apps talking to each other with consent as determined by the app not the user. Example is the share dialogue. As far as 'safe' that depends on how you're defining the term and its antonym. It would receive all the benefits outlined in our docs. Also can be determined by if you want to use sandboxed play services in your owner profile or not. If not you won't get prompt push notifications as Whatsapp uses FCM (Firebase Cloud Messaging) so if you don't, using both SPS and Whatsapp in a secondary user has that benefit too.\", https://x.com/Metr0pl3x/status/1817163192029663511\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, July 18. \"Update it. It's why we don't recommend it over sandboxed Play Services and the Play Store.\", https://x.com/Metr0pl3x/status/1813985508298760472\nGrapheneOS (@GrapheneOS), 2023, October 22. \"Physical SIMs work fine out of the box. eSIMs currently require installing sandboxed Google Play from our app repository. We're going to be adding a non-Google-dependent eSIM activation system in the near future but physical SIMs still work fine so it's not max priority. It's trivial to install sandboxed Google Play from the app repository. Activating an eSIM is the same process as the stock OS after that. It's not any harder. The non-Google eSIM activation system is just not ready for inclusion yet. It doesn't meet our standards quite yet. [outdated information]\", https://x.com/GrapheneOS/status/1716254417668694195\nGrapheneOS (@GrapheneOS), 2023, October 23. \"GrapheneOS comes with our app repository and has support for sandboxed Google Play. Aurora Store is simply another frontend to the Play Store and doesn't fully work compared to sandboxed Google Play. Either way, you're installed packages from the Play Store and are trusting it. Aurora Store installs the wrong variant of apps by default due to not searching or fetching apps based on device model (anonymous mode). It logs into shared Google accounts by default which is problematic and gradually breaking, although you can make your own throwaway account. Many apps from the Play Store require the Play Store for Play Asset Delivery, Play Feature Delivery, app/content license checks, in-app payments and other Play Store features. This all works with the sandboxed Play Store. More and more Play Store apps depend on these things. Aurora Store doesn't verify Play Store signature metadata and doesn't use a reduced CA set or pinning like the Play Store itself. That means the downloads of apps with Aurora Store are only secured by HTTPS with every WebPKI CA trusted, which isn't very good. You're installing and running apps downloaded from the Play Store with Aurora Store. It's not a different store. That means you're still trusting the Play Store to give you the apps. Many apps contain Google Play libraries so you're running it as part of many of the apps anyway.\", https://x.com/GrapheneOS/status/1716516119081603307\nGrapheneOS (@GrapheneOS), 2023, October 23. \"You lose certain features requiring privileged OS integration like Android Auto [outdated]. Google Pay NFC payments aren't available since they require a Google certified OS but it would work if they allowed it. There are working NFC payment services (ones allowing an alternate OS).\", https://x.com/GrapheneOS/status/1716519326847553759\nGrapheneOS (@GrapheneOS), 2024, July 20. \"If you want to make Play Store purchases, you'll end up needing a long term, cross-device account if you want to use those purchases across devices. If you don't need to use purchases you can make a separate account for each install of Google Play including in separate profiles.\", https://x.com/GrapheneOS/status/1814703100122304977\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, June 15. \"Factory reset will not remove an eSIM. So pretty much like a physical SIM in the device someone with access will be able to use the service until you advise your carrier and have the SIM deactivated and transferred to a replacement.\", https://x.com/Metr0pl3x/status/1801998999362077114\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, May 28. \"Sensors - Default Off. Memory Tagging - Default On for Third Party Apps. Native Code Debugging - Default Off. Off the top of my head as somewhere to start.\", https://x.com/Metr0pl3x/status/1795517359089946721\ntuxsudo (@tuxpizza), 2024, May 25. \"I would try using Molly instead, which is a 3rd party Signal app that improves notification performance when not using play services. Personally I don't have an issue with Signal battery usage however.\", https://x.com/tuxpizza/status/1794540766066598352\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, May 26. \"Ditto, percentages under battery info is relative, however I use Molly as the more efficient notifications are just the icing on the cake for their other additions.\", https://x.com/Metr0pl3x/status/1794673310208610787\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, May 7. \"Find My Device might be on the settings listing for Security but it doesn't show when tapped through as the Find My Device app is NOT installed alongside Play Services. As Play Services isn't a system app some FMD functions may not work however for location functionality etc it should work in any user if active with Play Services and the appropriate permissions & Location services config, if desired, once installed from the Play Store.\", https://x.com/Metr0pl3x/status/1788009654192123989\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, April 24. \"If you have it installed in another user and it's updated to latest then trying to install from their website will be using the older version. If the original user that installed it is owner then use the multiple user settings UI to install it in your new/chosen user. See: https://privsec.dev/posts/android/f-droid-security-issues/#5-confusing-ux\", https://x.com/Metr0pl3x/status/1783269571513794995\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, March 1. \"Where was Waze installed from? I ask as I believe Android Auto limits apps to those installed from the Play Store, other apps would require changing a setting in AA under Dev Mode.\", https://x.com/Metr0pl3x/status/1763720610654380541\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, February 11. \"Yes this is a change made recently read the release notes below:\nhttps://grapheneos.org/releases#2024012600\", https://x.com/Metr0pl3x/status/1756838544457281829\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, February 7. \"It appears you're right that Google Play Services is now a prerequisite for Google Maps to work.\", https://x.com/Metr0pl3x/status/1755373735702417750\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, January 30. \"Open Dialer. Dial ##4636##. Tap Phone Information. Check the greyed out toggle for VoLTE. Let me know if it is in the on or off position.\", https://x.com/Metr0pl3x/status/1752377306595709026\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, January 17. \"Don't use either, AA should be installed from our Apps app only. It is why you see gmscompat as a dependency. The same thing applies to prevent Play Services updating itself to a version not supported by the compatibility layer too.\", https://x.com/Metr0pl3x/status/1747723879785656617\nMetropleX | GrapheneOS (@Metr0pl3x), 2024, January 15. \"Do not use Aurora to update this, use Auroras built in blacklist. This is updated within the OS. Pixel camera services is a small component to let apps use part of the camera HAL, this enables Night Mode in GrapheneOS Secure Camera. See release notes below: https://grapheneos.org/releases#2024011300\", https://x.com/Metr0pl3x/status/1747031361993502855\nMetropleX | GrapheneOS (@Metr0pl3x), 2023, December 7. \"Go to App Info for the system camera and then 'Open by Default' then ensure Open Supported Links is enabled. If it is you'll see the 'Clear Default Preferences' button. Tap it and then do the shortcut again.\", https://x.com/Metr0pl3x/status/1732904883106296148\nGrapheneOS (@GrapheneOS), 2024, November 21. \"The auto-reboot feature is based on a timer after the device is locked. It's cancelled by you unlocking the phone successfully. As long as you regularly use the phone, it won't reboot with the default settings. Default is 18 hours since phone was locked without successful unlock. If you lowered it to a value that's too low for you and ends up rebooting your device, you can raise it back to 18 hours. We strongly recommend leaving the feature enabled. iOS 18.1 recently introduced a similar feature and it's hard-wired to 72 hours which is our maximum option.\", https://x.com/GrapheneOS/status/1859640403101876353\nGrapheneOS (@GrapheneOS), 2024, November 21. \"Our 2-factor fingerprint unlock feature will be launching soon. Our recommended unlock setup will be a strong diceware passphrase for the main unlock method and fingerprint + random 4-6 digit PIN for secondary unlock (it's a special case where 4 digits is fine due to 5 attempts).\", https://x.com/GrapheneOS/status/1859642366375887230\nGrapheneOS (@GrapheneOS), 2024, November 21. \"We recommend using the built-in backup app, but change it to device-to-device mode in the menu in order to have it backup much more app data than it does in the default mode. Don't wipe the previous device until you've made sure everything you need is transferred. In device-to-device mode, it will transfer the vast majority of app data. You can optionally use it to transfer your home directory too, which is a good way to do it. Bear in mind it's per-profile and you need to backup/restore each user, etc. separately. In rare cases, certain apps aren't compatible with any form of backups even in device-to-device mode. Signal is the most common example because they encrypt the database with a hardware keystore key (one that's available even before first unlock so it accomplishes very little). For Signal or the hardened Molly fork of it, you need to use their backup/restore instead. You can use it as an opportunity to switch to Molly if you want, although note sometimes it's a version behind and the backups can only be restored on an equal/greater version.\", https://x.com/GrapheneOS/status/1859640828353757593\nGrapheneOS (@GrapheneOS), 2024, November 20. \"GrapheneOS is not a modification of the stock Pixel OS and the term custom ROM is not correct. It's a Linux distribution based on the Android Open Source Project. It receives over-the-air updates from GrapheneOS. All default connections are listed at https://grapheneos.org/faq#default-connections.\", https://x.com/GrapheneOS/status/1859443591740891415\nGrapheneOS (@GrapheneOS), 2024, November 20. \"Cellebrite Premium has a capability called IPR for iOS enabling it to extract or brute force the lock method based on data left around in memory after unlocking. It's not clear if this has been fixed on newer iOS versions because Cellebrite stopped listing where it was supported. MSAB (XRY) and Magnet Forensics (Graykey) were using a similar approach with Android but their attack vector was closed on Pixels in April 2024 via reset attack mitigation. It's not clear if Android still leaves around a hash or something else in RAM so we added more zeroing. For Pixels, we have details on how XRY was exploiting it and know they were booting into a firmware mode and exploiting that to get OS memory. Android properly clears the PIN/password, etc. after unlock but it left around a hash as part of internal work they could brute force.\", https://x.com/GrapheneOS/status/1859400303658869203\nGrapheneOS (@GrapheneOS), 2024, November 20. \"Users have varying tolerance for the device going back to BFU mode after a certain amount of time. That's why the auto-reboot timer we added in June 2021 is configuration. We moved to a 72 hour default and then 18 hours but people can set it as low as 10 minutes if they want it. It's possible to try to keep more data at rest in AFU but it heavily depends on apps. Even Signal has shown zero interest in keeping most of their database at rest while locked. There's a fork of it on Android called Molly which does implement keeping the database at rest. iOS makes it a bit easier than Android to keep data at rest while locked than Android, but yet there are better options for apps taking advantage of it on Android including 2FA apps and other things. It's not a lot of apps taking advantage of it and not super mainstream ones. Another example is that Android offers the Chromium layer-1 sandbox to every app as an isolatedProcess toggle for out-of-process services. It's easy to use. It's essentially not used in anything other than the OS and Chromium-based browsers. App developers do not do hardening... People might reasonably think Signal, etc. try to harden a lot against getting exploited via media, WebRTC, etc. and they'd be wrong. WebRTC in Signal is based on the Chromium implementation but doesn't sandbox it like Chromium does. At least they've gotten better at updating it. Trying to get app developers to opt-in to keeping data at rest when locked is never going to work out. Even if they'd done things the other way around and developers had to opt-in to not keeping it at rest when locked, most apps would probably just opt-in for all their data. The OS probably just needs to offer a per-app toggle for apps getting frozen while locked with all their data put at rest, which still won't benefit most users but at least people who are at risk can be told to use it and can enabled the feature for Signal, etc.\", https://x.com/GrapheneOS/status/1859406214746042771\nGrapheneOS (@GrapheneOS), 2024, November 20. \"GrapheneOS is a Linux distribution too. Linux doesn't mean having to use systemd, glibc, GNU coreutils, GNOME, etc. This is clearly Linux: https://github.com/GrapheneOS/kernel_common-6.6 https://github.com/GrapheneOS/kernel_common-6.1 https://github.com/GrapheneOS/kernel_common-5.15 https://github.com/GrapheneOS/kernel_common-5.10 It's not a different OS kernel, it's Linux. Linux kernel's monolithic architecture is bad for security as is it nearly entirely being written in C. It's not anywhere close to ideal for security. It would ideally be based on a microkernel written in a memory safe language. We can use hardware virtualization to improve this.\", https://x.com/GrapheneOS/status/1859317320985756020\nGrapheneOS (@GrapheneOS), 2024, November 20. \"Ubuntu Touch is quite unlike GrapheneOS. It doesn't have the basics of proper privacy/security patches or basic privacy/security features. It's missing a proper application security model, sandboxing throughout the OS, modern exploit protections, broad use of memory safety, etc. GrapheneOS is a hardened OS providing a very high level of privacy and security. That requires protecting against remote exploits, local exploits from apps and forensic data extraction. Ubuntu Touch is incredibly weak to all of that without basic industry standard defenses.\", https://x.com/GrapheneOS/status/1859316624106422556\nGrapheneOS (@GrapheneOS), 2024, November 20. \"You do not need to use multiple profiles to use GrapheneOS. You can use it the same way as you use the stock OS. Don't listen to people overcomplicating things and trying to convince you that their way of using it is the only way to use it. It's easy to install and easy to use. You can install Google Play in your main user. You don't need to use additional profiles. Using multiple profiles is NOT required to avoid Google Play having any special access. They're regular apps on GrapheneOS (sandboxed Google Play) without needing to use multiple profiles. Putting it into a Private Space is something you can optionally do if you WANT to keep it more separate from other apps. It is not required. It is not needed to avoid them having any special access. On GrapheneOS, they're regular apps with zero special access if you install them.\", https://x.com/GrapheneOS/status/1859314961354539074\nGrapheneOS (@GrapheneOS), 2024, November 20. \"You can use the Play Store as a regular app with no special access. It does not receive any special access or control. You do not need to grant any standard permissions beyond the ability to install apps. You do not need to overthink it and use it in a strange way at all. GrapheneOS includes our own App Store for obtaining basic things including if you want to install sandboxed Google Play. There's no special process to install it, you just press install in there and approve the install as you would do with any app. There's nothing else to it. You can use other Android app stores. We have Accrescent in our app repository as another option which will have a lot more apps in the future. There are a lot of other Android app store options. Play Store is still one of the best choices until other options improve though. Apps are sandboxed and can't access each other's data or your data. Apps within the same profile can choose to talk to each other. If you want to split things into separate profiles, you can do that, but Google Play are regular sandboxed apps if you install them regardless. There is no specific reason you need to use a separate profile to use Google Play on GrapheneOS. They are regular apps like any others. They don't work any differently in regards to profiles, etc. It's the same as other apps, that's the point of our sandboxed Google Play feature.\", https://x.com/GrapheneOS/status/1859320952628154694\nGrapheneOS (@GrapheneOS), 2024, November 19. \"Cellebrite Premium is clearly exploiting the stock Pixel OS via USB rather than using this approach. Therefore, Cellebrite didn't lose any capabilities because of the improvement. Our exploit protections have been successfully blocking them even before major improvements in 2024. [...] Cellebrite's approach involves attacking the OS itself so all of our generic memory corruption exploit protections and other defenses are there to stop it. We also nearly fully wiped out the USB attack vector for the OS with our 2024 overhaul of our USB attack surface reduction. By default, GrapheneOS disables new USB-C connections as soon as the device is locked at both a hardware and software level. It then fully disables USB-C data at a hardware level once existing connections end or right away if there weren't any. They'd need another attack vector. For example, they could still target GrapheneOS via Wi-Fi, Bluetooth or cellular. However, getting into the device from any of those will still be much harder than with the stock OS, and it's more complex than USB in general. There's a reason they have always preferred USB.\", https://x.com/GrapheneOS/status/1858914791458316652\nGrapheneOS (@GrapheneOS), 2024, November 19. \"Since 2021, we've had an auto-reboot timer feature which reboots the device after it's locked if it isn't unlocked before the timer expires. iOS recently added this with a hard-wired 72 hour timer. Our default is 18 hours but users can configure it from 10 minutes to 72 hours. If you need max protection, using the 10 minute auto-reboot would be ideal. There's also the option to fully disabling USB-C while OS is booted by also disabling charging including USB-PD. Can also enable turning off Wi-Fi and Bluetooth via timers, which we plan to extend to NFC.\", https://x.com/GrapheneOS/status/1858918113162432806\nGrapheneOS (@GrapheneOS), 2024, November 19. \"GrapheneOS has an LTE-only mode as an additional cellular mode for attack surface reduction. Android also has a standard 2G disable toggle which we make available for all carriers instead of only certain ones. We recommend using our LTE-only mode for attack surface reduction. We strongly recommend 8th/9th gen Pixels for greatly improved security on GrapheneOS via hardware memory tagging. It's enabled for the base OS including apps by default and opt-in for user installed apps, which we recommend, and then opt-out per app for apps with bugs it catches. If you want max security, get any of the 4 currently available 9th gen Pixels to use GrapheneOS. They have more cellular radio hardening and GrapheneOS-specific kernel hardening implemented right now, but 8th gen is likely going to upgrade to the same 6.1 kernel branch soon.\", https://x.com/GrapheneOS/status/1858935593327358220\nGrapheneOS (@GrapheneOS), 2024, November 19. \"More info available at https://grapheneos.org/features, https://grapheneos.org/faq#recommended-devices and our security standards for new hardware are at https://grapheneos.org/faq#future-devices. 9th gen Pixels provide all of that. 8th gen are still temporarily on the 5.15 Linux kernel LTS branch but will move to 6.1. 6th/7th gen Pixels may move to the 6.1 Linux kernel LTS branch too but are still on 5.10 at the moment. This mostly matters in terms of long term support. Newer kernel versions have more attack surface but also better security features. Older branches aren't necessarily worse. However, older branches gradually get fewer patches backported and eventually have support dropped. Being on 6.6 instead of 6.1 isn't clearly more secure due to more attack surface, but being on 5.15 or 5.10 is starting to be clearly worse and they should be moved to 6.1 soon. 6th/7th gen Pixels provide nearly all those requirements. They're ARMv8.2 instead of ARMv9 though so they don't have PAC, BTI or most important MTE for hardware memory tagging. MTE is near exclusive to Pixels and ARMv9 does NOT imply having MTE support at all. It's optional.\", https://x.com/GrapheneOS/status/1858941217842889093\nGrapheneOS (@GrapheneOS), 2024, November 17. \"iOS is the next best option after GrapheneOS for privacy and security on a smartphone or tablet. It has some advantages and disadvantages, but GrapheneOS is better overall. Other alternative Android-based operating systems reduce security substantially rather than improving it. You can use the sandboxed Google Play compatibility layer for near perfect app compatibility if you want to use it on your main device. Private Space is now a nice way to keep that separate without a major usability loss if you want to avoid having it in your main user profile. The best source of info to understand what we provide is https://grapheneos.org/features which covers most of what we improve to standard Android 15. https://eylenburg.github.io/android_comparison.htm is a basic third party comparison with a privacy focus. Generally high quality and they're slowly expanding it.\", https://x.com/GrapheneOS/status/1858373098715730007\nGrapheneOS (@GrapheneOS), 2024, November 14. \"You need one of the devices officially supported by GrapheneOS. These are the recommended ones: https://grapheneos.org/faq#recommended-devices Pixels are currently the only devices meeting our security requirements. Most Android devices have awful security and also very poor alternate OS support. They're the only Android hardware with competitive security with iPhones. They also have the best alternate OS support and most relatively acceptable devices are completely ruled out from their lack of any serious alternate OS support such as all Samsung phones/tablets. Our hardware requirements are listed here: https://grapheneos.org/faq#future-devices As the main alternative, Samsung devices are missing a couple of the security features, don't get all the updates right away and most importantly cripple the device including security if you use another OS. Non-Pixel, non-Galaxy devices focus on security much less and are missing more of the security features we need. Samsung isn't far from providing what we need if they didn't have such awful alternate OS support disabling a bunch of the security features we need and other issues.\", https://x.com/GrapheneOS/status/1857063255811711412\nGrapheneOS (@GrapheneOS), 2024, November 17. \"Location works well on GrapheneOS. You were trying to use apps using the Google Play location service without having Google Play installed. For example, Organic Maps from the Play Store uses the Google Play location service by default but it can be toggled off to use the OS. When sandboxed Google Play is installed, GrapheneOS redirects apps from using the Google Play location service to using the OS location service instead. If you didn't disable the rerouting and grant permissions to Google Play services, you're only using the OS location service. You can try using Organic Maps with Google Play location disabled and GPSTest in a profile without sandboxed Google Play and you'll see it works fine when there's satellite reception. Google Play is not required for location on GrapheneOS and installing it doesn't improve it. It's possible to enable the Google Play network location service, grant the required permissions and disable rerouting location requests to the OS in order to use it. However, it's not active from installing it. You aren't improving location detection by installing Google Play.\", https://x.com/GrapheneOS/status/1858230623959450039\nGrapheneOS (@GrapheneOS), 2024, November 17. \"Are you testing outdoors where you have satellite reception? Satellite-based location can't be expected to work indoors in concrete buildings. It tends to work in common wood frame / drywall / vinyl siding US homes but it's not usually going to work in an apartment building, etc. It seems that you're testing indoors and expecting to get satellite-based location there. That is not how you're getting location detection indoors with iOS or typical Android with Google Play. They're sending nearby Wi-Fi networks, cell towers and Bluetooth beacons to a service. GrapheneOS is in the process of making our own network-based location client with multiple options, which will initially be either our proxy to Apple's service or directly using Apple's service. We're also going to be making our own database mainly based on scraped data too.\", https://x.com/GrapheneOS/status/1858242058034774239\nGrapheneOS (@GrapheneOS), 2024, November 13. \"End-of-life Pixels shouldn't be used anymore regardless of the OS choice. GrapheneOS provides extended support past end-of-life for devices which didn't have a 5 or 7 year minimum support guarantee from launch. This is only for harm reduction and we don't recommend using it. See https://grapheneos.org/faq#recommended-devices for the list of recommended devices and https://grapheneos.org/faq#future-devices for the list of hardware requirements determining what we can support. We strongly recommend 8th/9th gen Pixels due to the hardware memory tagging feature we use and 7 years of support. A Pixel 8a is the ideal budget device right now since it has the same security and nearly the same performance as the other 8th gen Pixels. Even has the same 8GB of memory as the Pixel 8. Pixel 8a is still a quite new device (May 2024) though so there aren't a lot of deals yet.\", https://x.com/GrapheneOS/status/1856811240187994434\nGrapheneOS (@GrapheneOS), 2024, November 14. \"Many Android apps depend on Google Play, so you need sandboxed Google Play for them. Some apps have memory corruption bugs during regular use which get caught by GrapheneOS features and we provide a per-app exploit protection compatibility mode toggle to use those on GrapheneOS. The UI is nearly identical to the stock Pixel OS. A lot of the bundled apps are the sample AOSP apps and we expect users to replace them, we just don't want to bundle a bunch of third party apps and services. Battery life is a lot better than the stock OS, i.e. quite good. GrapheneOS with no sandboxed Google Play or other battery hungry apps has much better battery life. If you use a single install of sandboxed Google Play (i.e. one profile with it), it's still a lot better. It's only worse if you do something like using Signal's non-FCM push. Not anything we can do about Signal having such inefficient non-FCM push. Molly is a fork of Signal making it much better. We strongly recommend using Molly if you want to use Signal in a profile without sandboxed Google Play. Battery life is based on your apps/configuration.\", https://x.com/GrapheneOS/status/1857054898065473628\nGrapheneOS (@GrapheneOS), 2024, November 14. \"Samsung doesn't provide proper alternate OS support so we can't support their devices. They cripple the device if you use another OS including disabling important security features we need. They're also missing a couple security features we need but that's not the main issue. Our hardware requirements are listed here: https://grapheneos.org/faq#future-devices We need secure hardware and firmware with proper updates. It also needs to provide the security features we use to defend users against attacks. Hardware memory tagging is the most important vs. remote attacks. Hardware memory tagging is not implemented by Snapdragon yet. Samsung Exynos and MediaTek Dimensity supposedly support it in some form but the actual devices using them don't really support it. Other than that, it's mainly OEMs skipping security features or not doing updates.\", https://x.com/GrapheneOS/status/1857060091343540317\nGrapheneOS (@GrapheneOS), 2024, November 14. \"Our hardware requirements are listed here: https://grapheneos.org/faq#future-devices As the main alternative, Samsung devices are missing a couple of the security features, don't get all the updates right away and most importantly cripple the device including security if you use another OS. Non-Pixel, non-Galaxy devices focus on security much less and are missing more of the security features we need. Samsung isn't far from providing what we need if they didn't have such awful alternate OS support disabling a bunch of the security features we need and other issues.\", https://x.com/GrapheneOS/status/1857054297663381606\nGrapheneOS (@GrapheneOS), 2024, November 14. \"The battery life is good, especially with GrapheneOS. Pixel 9 improved it quite a lot over the Pixel 8 due to the more efficient cellular radio.\", https://x.com/GrapheneOS/status/1857053963461317113\nGrapheneOS (@GrapheneOS), 2024, November 22. \"Private Space is a standard Android 15 feature rather than a GrapheneOS feature. They were listing the features we add (see https://grapheneos.org/features) rather than standard ones. We do plan improvements to Private Space including eventually being able to have more than one at once.\", https://x.com/GrapheneOS/status/1859955930915570100\nGrapheneOS (@GrapheneOS), 2024, November 22. \"NFC payments already work on GrapheneOS. The issue you're having is that Google Pay bans using an alternate OS. Most European banks have their own tap-to-pay which works on GrapheneOS. Google Pay banning GrapheneOS isn't an issue we can solve without regulatory action or court.\", https://x.com/GrapheneOS/status/1859956401663320536\nGrapheneOS (@GrapheneOS), 2024, November 13. \"Yes, in our Security> Exploit protection menu. You might as well turn that on for both Wi-Fi and Bluetooth. We might enable it by default at some point, perhaps only for fresh installs though to avoid changing how things work for existing users. We'd like to do it for everyone.\", https://x.com/GrapheneOS/status/1856820960365609082\nGrapheneOS (@GrapheneOS), 2024, November 12. \"The apps work but Google doesn't allow using tap-to-pay in those apps on an OS not officially licensing Google Play. It's possible to trickwith a rootkit hiding it's not the stock OS but that's unreliable and being gradually blocked. It's a temporary workaround not a solution.\", https://x.com/GrapheneOS/status/1856387498727092453\nGrapheneOS (@GrapheneOS), 2024, August 6. \"https://arstechnica.com/tech-policy/2024/08/google-loses-dojs-big-monopoly-trial-over-search-business/ Action is still urgently needed to address the highly anti-competitive Google Mobiles Services licensing system and the Play Integrity API which are a major part of Google maintaining their monopolies over search and many parts of the mobile market. We recently published a detailed thread about this here: https://x.com/GrapheneOS/status/1818414579153596422. We're in contact with the regulators in MULTIPLE countries about this. Don't fall for Google pretending Play Integrity API is security related or that their licensing system is about compatibility. Android and Chromium would massively benefit from proper collaboration between stakeholders without Google's business model getting in the way. Should be forced to deal with both following the model of the LLVM Foundation and also spin off Google Play into an independent company. Google is actively cracking down on competition in the mobile space by convincing app developers to use their Play Integrity API. Play Integrity API bans using operating systems not licensing Google's apps/services and agreeing to highly restrictive and anti-competitive terms. Google's licensing agreement directly bans OEMs from working with GrapheneOS and producing phones with it. Google sabotages their own products such as the Play Store to boost core monopolies. If it was a competitive market, they'd want their apps and services available to any OS. GrapheneOS has demonstrated Google Play works well as regular sandboxed apps without any special integration into the OS via our sandboxed Google Play feature. Google should be forced to spin off Google Play into an independent company competing with other app stores / services.\", https://x.com/GrapheneOS/status/1820902008519569873\nGrapheneOS (@GrapheneOS), 2024, July 30. \"https://x.com/arstechnica/status/1818325952553947517 The article unfortunately leaves out most of the points we made in the thread. GrapheneOS supports hardware-based attestation and it's entirely possible for Google to allow it as part of the Play Integrity API. They choose to ban using GrapheneOS. Play Integrity API has no minimum security patch level and nearly all these apps use weak software-based checks that are easily bypassed by attackers. The hardware-based checks rely on trusting every key distributed to every certified Android device, which are often leaked. Hardware-based attestation can be used for security purposes such as verifying device integrity with a pinning-based approach without the weakness of being vulnerable to leaked keys from the whole Android ecosystem since specific per-app keys in the secure element can be pinned. Play Integrity API is claimed to be based on devices complying with the Compatibility Test Suite and Compatibility Definition Document. We have irrefutable proof that the majority of certified Android devices do not comply with the CTS/CDD. Play Integrity API is based on lies. Essentially every non-Pixel device has important CTS failures not caused by CTS bugs. OEMs are cheating to obtain certification. Google claims GrapheneOS can't be permitted because we don't have a certification where they freely allow cheating and don't ban non-compliant devices. Since Play Integrity doesn't even have a minimum security patch level, it permits a device with multiple years of missing patches. Hardware attestation was required on all devices launched with Android 8 or later, but they don't enforce it to permit non-compliant devices. The reality is that the Play Integrity API permits devices from companies partnered with Google with privileged Google Play integration when they're running the stock OS. It's easy to bypass, but they'll make changes to block it being done at scale long term such as if we did it. It does not matter if these devices have years of missing security patches. It doesn't matter if the companies skipped or improperly implemented mandatory security features despite that being required by CDD compliance. Failing even very important CTS tests doesn't matter either. Google can either permit GrapheneOS in the Play Integrity API in the near future via the approach documented at https://grapheneos.org/articles/attestation-compatibility-guide or we'll be taking legal action against them and their partners. We've started the process of talking to regulators and they're interested. We're not going to give Google veto power over what we're allowed to do in GrapheneOS. We comply with CTS and CDD except when it limits our ability to provide our users with privacy and security. Google wants to be in charge of which privacy/security features can be added. Nope. Google's behavior in the mobile space is highly anti-competitive. Google should be forbidden from including Google Mobile Services with privileged access unavailable to regular apps and services. GrapheneOS sandboxed Google Play proves that hardly anything even needs to change. Google should also be forbidden from participating in blocking using alternate hardware/firmware/software. They've abused their market position to reinforce their monopolies. They've used security as an excuse despite what they're doing having no relevance to it and REDUCING it. Google is forbidding people from using a growing number of apps and services on an objectively far more private and secure OS that's holding up much better against multiple commercial exploit developers: https://x.com/GrapheneOS/status/1815105015079416062. They're holding back security, not protecting it. We've put a lot of effort into collaborating with Google to improve privacy and security for all Android users. Their business team has repeatedly vetoed even considering giving us partner access. They rolled back us being granted security partner access by the security team. As with how they handle giving out partner access, the Play Integrity API serves the interests of Google's business model. They have no valid excuse for not allowing GrapheneOS to pass device and strong integrity. If app developers want to ban it, they can still do it themselves. After our security partner access was revoked, we stopped most of our work on improving Android security. We continued reporting vulnerabilities upstream. However, we're going to stop reporting most vulnerabilities until GrapheneOS is no longer blocked by the Play Integrity API. This year, we reported multiple serious vulnerabilities to Android used by widely used commercial exploit tools: https://source.android.com/docs/security/overview/acknowledgements. If Google wants more of that in the future, they can use hardware attestation to permit GrapheneOS for their device/strong integrity checks. Authy's response about their usage of the Play Integrity API shows their service is highly insecure and depends on having client side validation. Play Integrity is thoroughly insecure and easily bypassed, so it's unfortunate that according to Authy their security depends on it. If Authy insists on using it, they should use the standard Android hardware attestation API to permit using GrapheneOS too. It's easy to do: https://grapheneos.org/articles/attestation-compatibility-guide. Banning 250k+ people with the most secure smartphones from using your app is anti-security, not pro-security. It's very unfortunate when new apps adopt the Play Integrity API and stop working. Authy isn't a very good choice for 2FA but many people use it and it's a problem for us for a widely used app to be incompatible. A single widely used app losing compatibility is a big deal to us.\", https://x.com/GrapheneOS/status/1818414579153596422\nGrapheneOS (@GrapheneOS), 2024, November 12. \"GrapheneOS since June 2021 and iOS since 18.1 in October 2024 are starting a timer when the device is locked and rebooting the device when the timer elapses without a successful unlock. It's not done to trigger regular reboots but rather to get data back at rest after a while. It's implemented as a protection against forensic data extraction in both GrapheneOS and iOS 18.1. If an attacker exploited the device and currently has kernel/root level access, a reboot could theoretically reduce their access, but it's not why either of these features exists. [...] Yes, the reboot is to get it back to Before First Unlock state after the device has been locked for a given amount of time without a successful unlock. If you consistently use it, it will never reboot automatically since it's an 18h default timer for us or 4 days for iOS 18.1. It has no impact on usability with consistent usage of the device. GrapheneOS has very frequent updates so there are regular user triggered reboots after updating the OS. Android updates work a bit differently than iOS and install in the background but a reboot is still needed.\", https://x.com/GrapheneOS/status/1856374417254658306\nGrapheneOS (@GrapheneOS), 2024, November 12. \"Location can be spoofed fairly easily since most GNSS isn't authenticated. GNSS also doesn't really work reliably indoors. Network-based location is how indoor and quick location estimates work, which is based on nearby cell towers and Wi-Fi networks. Neither is super reliable.\", https://x.com/GrapheneOS/status/1856375759775932477\nGrapheneOS (@GrapheneOS), 2024, November 11. \"Our Contacts Scopes feature causes apps to believe they've been granted the Contacts permission so they won't keep requesting it. It also has more granularity than the iOS 18 implementation: you can grant access to specific data for contacts instead of only the full contact info. Our Storage Scopes feature works the same way for all of the media and storage permissions. We plan to add more of these scope features for other things like Location (for a better alternative to Android's Mock Location feature) and other permissions like Camera and Microphone.\", https://x.com/GrapheneOS/status/1856006392290611273\nGrapheneOS (@GrapheneOS), 2024, November 11. \"It's even finer grained than specific contacts on GrapheneOS and apps won't annoy users for using it as is happening here since it appears as full Contacts permission access. Our Storage Scopes works the same way in that regard as will future features. There will be similar features for Location (per-app, unlike standard Android Mock Location), Microphone, Camera, etc. For Microphone and Camera, you'll be able to set an audio / video file which gets treated as if it's data from the camera.\", https://x.com/GrapheneOS/status/1856026315444400613\nGrapheneOS (@GrapheneOS), 2024, November 10. \"You don't have to sign into an account. If you want to use features requiring an account, you can make a throwaway one. There's no particular reason you need to use an existing account. You can use a different one for each profile where you use it.\", https://x.com/GrapheneOS/status/1855723027629248856\nGrapheneOS (@GrapheneOS), 2024, November 11. \"We have our hardware-based USB-C port control feature to defend against this and other classes of USB vulnerabilities. By default, when the device gets locked, we disable enabling USB-C alternate modes, adding new USB peripherals or activating USB gadget modes in the OS. Our feature also sets the hardware USB controller mode to block all new USB connections. As soon as there are no remaining active connections made before locking, we have the USB-C controller disable USB-C data entirely. This replaced our previous software-only approach to this. Our earlier software approach was the part we still have for blocking new USB peripherals in the Linux kernel once the device is locked. Blocking USB gadget modes is a minor addition in case there's a bug in the Android USB gadget mode control. The rest is a major improvement.\", https://x.com/GrapheneOS/status/1855974437817893176\nGrapheneOS (@GrapheneOS), 2024, November 10. \"Play Integrity API runs fine but it's not going to be considered a Google certified operating system. GrapheneOS supports verified boot, hardware-based attestation and all the rest of the standard security model but Google decides based on if they licensed it not security.\", https://x.com/GrapheneOS/status/1855714441859654077\nGrapheneOS (@GrapheneOS), 2024, November 10. \"Google's main excuse would be claiming Google Play can't work that way (we proved it can) and that important features require it which would be unsafe to give to other apps (stuff like accessibility service access exists, they can just use the same approach or stop doing it). Example: they want to scan and block installing APKs detected as malware, which sandboxed Play Store can't do (it can scan installed APKs but isn't queried for installing and can't block them). Okay, just make a special access permission for that and other AV apps could use it.\", https://x.com/GrapheneOS/status/1855685039893164415\nGrapheneOS (@GrapheneOS), 2024, November 10. \"Our sandboxed Google Play compatibility layer is open source code shipped as part of GrapheneOS which enables optionally installing and running Google apps as regular apps in the standard app sandbox without any special access, control or privileged integration into the OS. In order to make it easy to install sandboxed Google Play, our App Store provides mirrors of the official releases. These have through extensive testing by users either opting into the Alpha and Beta channels for those in our App Store along or Beta releases in the Play Store. Our compatibility layer teaches Play services and the Play Store how to function as regular apps via the standard Android APIs and permissions. It's not necessary to grant any permissions to either of them in order to use them and provide compatibility with near 100% of apps. Apps with mandatory or optional dependencies on Google Play are including Google Play libraries in their app. Many of these libraries such as Ads and Analytics work fine without Google Play services. These libraries could do everything sandboxed Google Play can do without it. For example, Google could include fallback code in the Firebase Cloud Messaging library to support receiving push messages through a service run by the app itself. Google could also support using Play services and Play Store as sandboxed apps. We've shown it works fine that way. It's important to understand that our approach is not granting any access to them or allowing them to do anything they couldn't do without it. Our compatibility layer demonstrates a regulator could reasonably require Google to not use capabilities unavailable to regular apps. Our approach also allows us to reimplement arbitrary portions of Google Play ourselves. For example, we reimplement the Play services location service by using the OS location service. By default, apps using Google Play location are redirected to standard GrapheneOS location. We provide emulated network location support to use apps depending on network location without it. We're in the process of implementing our own network location client with support for choosing between multiple services. It's currently being tested and polished up to ship it. Our opt-in network location that's going to be shipped soon is not tied to Google Play compatibility. It's available to any apps using the OS location API. Our default enabled redirection means that apps using Google Play location will transparently use this when it's enabled. We'll also be providing support for using Apple's location service or a GrapheneOS proxy to it as the starting point. We're also going to be providing our own location service based on our own database built from data scraped/merged from multiple sources which is entirely legal. Our own network location service will support downloading regional databases to use it entirely locally without sending location data to a service. We plan to provide the same features for the SUPL service used for A-GNSS which currently defaults to a GrapheneOS proxy to Google. We're also going to be providing our own implementation of geocoding (converting addresses to location) with a choice of providers which is currently in an experimental state. We can very easily host that ourselves too. We'll be doing the same for many other things over time. Our position is that useful features should be directly provided in the OS disconnected from Google Play compatibility. Apps using Google Play for it can be redirected to using the OS implementation instead with a toggle for it. We can also unify it with the provider toggles. For network-based location, it's completely legal for us to scrape enormous amounts of data from publicly available services not even requiring accounts to combine into our own multi-source database. It will allow us to provide non-satellite-dependent offline location detection. We have additional extensions beyond the baseline compatibility layer to support Android Auto. It's disabled by default, and Android Auto is a regular sandboxed app when installed. Users can choose to grant it extra USB access to use wired Android Auto. Wireless needs a lot more.\", https://x.com/GrapheneOS/status/1855688621824565335\nGrapheneOS (@GrapheneOS), 2024, November 10. \"Detecting location based on querying a service using nearby cell towers, Wi-Fi networks and/or Bluetooth beacons to get a location estimate. It can function in different ways, and we plan to enable doing it entirely locally via our service supporting downloading regional data. It's an alternative to satellite-based location (GNSS) which can work well indoors in cities and can give a quicker rough estimate. GrapheneOS currently relies on satellite-based location (GNSS) with services to assist it (A-GNSS) called PSDS (database downloads) and SUPL. PSDS is a database of predicted satellite locations and topographical data to speed up satellite locking locks. We host our own cache of the data on our servers. SUPL is kind of a network location lite requesting data on a carrier's cell towers to speed up GNSS which we proxy. [...] Apps can't scan for nearby cell towers or Wi-Fi networks without the Location permission. IoT devices with Wi-Fi can certainly detect their location via Wi-Fi scans. Look at https://wigle.net for an example. Apple and Google have much better databases. We don't like that network location is based on revealing location to a service, although if it's done with individual queries without a long live connection via a VPN that's much less of an issue. That's why we want local database support, requiring having our own database. Building our own db entirely with data we gather is unrealistic. It'd require having people submit huge amounts of privacy sensitive data. Our plan is to scrape the data from services like the Apple location service and merge it with data from BeaconDB (https://beacondb.net). The way this works in general is that you scan for nearby cell towers, Wi-Fi APs and Bluetooth devices which then get submitted to a service to get the locations of those things which can then be used to figure out location based on signal strength to each of them. Doing it with cell towers alone is already decent for a rough estimate. Wi-Fi is incredible for this in cities. This is how iOS and Android manage to provide indoor location without satellite receptions. They get nearly all users to not opt-out of it. People are used to it. [...] Wi-Fi APs should not be brought with you since their location can be found through tons of devices sending that data to Apple, Google, etc. If you move, bringing the same Wi-Fi router will give data showing someone moved from your old location to the new one. If your Wi-Fi AP simply stays in one location, that's not a privacy issue. Hiding the SSID doesn't change that there are packets marked as coming from and going to the MAC address for that AP. Hidden SSIDs also reduce client privacy and enable tracking. Each hidden AP hidden network you have saved on a laptop, phone, etc. is broadcast every time they scan for Wi-Fi networks. It allows tracking client devices despite MAC randomization. Should never save any hidden SSID networks. Hidden SSIDs only really reduce privacy.\", https://x.com/GrapheneOS/status/1855669762535075977\nGrapheneOS (@GrapheneOS), 2024, November 10. \"Pixel 5a has known unpatched remote code execution vulnerabilities. It also lacks important GrapheneOS features, including ones relevant to this attack vector such as our USB-C port control feature. It only has the older approach similar to one of the opt-in iOS options. You should be worried about more than forensic data extraction on a Pixel 5a. You don't have firmware or driver updates in general for Wi-Fi, cellular, Bluetooth, GPU and the rest of the hardware. You also don't have current era GrapheneOS defenses. We ship AOSP ASB backports. AOSP ASB backports are backports of the High and Critical severity AOSP patches. They don't include Moderate or lower severity patches for the most part and also don't include firmware or driver patches. We strongly recommend not using end-of-life devices.\", https://x.com/GrapheneOS/status/1855619220936995007\nGrapheneOS (@GrapheneOS), 2024, November 10. \"We could mirror all the contents of Accrescent in our own App Store but we don't really want to do it right now especially since Accrescent may start doing things in a way that's not directly compatible with our App Store. We also want a more catered approach for our App Store. We don't want to simply put any app which a developer uploads to Accrescent in our App Store. Accrescent is aiming to be an app store for everything and that's not what we want to provide. We only want to provide our own hardened builds of a tiny number of recommended apps. Our approach will likely be making our own lightly modified forks of a small number of apps like Organic Maps with a modified app id and icon so you can still install the developer release on GrapheneOS without it conflicting or causing confusion.\", https://x.com/GrapheneOS/status/1855620262827274631\nGrapheneOS (@GrapheneOS), 2024, November 10. \"You can use most of the AI features within the relevant apps such as Google Photos and the hardware acceleration for them via the TPU works fine. Their screenshot editor is available via the Markup app in our App Store since it's not available in the Play Store so we mirrored it.\", https://x.com/GrapheneOS/status/1855586274205159587\nGrapheneOS (@GrapheneOS), 2024, November 10. \"It's possible to verify that it's running GrapheneOS. We have a recommended process for dealing with these devices at https://grapheneos.org/faq#preinstalled-devices. Theoretically, there could have been tampering with hardware, but there are some trustworthy vendors selling them not only sketchy ones.\", https://x.com/GrapheneOS/status/1855585980889116964\nGrapheneOS (@GrapheneOS), 2024, November 9. \"Punkt. MC02 phone doesn't run GrapheneOS. It still runs a fork of Android 13 while GrapheneOS is solely based on Android 15. MC02 is clearly using the LineageOS update client, not the GrapheneOS update client. It's problematic that some people think it's a way to get GrapheneOS. MC02 appears to use an older version of our sandboxed Google Play compatibility layer, but they haven't kept up with our updates at all so they don't have the full app compatibility of GrapheneOS. We're unsure how much other code they used from GrapheneOS but it's not current. There are many companies selling devices with GrapheneOS preinstalled. It's also very easy to install it on your own with https://grapheneos.org/install/web from a web browser on another device. MC02 isn't a way to obtain GrapheneOS preinstalled and GrapheneOS can't be installed onto it. There's a lot of media coverage about the device including reviews where it's portrayed as running GrapheneOS. We weren't contacted by news publications about their stories/reviews. We would have been happy to correct misconceptions if we have been contacted about any of this.\", https://x.com/GrapheneOS/status/1855384955540365562\nGrapheneOS (@GrapheneOS), 2024, November 9. \"Purism promotes their promotes with outright misinformation about devices like iPhones with dramatically better privacy and security than their products. They're blatant scammers, not simply bad products, and they've done immense harm with their propagation of misinformation. Many people are also wrongly led to believe their devices have open firmware and open firmware when that's not the case at all. They portray shipping open software as the underlying hardware and firmware being open, which is wrong. It's more than just misleading marketing.\", https://x.com/GrapheneOS/status/1855289903866642716\nGrapheneOS (@GrapheneOS), 2024, November 8. \"We don't think they have a horrible security or privacy posture. It's a better option than anything other than GrapheneOS. They don't deploy nearly as aggressive exploit protection features as we do because they don't want to make even small sacrifices for security. In general, Apple and Google won't accept even a 1% performance hit for security. Google has been more willing to ship security features with a performance cost. Apple and Google are capable of building great security features but prioritize other things more.\", https://x.com/GrapheneOS/status/1855097145633677804\nGrapheneOS (@GrapheneOS), 2024, November 9. \"It's essentially the opposite of GrapheneOS. It massively rolls back security with years of delays for many important privacy/security patches, compromising the privacy/security model, building in privileged support for Google services (microG), their own invasive services, etc. /e/OS is just an outdated fork of LineageOS with their own problematic apps/services. LineageOS significantly reduces security compared to AOSP but it's not nearly as bad as /e/OS. It's a much worst version of LineageOS marketed as being private and secure when it really isn't.\", https://x.com/GrapheneOS/status/1855253217073824208\nGrapheneOS (@GrapheneOS), 2024, November 7. \"GrapheneOS and CalyxOS are very different. GrapheneOS is a hardened OS with substantial privacy/security improvements: https://grapheneos.org/features. https://eylenburg.github.io/android_comparison.htm is a third party comparison between different alternate mobile operating systems. It could include many more privacy/security features but it's a good starting point. CalyxOS is not a hardened OS. It greatly reduces security vs. AOSP via added attack surface, weakened security model and slow patches. It's currently significantly behind on patches and may fall a couple months behind. They consistently mislead users about this including setting an inaccurate patch level. Compatibility with Android apps is also much different. GrapheneOS provides our sandboxed Google Play compatibility layer: https://grapheneos.org/usage#sandboxed-google-play. Can run vast majority of Play Store apps on GrapheneOS, but not CalyxOS with the far more limited and less secure microG approach.\", https://x.com/GrapheneOS/status/1854437075988463906\nGrapheneOS (@GrapheneOS), 2024, November 6. \"Volla OS substantially rolls back privacy and security compared to standard AOSP. It's similar to LineageOS. It's not a hardened OS like GrapheneOS and is not comparable to it. Much like CalyxOS, it's a product claiming to be private/secure but both are much worse than an iPhone. iPhones are the next best option for privacy and security after GrapheneOS, not any of these highly problematic alternate operating systems based on AOSP/LineageOS without the ability to provide the standard privacy/security patches and features from AOSP. iPhones are better. Full Android privacy and security patches are released via the monthly, quarterly and yearly releases. There are partial backports of most High and Critical severity issues to older releases such as Android 14. They're not on the latest release so they don't have current patches. GrapheneOS starts from the baseline of the latest Android Open Source Project and implements massive privacy and security improvements on top. Volla OS, LineageOS, CalyxOS and /e/OS roll back security rather than improving it. Ubuntu Mobile is a far further regression than those. An iPhone has much better privacy and security than those. The latest Android Open Source Project on a Pixel has competitive security with an iPhone, but not privacy, and we're not referring to Google services but rather missing privacy features. Older releases are far worse. iPhones are far from perfect but putting putting non-hardened AOSP 13/14 on typical Qualcomm/MediaTek Android hardware is dramatically worse. Most of these options are also adding their own problematic services and Apple is in fact doing better than them at E2EE services, etc. [...] Knox is almost entirely branding for standard security features. Samsung devices don't have competitive security with iPhones and Pixels. Samsung devices also don't have serious alternate OS support and cripple the device including disabling important security features. In terms of security features, Samsung devices are the next best option after Pixels in the Android world, but there's a huge gap. Security is also about more than having a bunch of good features. Samsung adds a ton of complexity and attack surface with their overall changes.\", https://x.com/GrapheneOS/status/1854268559712567470\nGrapheneOS (@GrapheneOS), 2024, November 6. \"CalyxOS has far worse app compatibility than GrapheneOS and doesn't have remotely comparable privacy or security features. CalyxOS is consistently far behind on even providing basic privacy/security patches including right now, and does not preserve the standard security model. CalyxOS is still based on Android 14 QPR3 without the full October security patches or the already released November security patches. Their changes to the OS add significant attack surface, compromise the security model and do not do significantly improve privacy or security. GrapheneOS is much easier to install, much easier to use due to much broader app compatibility and actually massively improves privacy and security. CalyxOS has leaky network toggles, misguided anti-privacy VPN changes and an unsafe panic feature not reliably deleting data, etc. Calyx misleads their users about the privacy and security it provides. They set an inaccurate Android security patch level field and lie about which patches are shipped in their official release notes. They downplay the impact of unpatched vulnerabilities and other issues. Calyx Institute has a long history of reselling T-Mobile and Sprint cellular service via highly insecure hotspot devices which they peddle as private and secure. They pretend to be heavily involved in privacy/security but do nothing of actual substance. It's just empty marketing.\", https://x.com/GrapheneOS/status/1854246847398707601\nGrapheneOS (@GrapheneOS), 2024, November 6. \"Google apps aren't malware and as long as you're not opting into the invasive usage stats reporting or uploading your photos to their service it doesn't really matter. There are nicer local gallery apps for viewing photos than Google Photos but without fancy AI editing features. There are various options for a local gallery app, end-to-end sync for photos, keyboards, etc. Gboard is definitely one of the nicest keyboards and at the moment options like FlorisBoard are missing basic features and the same level of usability, but hopefully they improve. [...] Active keyboards have access to all the text you're entering, the text you're editing and the clipboard contents at all times. Gboard is not malware and does not send sensitive user data to them unless you opt-in to it, but we'd prefer having a nicer keyboard ourselves. [...] The default one in GrapheneOS is fine. FlorisBoard will eventually provide a lot more features but is missing some basic features right now. Gboard is fine as long as you don't opt-in to usage stats and other invasive options. We can't really recommend anything else. The default keyboard in GrapheneOS is essentially Gboard from 2014. It used to be an open source project with a few closed source components for the Google Keyboard releases such as the library for swipe typing. It ended up becoming entirely closed source and rebranded to Gboard. You can use emojis with the default keyboard. Press and hold the enter key to show it. It's missing emoji search, gender, skin color, more recently added emoji, etc. If people care about emoji they need FlorisBoard or Gboard. We plan to eventually migrate to another keyboard. If you don't care about emoji, the default keyboard is fine. It's missing some nice things like sliding on the space bar to move the cursor, one handed mode and other innovations added to Gboard after they stopped releasing the code as open source. It has the UI part of swipe typing but the library to make it work was closed source so that's missing. FlorisBoard is almost entirely all around better in terms of features but it has some major features missing and the UI isn't very polished yet. We'll likely make a fork of it.\", https://x.com/GrapheneOS/status/1854210848559976748\nGrapheneOS (@GrapheneOS), 2024, November 4. \"Emergency alerts are sent out through the cellular network to all phones with a cellular connection. It's possible for a phone to receive them with no SIM. Only airplane mode prevents receiving them. In the stock OS, presidential alerts (often renamed elsewhere) can't be toggled. The alerts can be sent out to a specific area via the cellular network if the carriers support it. However, they can also include geofencing data inside of the alert metadata for the OS to check if it can quickly obtain the location (GNSS is usually too slow for this though). GrapheneOS doesn't currently have built-in network location so the geofencing will often be ignored unless there's already a cached location from something else using it. It doesn't particularly matter since the alerts are usually not sent outside of the region impacted. GrapheneOS will have an opt-in built-in network location feature in the near future. We have an experimental implementation which we're testing already. It will not be enabled by default for privacy reasons.\", https://x.com/GrapheneOS/status/1853424249681121759\nGrapheneOS (@GrapheneOS), 2024, November 4. \"Changing IMEI wouldn't prevent tracking via cellular since there are other identifiers specific to radios and also extensive fingerprinting possibilities. Choosing a random IMEI would make you almost entirely unique as a starting point. If people want to avoid both the device IDs and SIM IDs being located by the cellular network, they need to use airplane mode to disable the cellular radio: https://x.com/GrapheneOS/status/1846712535480774794. May also want to toggle off the SIM so the Wi-Fi calling/texting tunnel is disabled too. We plan to provide more configuration for controlling Wi-Fi calling/texting where users can entirely toggle off the IPsec tunnel it uses while still using the SIM. It's not one of our top priorities since disabling the SIM is already available as a standard option and does that. Buying both the phone and SIM anonymously does start you off without an identifier tied to you, but the location can be tracked over time while they're used and it could be tied to you through that. You'd need to replace both the phone and SIM together to get a fresh start.\", https://x.com/GrapheneOS/status/1852764816743547243\nGrapheneOS (@GrapheneOS), 2024, October 16. \"IMEI is not the only hardware identifier for the device available to the cellular network. Changing the IMEi alone isn't enough to hide the device identity from the network. It will only hide one commonly used ID rather than making the device not uniquely identifiable. Carriers often detect device model via IMEI and multiple other ways as part of their standard operating procedure. They change how things work based on the detected capabilities but also hard-wired quirks for device models, etc. Devices send a lot of info on capabilities. It's possible to detect the devices with an IMEI not matching their capabilities/configuration or to detect that there's a device with the IMEI changing repeatedly but the other device identifiers remaining the same. You could end up drawing attention to yourself by doing it... Similarly, using a very niche hardware device to connect to the network such as a standalone hotspot device stands out. Those devices are also far less secure than simply using a Pixel with GrapheneOS. They typically don't get proper updates and lack basic security measures. If you really want to have cellular done from a separate device, a used Pixel with GrapheneOS is a good option. If you want a fresh identity for the cellular network, there isn't really much alternative to using both a fresh device and SIM. Wi-Fi has a much more private design. To summarize: 1) IMEI randomization is a poor way to improve privacy and will draw attention to yourself in practice. 2) Dedicated Hotspot devices aren't good for privacy/security. 3) Use airplane mode + Wi-Fi with our default per-connection MAC randomization for better privacy. MAC randomization alone was not enough. Wi-Fi has had major privacy improvements on common devices like iPhones and Pixels in recent years to neuter other tracking based on sequence numbers, etc. We're aware of 1 remaining issue which has is reported/accepted as a vulnerability. Also, bear in mind that carrying around a Wi-Fi access point (AP) is the opposite of private. An AP has a persistent MAC even if it's random upon creating the AP such as making a hotspot with a phone. Wi-Fi does not have MAC rotation like Bluetooth Low Energy privacy extensions. GrapheneOS uses per-connection MAC rand and per-connection DHCP as improvements over the standard Android Open Source Project. The MAC still remains the same while connected, and an AP isn't going to cycle until it's reset. Wi-Fi does not try to do what BLE privacy extensions do. To clarify, an access point means a router such as Wi-Fi hotspot from a phone. If you enable Wi-Fi hotspot, it chooses a random MAC and uses it until it's disabled. Bluetooth LE tries to provide privacy for a paired device being carried around incl. MAC rotation. Wi-Fi does not. [...] The radio has hardware IDs so it's very clear that it's the same device using a new SIM. There is not much point cycling one without also cycling the other at the same time. [...] In general, you should only use the supported devices, which means Pixel 6 and later. 8th/9th generation Pixels have a much more hardened OS via hardware memory tagging. 9th generation Pixels currently have more cellular baseband hardening but it may get backported to 8th gen. We're not at all particularly recommending using a 2nd device for cellular though. We're just pointing out that using GrapheneOS on a used Pixel 6a would be much more secure for that purpose than using the dedicated hotspot devices. The hotspot devices are generally awful. [...] We've explained it in this thread: 1) There are more hardware identifiers than IMEI, IMSI and EID. Avoiding using eSIMs doesn't avoid having more hardware identifiers than IMEI. 2) Network sees what kind of radio/device it is and how it behaves, which impacts how it functions. To expand on the 2nd point, that means if you choose a completely random IMEI instead of a random choice among IMEIs of a similar type of device, it will clearly be a device that's using a random IMEI. By clearly having a random IMEI, that stands out a lot and invites attention. There are typically other hardware identifiers for the radio than IMEI and the eSIM ID. It is similar to how many Wi-Fi adapters largely used to leak other info and had silly things like info on the type of adapter or even a serial number included in each probe request. [...] They can uniquely identify the radio so with a non-KYC device and non-KYC SIM, you're essentially using a persistent pseudonym. If you randomize the IMEI and switch SIMs there are still other IDs and a fully random IMEI isn't going to match your hardware which is obvious. [...] IMEI and IMSI are the most common but there are several other common hardware identifiers including ones specific to a physical SIM and ones specific to an eSIM. It's more complex than Wi-Fi where MAC address randomization commonly doesn't actually work due to other IDs. Look into papers on MAC randomization not actually working. There's some nice research on it. iPhones and Pixels made adjustments to Wi-Fi with the Wi-Fi vendors to address this but our community has found some remaining minor weaknesses which have been reported to get fixed. Cellular is far more of a mess than Wi-Fi where minimal probe information, probe sequence number randomization and a few other things combined with MAC randomization and higher level OS changes should be enough. Cellular is designed to identity the device so it's much different. In the past, Wi-Fi hardware, firmware and drivers were not carefully designed and implementation to avoid identifying the hardware to the network. This has been largely addressed for iPhones and Pixels, and it's trickling down to other smartphones. Laptops are more behind though. Cellular is a completely different story. It has a bunch of explicit device identifiers but also other ways it identifies itself to the network. There's also often persistent state for the radio which can do the same thing rather than it just being completely fresh each boot. There isn't comparable research into cellular privacy partly because of a mix of perceived and real regulatory and legal issues. People believe the issue is that changing the IMEI is illegal, but it's often legal. The real issue is that it leaks device identity despite that... Changing the main IMEI that's provided also doesn't mean that the original IMEI isn't available via diagnostic messages, but there are other identifiers. There's also often persistent radio state usable for tracking it regardless of the device identifiers. It's not very private. eSIM ID is a well documented identifier but there are more than those. Even aside from all the actual identifiers, there are other ways of tracking similar to how MAC randomization is not enough for Wi-Fi even if it's the only identifier, but cellular has more than 1 identifier. [...] There are more cellular radio identifiers than IMEI. People should really be aware that they are not truly preventing uniquely identifying the device through changing the IMEI. Randomizing IMEI is easily detected and clearly stands out from other devices too. Cellular network can see what kind of radio is connecting and what capabilities there are along with there usually being some persistent state for the radio rather than each boot being an entirely fresh start. There are also other IDs. It's just not designed to avoid tracking. It's not specific to smartphones or particular kinds of radios, it's cellular in general. Wi-Fi had awful privacy even with MAC randomization and often had other things like a serial number included in probe requests, etc. but there was research into it and problems got solved. Wi-Fi is still not perfect but at least on iPhones and Pixels it's a lot better than it was. We're aware of 1 minor issue with the Broadcom Wi-Fi radio on Pixels which only applies when connected to a network, not scanning. It has been reported and acknowledged as a security bug. [...] We're talking about there being other hardware identifiers or even the original IMEI still being shared based on research done by our community and others including with mobile hotspot devices. It is similar to Wi-Fi MAC rand before efforts to get rid of other leaked identifiers. You know there's an official eSIM ID (EID) for the eSIM secure element, right? That's a clearly documented non-IMEI hardware identifier. We're also talking about using a physical SIM though, not only eSIM. See https://support.google.com/pixelphone/answer/10402530. We're talking about more than that though. We cannot share the very minor Wi-Fi privacy issue disclosed to us because a firmware/driver patch is not available yet, but other Wi-Fi adapters often have far bigger issues and it's a good thing that the remaining privacy issues with regular Wi-Fi usage have become so minor. The standard Broadcom Wi-Fi adapters on Nexus and other devices used to have firmware from Broadcom adding a whole bunch of unnecessary info to probe requests and other Wi-Fi packets. They also used a sequential probe sequence number and other counters. until fully reset. Some Wi-Fi adapters literally sent off a serial number not just MAC, defeating MAC randomization. Essentially all would end up having a quite unique probe sequence number after being used for a while with a predictable rate of increase. These issues were solved for Pixels and should have trickled down to other Android devices now, although we haven't looked at how it currently works elsewhere. One relatively minor non-scanning issue is known and they should be working on it and fixing it in a few months. [...] '> I saw you recommended turning the phone off, that also doesn't prevent tracking. Powered off in a faraday bag, maybe.' Airplane mode and turning off the device work fine in practice if it doesn't have a special offline tracking system such as Apple's optional Find My network. Regulations essentially require both of those to work properly but are poorly enforced. Devices can make mistakes and have issues like airplane mode not being active in early boot. It's unlikely for it to be improperly implemented on something like an iPhone though.\", https://x.com/GrapheneOS/status/1846709900715458703\nGrapheneOS (@GrapheneOS), 2024, October 17. \"Changing the IMEI does not work in the same way as MAC randomization with a modern Wi-Fi radio supporting private usage by avoiding other identifiers. It will draw attention and will not hide the device identity in practice. More would be required, as it is with Wi-Fi MAC rand. We've explained how it can hurt: it's very obvious that it's being spoofed if you simply choose random values corresponding to random models of phones or unused ranges. There are also multiple other identifiers in practice. What if I do not choose random value, but different  value for the same model of the device, from used ranges? There are 4G modems with open Telnet/SSH where you can spoof everything and then connect them by cable to GrapheneOS phone used as a tablet. That SSH access is not to the baseband but rather an OS next to it. It is comparable to having that access to the OS on the phone. It doesn't mean you have control over the actual baseband and the identifiers that it's going to be giving to the network. Choosing valid values for that type of device avoids drawing attention to yourself in the same way if you manage to choose values which are not in use by other people or some other issue like that which could draw attention. Does not address other issues.\", https://x.com/GrapheneOS/status/1846825654852981015\nGrapheneOS (@GrapheneOS), 2024, October 17. \"Cellular and Wi-Fi both provide location detection and is the main way the initial location detection works for most Android and iOS users. Cellular radio in the phones we support is isolated and much more secure. How is this an improvement? It's higher not lower attack surface. People using cellular through the devices we support have an increasingly hardened radio that's finally shipping the kinds of security features used for OS components like media handling. You do not even get proper patching with these hotspot devices. https://security.googleblog.com/2024/10/pixel-proactive-security-cellular-modems.html [...] They don't have control over the overall device if they exploit the radio and have less access than if they exploited one of these badly secured devices since they can't attack the phone via Wi-Fi and/or USB without actually being in physical proximity or having physical access.\",\nGrapheneOS (@GrapheneOS), 2024, October 30. \"Pixel 5 has been end-of-life since after November 2023 and you were supposed to replace it before December 2023. It hasn't received firmware patches for almost a year and driver support has ended too. You should be replacing it with an officially supported device for GrapheneOS. Cellebrite not having exploits for GrapheneOS on a Pixel 5 doesn't make it a secure device. If you want to have a secure device, you need to replace it. You've been receiving extended support releases, which are not secure and not meant to be used. We've made this 100% clear.\", https://x.com/GrapheneOS/status/1851618384317755629\nGrapheneOS (@GrapheneOS), 2024, October 29. \"You can use airplane mode when you don't want to be using cellular. Primarily using Wi-Fi is often completely viable. Still useful to have a cellular subscription in case you need it, but you can turn it back off via airplane mode when you're through using it. Also worth noting that carriers often support Wi-Fi calling and texting. Airplane mode turns off cellular. Turning off the SIM is a different thing, which doesn't disable cellular. If you want Wi-Fi calling/texting off, you need to also turn off the SIM alongside airplane mode. Emergency calls and alerts work without a SIM based on the phone still being able to use cellular in a much more limited mode. Airplane mode is the way to disable it, not avoiding having a SIM or disabling them, which is useful but accomplishes something else. You may want both.\", https://x.com/GrapheneOS/status/1851359771225411833\nGrapheneOS (@GrapheneOS), 2024, October 29. \"It's possible to use a work profile alongside a Private Space so you could consider using a work profile too if you want things in 3 separate profiles usable side-by-side. Private Space is nicer than a work profile but local management apps for work profiles can still be used. We're going to add a toggle for isolated Private Space clipboard soon which is the main limitation vs. user profiles. For a work profile, they're overall less isolated than Private Space and depend on a work profile management app in the Owner user determining their isolation.\", https://x.com/GrapheneOS/status/1851261401408450591\nGrapheneOS (@GrapheneOS), 2024, October 29. \"It provides different features and integrity levels. The strong integrity level is based on hardware attestation. That involves the hardware providing a signed response to a challenge about the lock state, operating system, etc. and can't be spoofed within the OS. It has a weaker device integrity level based on checking a bunch of stuff within the OS from Play services, which is possible to spoof. However, they use fingerprinting techniques such as GPU fingerprinting and send along that data, which enables detecting and banning spoofing. It is NOT practical to pretend to pass these checks. It is only possible in the short term at a small scale. It will get banned and stop working. The only possible solution is regulatory or legal action based on this being highly anti-competitive and clearly illegal behavior.\", https://x.com/GrapheneOS/status/1851242030153838752\nGrapheneOS (@GrapheneOS), 2024, October 29. \"For apps which implement their own push like Signal or the more efficient privacy/security focused Molly fork, it will work fine out of the box but needs the battery usage exception the app requests to work properly. Most Android apps only support doing push via Google Play. If you don't have sandboxed Google Play, you won't have push in apps which only know how to use FCM for it. There's no setup involved beyond installing Google Play and giving it the battery optimization exception our compatibility layer requests or doing that manually afterwards. Some apps including Signal need to be installed after sandboxed Google Play in order to use it properly. Beyond that, there's no setup involved. GrapheneOS doesn't have any special system or requirements for push notifications. You probably just don't use Google Play.\", https://x.com/GrapheneOS/status/1851209993946542086\nGrapheneOS (@GrapheneOS), 2024, October 27. \"The purpose of GrapheneOS is not specifically avoiding Google apps and services. The purpose of it is providing a high level of privacy and security, [...] GrapheneOS has our sandboxed Google Play compatibility layer. We go out of the way to provide the ability to run Google apps and services along with the many apps depending on those as regular sandboxed apps. It's a lot of ongoing work to provide it, not a trivial feature.\", https://x.com/GrapheneOS/status/1850489337793724495\nGrapheneOS (@GrapheneOS), 2024, October 27. \"You can use airplane mode to disable the cellular radio and it will work properly with a properly implemented phone. We recommend that people use airplane mode with GrapheneOS and use Wi-Fi when they don't want the cellular network to know the location of the device.\", https://x.com/GrapheneOS/status/1850484589313053085\nGrapheneOS (@GrapheneOS), 2024, October 27. \"Sandboxed Google Play is the regular official Google Play. Our compatibility layer teaches it how to run within the regular app sandbox. It has absolutely no additional access or control beyond any other regular app. Therefore, using it gives 0 additional access to Google Play. The apps you're using which depend on Google Play are using the Google Play SDK and libraries. That means they run Google Play code as part of themselves. They run in the same app sandbox as sandboxed Google Play. You aren't giving any more access by using sandboxed Google Play.\", https://x.com/GrapheneOS/status/1850483664338989220\nGrapheneOS (@GrapheneOS), 2024, October 27. \"It doesn't defeat the purpose of using GrapheneOS at all. That's a common major misconception about GrapheneOS. They still have all of the privacy and security features offered by GrapheneOS and are using Google Play as regular sandbox apps via our sandboxed Google Play feature.\", https://x.com/GrapheneOS/status/1850483372801323013\nGrapheneOS (@GrapheneOS), 2024, October 22. \"GrapheneOS fully supports the Private Space feature in Android 15, which is essentially a separate user nested inside of the Owner user. We strongly recommend it as a replacement for a work profile managed by a local profile admin app. It has better OS integration and isolation. Private Space is an isolated workspace (profile) for apps and data similar to both user profiles and work profiles. All 3 forms of profiles also have entirely separate VPN configuration which is very useful even if you connected to the same VPN, since exit IPs can be separate. All forms of profiles have separate encryption keys. You can keep a Private Space at rest while the Owner user is logged in just as you can with a secondary user. Private Space makes it easier to share data than users. The clipboard is shared, but we could add a setting for it. GrapheneOS users choose to use the OS in different ways. A lot of people largely use open source apps and not sandboxed Google Play. Others use sandboxed Google Play in their main profile. Many use sandboxed Google Play in a dedicated profile to choose which apps use it. Regardless of how people choose to use sandboxed Google Play, they're regular sandboxed apps without special access. Private Space makes it easier to use a dedicated profile for sandboxed Google Play though. It's also worth noting you can still use a work profile alongside it. All of our features including Contact Scopes, Storage Scopes and sandboxed Google Play have full support for Private Space. We added support for it significantly before the release of Android 15, even before the initial early release of the source code was published in September.\", https://x.com/GrapheneOS/status/1848744438568263956\nGrapheneOS (@GrapheneOS), 2024, October 26. \"Try running them in a secondary user instead to see if it's because Private Space apps can't use that functionality.\", https://x.com/GrapheneOS/status/1850237139260260734\nGrapheneOS (@GrapheneOS), 2024, October 22. \"Yes, Private Space is essentially a secondary user which you can activate nested inside of the Owner user. Apps can't communicate between the Owner user profile and the nested Private Space other than the clipboard. We could add a setting to control the shared clipboard though. Only the currently focused app and the active keyboard app can use the clipboard along with it showing a notice when the focused app reads clipboard data set by another app so that's a limited communication mechanism. We plan to provide more clipboard control in general anyway. [...] It's slightly more separate in a dedicated user but it's a lot more convenient in the Private Space. Having it in Private Space is nearly as convenient as having it in your Owner user profile directly. They're regular sandboxed apps regardless, including simply using 1 profile.\", https://x.com/GrapheneOS/status/1848750760252620814\nGrapheneOS (@GrapheneOS), 2024, October 22. \"It's nested within the Owner user so you can be using apps from both side by side and getting notifications from them together. It's easier and more convenient. It's a bit less isolated, but not much. Clipboard is shared but we could add a toggle for keeping it separate instead. [...] In that case, a secondary user would provide better isolation, but the Private Space does largely provide the same protection of the Owner profile from the Private Space just not quite as much in the other direction.\", https://x.com/GrapheneOS/status/1848748948992721273\nGrapheneOS (@GrapheneOS), 2024, October 22. \"Work profile management apps can enable a lot of communication between the Owner user and the nested work profile. Installed packages are shared across all kinds of profiles. App data is entirely profile specific. Whether or not an app is installed is profile specific. For a given app id such as app.organicmaps, the packages are shared across each profile with them installed. The packages are removed once every profile fully uninstalls the app. The OS enforces that packages must have a matching signing key and equal or greater versionCode. Changes to signing keys must be authorized with key rotation proofs. They implemented it as sharing the installed packages to save storage, memory (smaller page cache) and bandwidth. It's just something to bear in mind. The main way this comes up is because not all app developers follow best practices. You're supposed to have a unique app id for each variant of app, including different types of builds or builds signed with different keys. Some devs reuse ids. F-Droid is the main offender. They reuse the app ids used by developers for their standalone and/or Play Store releases for nearly every single app in F-Droid despite nearly all being signed with F-Droid keys. This causes conflicts across profiles, which is confusing to users. F-Droid also causes usability issues with this another way. The main download link on their website is almost always outdated instead of pointing to the latest stable release. Users usually have the latest stable release via an update, so they'll get a downgrade protection error.\", https://x.com/GrapheneOS/status/1848760198246088726\nGrapheneOS (@GrapheneOS), 2024, October 22. \"It's only possible to have a single Private Space and a single work profile, which must be in the Owner user. You can use both at once. You can make a bunch of secondary users though since we raise the limit on the number of users from 4 to 32.\", https://x.com/GrapheneOS/status/1848750455758983230\nGrapheneOS (@GrapheneOS), 2024, October 24. \"You can use Google Pay via a Pixel or Galaxy Watch paired with GrapheneOS. There are other tap-to-pay implementations available in some regions supporting GrapheneOS, and Curve Pay should be coming to the US soon.\", https://x.com/GrapheneOS/status/1849353061975601263\nGrapheneOS (@GrapheneOS), 2024, October 23. \"There are known issues with MMS in secondary users with Android 15 which we don't think are specific to GrapheneOS. You should always start debugging carrier issues by resetting cellular settings in Settings > System > Reset. If possible, remove SIM first. If the reset doesn't resolve it, check the APN settings and the cellular network settings such as VoLTE, VoWiFi, etc. It's possible you have VoWiFi enabled now when you didn't before and it isn't working for you. You'll need to go through settings to figure out a possible cause.\", https://x.com/GrapheneOS/status/1849146010246815986\nGrapheneOS (@GrapheneOS), 2024, October 23. \"They don't block 5G in any countries, that's a misconception about how things work. Pixel carrier configurations are gradually expanding to more carriers with additional features supported including in countries where the devices aren't currently being sold officially. Features known to have problematic compatibility issues such as 5G and VoLTE largely use a whitelisting system where Google has to test and certify networks are working. They have to regularly test it so it's not just a one time thing. That's why it's slow to expand universally.If carriers implemented these things properly according to the standards, it wouldn't work this way. Unfortunately, they do not follow the standards and use lots of shady equipment and software doing things wrong. Lots of quirks need to handled. We don't want to risk breakage. We plan to add more override toggles for users to force enable features like 5G when it's not whitelisted for their carrier. We aren't going to change the defaults from matching stock Pixel OS. It's not as if they do this to make their devices less useful, it's for stability.Having 4G instead of 5G isn't that big of a deal compared to potentially having major breakage causing missed calls, etc. People already have plenty of issues despite their conservative approach. There would be far more issues if they enabled everything carriers claim to support. Carriers supposedly having working 5G which is available with their carrier phones does not mean they implement standard 5G correctly and that it will work with every phone properly. There are major issues with how carriers handle VoLTE/VoNR and VoWiFi too.\", https://x.com/GrapheneOS/status/1848994520769143190\nGrapheneOS (@GrapheneOS), 2024, October 22. \"Android Auto is supported as an extension to sandboxed Google Play. It requires having sandboxed Google Play the the same user. It's not new with Android 15. We've supported it for a while. There are special toggles in Settings > Apps > Sandboxed Google Play to enable it. Android Auto won't currently work from a work profile or Private Space, just the Owner user or another top level user.\", https://x.com/GrapheneOS/status/1848798385366605892\nGrapheneOS (@GrapheneOS), 2024, October 22. \"Samsung is missing basic production support for an alternate OS where it's allowed to use the standard security features. They do have a lot of the Titan M2 secure element features with much less hardened secure elements. Some features are missing including very important ones.\", https://x.com/GrapheneOS/status/1848781303077916783\nGrapheneOS (@GrapheneOS), 2024, October 22. \"Note that there was a previous work profile feature which can still be used alongside the Private Space at the same time. Work profile had to be managed by a profile administration app intended to give control of it to a company deploying a configuration to devices. There are local management apps for a work profile which were essentially a poor man's Private Space before the feature existed. Private Space has better privacy/security and features. It has far more features in the UI and full support for a standalone lock method / encryption. It's possible to use a work profile at the same time as a Private Space. We expect most users won't be interested in work profile anymore unless they want to have 2 separate nested profiles. Work profiles are still odd and require a management app. Private Space is way nicer. It's only currently possible to make a single Private Space and it has to be in the Owner user. Same limitation applies to work profiles. It would be nice if it was possible to create multiple Private Spaces but it doesn't really fit into the UI using 1 Private tab everywhere.\", https://x.com/GrapheneOS/status/1848704263377055997\nGrapheneOS (@GrapheneOS), 2024, October 21. \"No, this is very improperly implemented and much better implementations exist which can be used via a VPN service in addition to using an actual VPN. RethinkDNS is one example. You're incorrect about this needing to be built into the OS or about root access being needed. LineageOS already heavily rolls back security but /e/OS is dramatically worse. They massively roll back privacy and security, go years missing important patches and mislead users. This is one of the clear examples of providing faulty marketing-focused features misleading users. There's nothing secure about /e/OS. Your screenshot also shows blatant false marketing misleading users about how privacy works and falsely claiming enumerating badness that's not even being done well stops all of it. You should tune out this noise from charlatans like /e/OS. Blocking a list of domains does not fundamentally improve privacy and cannot stop apps sharing data. Apps are perfectly capable of using the same domain for different purposes and contacting third parties from their server. They can also resolve names themselves. Apps do this. If you want to do enumerating badness, you can use a better implementation of it than this. You don't need us to include an implementation of it in GrapheneOS. It's already fully supported. Our approach is improving privacy on a fundamental level, not blocking some domains.\", https://x.com/GrapheneOS/status/1848352785462370658\nGrapheneOS (@GrapheneOS), 2024, October 21. \"CalyxOS is not a hardened OS. They reduce security rather than improving it. They don't take full advantage of the hardware security features and don't have any serious hardware security standards. Those Motorola devices are highly insecure. GrapheneOS is not empty marketing. Pixel 8 and later have support for 7 years from launch. That means they're much cheaper than a device which only gets 2 years of support. You should consider full lifetime of the device and the fact that there will be cheap used devices available with lots of remaining support. Those Motorola devices have poor hardware and firmware security from day one, do not support the hardware security features required by GrapheneOS and do not have much support time. We have no interest in providing yet another fake privacy/secure phone that's worse than iPhones. [...] Brand new devices with much worse hardware security, lack of proper updates and lower support time aren't cheaper than used Pixels. If you want cheap, buy used. Pixel 8a has 7 years of support from launch. What's going to compete with buying a used Pixel 8a in a year or two?\", https://x.com/GrapheneOS/status/1848335051202728078\nGrapheneOS (@GrapheneOS), 2024, October 21. \"This doesn't imply having all the privacy and security benefits of GrapheneOS. That would require having a device meeting all the requirements, fully porting all of our hardware-specific changes such as USB-C port control, kernel hardening, etc. and keeping up with the releases.\", https://x.com/GrapheneOS/status/1848335051202728078\nGrapheneOS (@GrapheneOS), 2024, October 21. \"Work profiles and Private Space are currently only designed to work in Owner, that's just how it was implemented. They're more robust features than secondary users. Main issue is they're expected to run while the user they're in is running when they're active. Owner always runs. A secondary user being 3 profiles running at the same time would be problematic for resource consumption especially since Owner always has to run. It would be perfectly fine on a Pixel 9 Pro which has 16GB of memory, but the Pixel 6a has 6GB of memory. That's quite a large range. Shelter is simply a way to enable / manage a work profile. It would be more efficient using Private Space since it doesn't require an app and it has nicer integration into the OS not relying on an app implementing features. You don't have to choose though, you can use both.\", https://x.com/GrapheneOS/status/1847093392938827976\nGrapheneOS (@GrapheneOS), 2024, October 18. \"GrapheneOS provides much better privacy and especially security compared to an iPhone. An iPhone has nearly the same kind of privacy issues with the default and mandatory services as a device with Google apps and services built in with privileged access. It's barely better. [...] Pixels already have very competitive hardware, firmware and software security with iPhones even before our massive improvements. Android is not one OS across a bunch of devices. It's an OS family. OEMs have their own operating systems based on the Android Open Source Project. [...]You can very easily buy phones without KYC. Is that what you're talking about? Apps can't access hardware IDs. Cellular of course requires an IMEI, EID for eSIMs and has other vendor-specific IDs but that doesn't imply it's tied to you. No mandatory account or anything either.\", https://x.com/GrapheneOS/status/1847208034558239074\nGrapheneOS (@GrapheneOS), 2024, October 18. \"GrapheneOS on a current generation Pixel is a huge security upgrade over a current generation iPhone. It's also a substantial privacy upgrade overall. It has no privacy invasive default connections, has much better VPN support, profiles, Sensors/Network toggles, etc. iOS has major advantages over Android in terms of privacy from apps but most of that isn't applicable to GrapheneOS due to Storage Scopes and Contact Scopes. iOS 18 contact access control was available in a more advanced form in GrapheneOS significantly earlier. iOS still has some features for providing privacy from apps which aren't yet available in GrapheneOS but there's quite a lot in GrapheneOS not available in there in this area too. In that specific area, they're about the same, but GrapheneOS is better for overall privacy. [...] Airplane mode works fine across them. Cellular involves uniquely identifying your cellular radio and cellular subscription to the network. In terms of security, we add more attack surface reduction features for the radio firmware, drivers, etc. There's a partnership between Samsung and Pixels that's resulting in much better hardening for their cellular radios along with vulnerability finding efforts: https://security.googleblog.com/2024/10/pixel-proactive-security-cellular-modems.html. There isn't really much to say about privacy, but there are security differences. As we've said multiple times previously, the move to Tensor from Snapdragon was a huge security upgrade in most ways but arguably initially a downgrade going from a Qualcomm cellular radio to Samsung. We wouldn't say that anymore, since Exynos radio security has improved a lot.\", https://x.com/GrapheneOS/status/1846971736064934268\nGrapheneOS (@GrapheneOS), 2024, October 17. \"Those devices are far less secure than a Pixel with GrapheneOS. Our recommendation in this thread was using a 2nd phone rather than a hotspot device which will generally have very poor security and tethering via USB rather than Wi-Fi: https://x.com/GrapheneOS/status/1846711406432849959. Location is available via having Wi-Fi or cellular. That is how network-based location works on both iOS and Android devices after all. Sensors and GNSS are not needed for location detection if you have either Wi-Fi or cellular. Also, cell networks know where their towers are. Cellular network knows where you are connect from via the connections to multiple towers. It doesn't need the client device to cooperate with it to locate it. The client device can detect where it is via network-based location without GNSS simply via cell tower signals. [...] A malicious cell tower could just pretend to be an existing one or submit their data to OpenCelliD. It's just like a MAC address. [...] If someone is making a fake cell tower, those are trivial things to do. There isn't really much value in trying to detect if it's a genuine cell tower other than against very clumsy / incompetent interception. Avoiding carrier-based calls and texts is important anyway.\", https://x.com/GrapheneOS/status/1846840246975037538\nGrapheneOS (@GrapheneOS), 2024, October 16. \"Cellular modem being connected via USB is far less secure than a tiny driver with strict IOMMU isolation. USB is a truly massive attack surface. [...] IMEI is not the only hardware identifier for the device available to the cellular network. Changing the IMEi alone isn't enough to hide the device identity from the network. It will only hide one commonly used ID rather than making the device not uniquely identifiable. Carrying around a Wi-Fi access point (AP) also adds a whole new trivial way to be tracked since an AP keeps the same MAC until it's recreated even on a privacy friendly device. Wi-Fi doesn't have MAC rotation features like Bluetooth Low Energy privacy extensions. [...] If you want to have cellular only on a secondary device we recommend still using a hardened device or at least one with a relatively modern cellular radio with decent hardening and proper security updates. Also, lots of improvements are being made to Exynos radio hardening. [...] There is no attempt by any of these cellular radios to provide some form of privacy though. They may sometimes expose a way to temporarily change the IMEI via a debugging command or unverified persistent state on storage but that doesn't mean they don't still have other ids.\", https://x.com/GrapheneOS/status/1846685642786591072\nGrapheneOS (@GrapheneOS), 2024, October 16. \"Yes, it's about attack surface reduction. 5G doesn't really reduce privacy significantly since triangulation of the location was entirely possible with 4G. It doesn't really change much beyond potentially making it a bit more precise, but the key word there is potentially. 5G is largely deployed as just a slightly enhanced form of 4G rather than fully taking advantage of all the capabilities. There's a huge range in the possible capabilities. We plan to provide the option of a 5G only mode in the near future as an additional option too. It didn't initially make sense to provide 5G only mode because it's more complex than 4G (more attack surface), wasn't widely available and had few advantages. In practice, people are mostly fine with the speed of 4G, but we need a new mode because 4G won't always be available. 4G and 5G are very similar and the baseline 5G is essentially enhanced 4G so it's unlikely 4G will actually be phased out any time soon. There's very little cost to supporting 4G in addition to 5G. We still want to be prepared for the future by having 5G only mode as an option.\", https://x.com/GrapheneOS/status/1846653946464440805\nGrapheneOS (@GrapheneOS), 2024, November 23. \"Worth noting you can use nearly every Android app from the Play Store on GrapheneOS rather than only the large open source app ecosystem. It's very easy to install and use with nearly the same UX as the stock Pixel OS beyond not having any Google apps/services built into the OS. The only apps you can't use are a very small number banning any alternate operating system via the Google Play Integrity API. Mainly applies to banking apps, but most do allow GrapheneOS in practice. Long term technical solution isn't possible, need a legal/regulatory one. Most people can use GrapheneOS without any significant compromise. If you want to move to more privacy respecting apps and services, that's separate. People using privacy invasive apps benefit most from our features like Contact Scopes, Storage Scopes, Sensors toggle, etc. [...] In Europe, most banks even have working tap-to-pay on GrapheneOS since they don't all use Google Pay. In the US, most banks use Google Pay and competition in that space is nearly dead. We're hopeful Curve Pay will change this. GrapheneOS is far more secure, not less secure...\", https://x.com/GrapheneOS/status/1860352772048032181\nGrapheneOS (@GrapheneOS), 2024, October 16. \"It has the same camera quality and features as the stock Pixel OS. When you the same app on both, it works the same way. If you want Pixel Camera, you can use it on GrapheneOS with full functionality.  Our own Camera app has basic HDR+, Night mode, multi-camera zoom, EIS, etc.\", https://x.com/GrapheneOS/status/1846650688425636340\nGrapheneOS (@GrapheneOS), 2024, October 16. \"That's completely wrong. Pixels have first class alternate OS support not voiding the warranty and support all the hardware-based security features for it. Play Integrity API exists across all devices with Google Play and is no better or worse on Pixels.\", https://x.com/GrapheneOS/status/1846461790164504733\nGrapheneOS (@GrapheneOS), 2024, October 15. \"Luckily, it's currently only a tiny subset of apps. It's mainly a subset of banking/financial apps including Google Pay with some incredibly strange outliers such as the McDonalds app and the Uber driver app. Location-based competitive games like Ingress are using it too. Basic anti-cheat is the only real use case for the Play Integrity API for the device integrity mode because it's so easy to bypass as a security feature via spoofing. GrapheneOS supports hardware attestation so they can still support it: https://grapheneos.org/articles/attestation-compatibility-guide. Location-based games like Ingress and Pokemon Go are experimenting with using it for trying to stop spoofing location, although they can't do anything about spoofing it via spoofing Wi-Fi APs or GNSS signals. That's too much effort for most people to do it for this though.\", https://x.com/GrapheneOS/status/1846407962467688841\nGrapheneOS (@GrapheneOS), 2024, October 15. \"Most Android OEMs neglect security in general. They're usually missing a bunch of important requirements from this list. They either don't provide alternate OS support or have half-baked partially working support for it since they treat it as a non-production hobbyist feature. Pixels have official production alternate OS support and it's important for them to have it since they're the Android Open Source Project reference devices used to develop Android. There's no reason to believe they would make alternate OS support worse. It's important to them.\", https://x.com/GrapheneOS/status/1846395087405654321\nGrapheneOS (@GrapheneOS), 2024, October 15. \"It's misinformation for promoting products through fearmongering. Those products have significantly worse isolation for the cellular baseband rather than better isolation. Attaching a cellular baseband via USB is far less secure than the typical approach that's being used. It isn't something that changed recently and we've never supported a device without IOMMU isolation for the cellular baseband. The early devices we supported didn't have proper isolation for Wi-Fi/Bluetooth yet but they did have it for the cellular baseband. It's very common for laptops and desktops to still lack isolation for these kinds of components including Wi-Fi and Bluetooth. It's much less common on phones, particularly for cellular, since the SoC vendors generally do a decent or good job dealing with it for this. Most Android OEMs are awful at security but this is handled by Qualcomm for Snapdragon devices so it's not up to them. Qualcomm historically provided cellular and GNSS radios as part of the SoC but since around 2015 increasingly provides Wi-Fi and Bluetooth too. It's isolated. [...] 10 years ago was 2014, the year we started our project, and there was good baseband isolation on Snapdragon. There was not generally proper IOMMU isolation for other components not included in the SoC. Qualcomm handled it well, but Android OEMs and other hardware vendors didn't. Since then, Android phone security has massively improved and proper isolation for most hardware components is the norm now. Laptop and desktop firmware/hardware security hasn't improved much and is increasingly far behind, but IOMMU isolation is slowly getting fixed there.\", https://x.com/GrapheneOS/status/1846372770692976760\nGrapheneOS (@GrapheneOS), 2024, October 15. \"Google Pay unfortunately bans using any OS not certified by Google including a modified stock OS for NFC payments. The other functionality of the relevant apps works fine. A small number of third party apps use the same Play Integrity API to ban using an alternate/modified OS.\", https://x.com/GrapheneOS/status/1846353784793882973\nGrapheneOS (@GrapheneOS), 2024, October 15. \"Google Play doesn't actually ever have root access but rather it has a bunch of privileged permissions giving powerful control/access. It also gets used as the backend for a bunch of OS services. It doesn't run in a regular app sandbox either or even the regular priv app domain.\", https://x.com/GrapheneOS/status/1846352990774370443\nGrapheneOS (@GrapheneOS), 2024, October 15. \"If you use sandboxed Google Play, you can see we have a Google Play services location request rerouting feature that's enabled by default and reroutes apps from asking Play services for location to asking the OS. We plan to do more of this.\", https://x.com/GrapheneOS/status/1846351625201275386\nGrapheneOS (@GrapheneOS), 2024, October 15. \"Aurora Store doesn't verify the signatures of apps it downloads from the Play Store so it's generally a better idea to use the sandboxed Play Store. It's best if apps are also officially published outside of the Play Store if they don't have a hard dependency on Google Play. By default, Aurora Store fetches shared Google account credentials from an Aurora Store service. This service has often gone down. These shared accounts it uses violate the terms of service since account sharing isn't allowed, and they can get banned for breaking those rules. Sharing Google accounts for the Play Store is also a potential security concern, but we haven't investigated what can be done with an account for the Play Store particularly via enterprise policy, etc. Not verifying signatures has much clearer security consequences though. Aurora Store can only really go down in terms of the default enabled shared accounts. If you aren't using that, then it doesn't need to connect to their services. Their services are likely down much less often now but the accounts can get banned and that will likely get wose.\", https://x.com/GrapheneOS/status/1846344817514848408\nGrapheneOS (@GrapheneOS), 2024, October 15. \"Librem 5 is a highly insecure device. It lacks the basic building blocks for privacy and security. It has a horrible set of component choices for security, doesn't get basic firmware updates and is missing very basic security features. It's the opposite of a secure device. You were also hardly endorsing Purism there but rather talking about it as someone interested in the topic. Purism has been heavily promoting their products with highly misleading and outright false claims for many years, and it's understandable people get misled by marketing. We have no interest at all in working with Purism or supporting their hardware. We're very interested in working with an OEM to make hardware to run GrapheneOS but there's absolutely no chance we work with them. They're anti-security and the false marketing causes serious harm. Yes, and they don't ship important firmware and microcode updates because of it. They also go out of the way to sabotage it by preventing updating important parts of the firmware from the OS, which instead has to be manually flashed with JTAG cables, etc. as if users will do it. They say update the firmware on their devices before shipping them, but then they don't get updates anymore. They essentially acknowledge that it's critical to security but then go out of the way to avoid it. This is the purpose and design of their products, not a side thing. Nearly the entire purpose of Purism products is avoiding updating the proprietary firmware from the OS. When they describe their hardware as open and freedom respecting, what they actually mean is the nearly entirely proprietary hardware/firmware works without the OS updating it. They make component choices largely based on using components which store firmware on their own flash storage and don't require the OS to load it. This is generally a bad security practice and it's no surprise that it leads to them choosing much less secure, sketchy components. [...] They not only avoid updating firmware/microcode, choose insecure components where they don't have to load it from the OS and block updating it but they even remove warnings in the Linux kernel and other software about out-of-date firmware and information on how to update it. Yes, that's what it does. The microcode/firmware is nearly entirely in the separate linux-firmware project. The vast majority of what they're doing is stripping out portions of the ability to update firmware/microcode and removing warnings about it. See https://phoronix.com/news/GNU-Linux-Libre-4.16-Released. This is also largely what the distributions based on the same ideology are doing. They don't only have an issue with shipping non-open-source software and in practice their main issue is actually with updating firmware/microcode that's going to be present regardless. They don't only have an issue with shipping the updates for it but even enabling people to obtain them and informing them about their device being insecure. They're actively covering up vulnerabilities to the point they're scrubbing it from log messages most people won't see. [...] They remove these kinds of warnings because they view it as promoting updating the firmware which is what they're against and the entire purpose of the project. They actively cover up security vulnerabilities and actively mislead people about it. It's ideological, that's all. They follow a religion based around software licensing, which tells them that closed source hardware and closed source firmware are out of scope as long as the firmware can't be updated. To them, a Wi-Fi SoC with no onboard firmware is bad. One with hard-wired firmware is good. They prefer hardware with the ability to update the firmware somehow disabled or broken, which they consider to be making it freedom respecting since the ability to update the closed source firmware is gone so they consider it to be hardware and out of scope for free software. It's standard FSF ideology. They could have taken the more logical approach considering open source hardware, firmware and software all important instead of only software. Instead, they came up with contrived justifications for why closed software is crucial but those aren't. Purism is catering to this ideology as their market niche. They attract true believers in it to work there too. They view this as a crucial ideological battle and therefore as with many political activists are fine with things like blatantly lying in order to advance their goals.\", https://x.com/GrapheneOS/status/1846323143285076435\nGrapheneOS (@GrapheneOS), 2024, October 15. \"We want to have our own hardware but we care deeply about providing an actually high level of privacy and security. It's incredibly hard to make a device with comparable security to a Pixel or iPhone, which are far ahead of other Android OEMs. We won't just make some awful white labelled device and falsely claim it's better as many others are doing. It would need to meet our official requirements and have a comparable quality of implementation, which makes it incredibly hard and beyond what most OEMs can really make. We've had the start of a partnership with 2 different OEMs but both lost interest in building a highly private and secure device. Both decided to take a cryptocurrency-related path instead where they were just providing a worse form of hardware wallet trying to get sales from it.\", https://x.com/GrapheneOS/status/1846261919768031240\nGrapheneOS (@GrapheneOS), 2024, October 15. \"Fairphone devices are highly insecure. They don't even get the basic privacy and security patches properly, have no secure element and in the case of the Fairphone 4 used publicly available signing keys for verified boot, attestation and other things which is truly ridiculous. Our hardware requirements are listed on our site and Fairphone doesn't even come close to providing them and has no plans to do it. Fairphone 5 was in many ways a regression, and they used an SoC with CPU cores from 2021 without current security features to save money on support. Fairphone doesn't really provide the kind of long term support they portray the devices as having. They portray shipping an update from 2022 in 2024 as 2 extra years of support compared to a device shipping it in 2022. That doesn't really make any sense and should be corrected. As with other non-Pixel Android OEMs, they do not ship the monthly and quarterly updates. They ship the yearly ones around a year or more late for new devices, which gets longer through the device's lifetime. The security backports are at least 1 month late, sometimes longer. They're missing the firmware and driver updates we need for GrapheneOS, proper support for the latest OS version (which is now Android 15) and a bunch of the security features. They're missing basics like working disk encryption for the average user not using a long passphrase.\", https://x.com/GrapheneOS/status/1846256555018133902\nGrapheneOS (@GrapheneOS), 2024, October 15. \"Our App Store has the Accrescent App Store included in it which we recommend for the apps available in it. One of the apps there is App Verifier for verifying manual downloads of apps. There' also the option of using Obtainium. F-Droid is more problematic than people realize. Our App Store is mainly used for our own apps and for users to install sandboxed Google Play, but we added Accrescent there too. Accrescent packages developer builds of apps and is going to be one of the best ways to obtain apps going forward. It has some useful apps already.\", https://x.com/GrapheneOS/status/1846249859759673695\nGrapheneOS (@GrapheneOS), 2024, October 15. \"LineageOS is a hobbyist project focused on broad device support and adding customization. It massively reduces security and stability. It's not really a privacy project and certainly not a security project but rather the opposite. It's not the same kind of thing as GrapheneOS.\", https://x.com/GrapheneOS/status/1846242655052972109\nGrapheneOS (@GrapheneOS), 2024, October 12. \"We plan to support other devices as soon as there are other devices providing a high level of security and proper alternate OS support. Our hardware requirements are listed here: https://grapheneos.org/faq#future-devices Currently, no other devices meet these requirements. We hope it changes.\", https://x.com/GrapheneOS/status/1845200201138700772\nGrapheneOS (@GrapheneOS), 2024, October 10. \"[...] extremely few apps use the Play Integrity API, mostly only banking/financial apps or competitive location-based games such as Ingress trying to block cheating. There are some extremely odd cases such as Authy, McDonalds and the Uber Driver (not passenger) app.\", https://x.com/GrapheneOS/status/1844540896647237659\nGrapheneOS (@GrapheneOS), 2024, October 10. \"It's a good thing to have lots of app and service options where people choose what they want instead of the OS bundling a huge amount of apps/services and giving them special integration unavailable to third party apps.\", https://x.com/GrapheneOS/status/1844539647516377286\nGrapheneOS (@GrapheneOS), 2024, October 10. \"F-Droid adds another trusted party for each app and has demonstrated they're extremely careless in regards to security and do not take it seriously. They do not follow security best practices or properly maintain the app or infrastructure. They attack things like app sandboxing. It is far better to use the official releases of the apps from developers obtained without going through F-Droid. Even for the tiny portion of apps F-Droid ships developer builds, they're still a trusted party for the initial install and they delay important patches. You can have privacy/security patches for apps indefinitely delayed until F-Droid sorts out getting their builds for app working again. There can and often are major disagreements with the app developer resulting in users not getting updates anymore. F-There are other options available. Look into App Verifier, Obtainium and Accrescent. F-Droid is not a good fit for people who care about privacy and security. Open source does not make things magically private and secure. Their approach and ideology conflicts with those things.\", https://x.com/GrapheneOS/status/1844535207644496240\nGrapheneOS (@GrapheneOS), 2024, October 10. \"/e/OS is not trying to build something remotely comparable to what we're doing. They're making a far less secure OS than Apple and Google with worse privacy from apps. They're also including their own privacy invasive apps/services including the OS default connections. It's based entirely around the idea that avoiding Google is what's valuable rather than having privacy. Apple already offers people an option for this and it's unclear what the far less private and secure /e/OS devices are meant to bring to the table compared to using an iPhone.\", https://x.com/GrapheneOS/status/1844530280742212046\nGrapheneOS (@GrapheneOS), 2024, October 9. \"A Nokia 3310 isn't a private or secure device but rather has more limited functionality. It's very important for privacy to use end-to-end encrypted calls and texts. Carrier-based calls and texts should be avoided since they're accessible to the carriers and also others. There's built-in support for monitoring calls and texts as part of the standard telephony system. A traditional phone has no private/secure communication option since it can't use an app for end-to-end encryption. There's still the same cellular location privacy issue too. A phone with support for using Signal, SimpleX, using Wi-Fi instead of only cellular, etc. is far better for privacy than a traditional cell phone. Traditional cell phones also have worse security against exploits. More functionality doesn't always mean less security. It's possible to make a more limited communication device with ONLY support for end-to-end encrypted calls and texts with no support for apps, insecure carrier-based calls and texts, etc. That can be done in a way that's more secure with a microkernel, etc. but it's not implied.\", https://x.com/GrapheneOS/status/1843912977071911004\nGrapheneOS (@GrapheneOS), 2024, October 9. \"It's possible to remove microphones and sensors usable as microphones as NitroKey offers for their NitroPhone product which is a modified Pixel with GrapheneOS. Can then attach a headset to the device. Removing only the microphones or providing a switch for those is incomplete. There are hardware products with incomplete switches for disabling audio recording since they're missing coverage of other sensors directly usable as microphones. Kill switches are a good concept as a last resort for a compromised device but need to be done properly to work well. Even with a way to block the cameras, microphones and other sensors from working, it's quite a disaster for a user's main personal device to get compromised. An attacker can still get all their data including photos, videos, documents, all the app login sessions, keys, etc. Even a properly designed kill switch for the cameras or overall audio recording can't offer any value while the user is actually using it. It makes sense to have a way of defending against a compromised device as a last resort, but a lot has already gone wrong at that point.\", https://x.com/GrapheneOS/status/1843911741857771619\nGrapheneOS (@GrapheneOS), 2024, October 8. \"[...] The most decentralized approach is each app using JobScheduler to schedule self-updates. It should ideally be done with a reusable library to avoid redoing the work in each app. Metadata can be signed/verified but relying on the standard package manager checks for this is fine. Android's OS package manager does signature verification with key pinning and downgrade protection. App just has to fetch it and make sure it has an app id matching itself so the server can't trick it into asking the user to install another app. The OS API can prevent this. [...]\", https://x.com/GrapheneOS/status/1843891080905011659\nGrapheneOS (@GrapheneOS), 2024, October 8. \"App Verifier shows how the initial downloads can be secured for apps published on websites with self-update systems. It includes a pinning database for signing key fingerprints along with manual verification. Pinning database work could become a separate collaborative project.\", https://x.com/GrapheneOS/status/1843889357524217877\nGrapheneOS (@GrapheneOS), 2024, October 8. \"An app store is useful to bootstrap the initial signing key pinning through the app store metadata being signed and containing a hash of the APKs. F-Droid is very problematic due to building and signing nearly all apps themselves on badly secured and outdated infrastructures. F-Droid adds an additional point of failure. It does not protect people from the developers putting in a hidden backdoor as many people wrongly believe. They simply build what's published automatically on a server and then sign it in batches regularly. F-Droid core team largely has very anti-security attitudes and has been extremely hostile to security researchers criticizing it including engaging in harassment towards the person who made https://privsec.dev and others including the GrapheneOS team. They've repeatedly engaged in thoroughly untrustworthy behavior including vulnerability cover ups and extreme forms of harassment towards security researchers, GrapheneOS developers and app developers. F-Droid needs to be replaced both due to technical and non-technical issues.\", https://x.com/GrapheneOS/status/1843883086683226340\nGrapheneOS (@GrapheneOS), 2024, October 8. \"Samsung flagships are the closest to meeting the security requirements among non-Pixel devices but completely ruin it with their lack of production alternate OS support. They cripple devices if you use another OS including disabling important security features. Not an option. There's no other decent device for us to support where we can provide a high level of security. It's not a limitation of GrapheneOS but rather those non-Pixel devices you want us to support are both much less well secured and lack proper first class alternate OS support.\", https://x.com/GrapheneOS/status/1843698531485528176\nGrapheneOS (@GrapheneOS), 2024, October 8. \"GrapheneOS won't be doing network location by default because we consider it privacy invasive and we have no connections to Google servers by default. However, this particular article is quite bad. It misattributes Google Play connections to Pixels and misinterprets a lot of it. Android devices with regularly integrated Google Play use Google's network-based location service by default with an opt-out toggle in the initial setup wizard. Google Play is a core part of those OSes and always has access to these things but does have a toggle for this service. Any Android OS with regular Google Play integration gives it massive control and access to data, not Pixels specifically. It makes a lot of connections but that isn't a meaningful way to quantify privacy invasiveness. That article involved cursory research + many assumptions. iOS has similar connections too. It has similar network location that's the default. We put a lot of care into avoiding privacy invasive default connections in GrapheneOS, but it's not something we have to put much work into. It's much more about what we choose not to include. We're working on implementing our own network location client and service right now which will be off by default with different choices. We're starting out with a choice between it being off (default), using our proxy to Apple's network location or directly Apple's service. We're also going to be hosting our own network location service. It will support downloading a regional database in addition to querying it, for entirely local offline location detection which works indoors without satellite reception. We'll be using Apple data + BeaconDB data. At the moment, GrapheneOS only has satellite-based location detection included but that's very fast due to PSDS and SUPL via our SUPL proxy. When we have our own location service, the default SUPL option can switch from a proxy to our own. It could even work locally eventually. It's already possible to use Google's network location on GrapheneOS but it involves assorted configuration to get it working since it requires turning off rerouting location requests to the OS, granting it access and opting in to their toggle since there's no OS integration.\", https://x.com/GrapheneOS/status/1843536159395475791\nGrapheneOS (@GrapheneOS), 2024, October 5. \"It's more than hyperbole. Commercial exploit developers and governments are successfully able to keep themselves able to break into iPhones and Pixels despite how much more secure they are than non-Pixel Android devices. Other devices are awful rather than these being incredible. Pixels have incredible security relative to non-Pixel Android devices because they're doing what every OEM should be doing as a bare minimum by putting lots of resources into exploit protections, memory safety, hardware-based security features, bug finding in multiple ways, etc. We list our hardware security requirements here: https://grapheneos.org/faq#future-devices. The most advanced feature there is memory tagging, which is a standard feature in standard ARMv9 Cortex cores + cache but not supported by non-Pixel devices and only used by GrapheneOS in production. There are no exotic features on our list, only very standard things which have poor adoption by most hardware vendors. Windows / Linux laptops are desktops are also dramatically worse at these things and are nowhere close to meeting even many of the most basic requirements.\", https://x.com/GrapheneOS/status/1842706078552953007\nGrapheneOS (@GrapheneOS), 2024, October 5. \"We certainly wouldn't describe GrapheneOS as being virtually impenetrable but it's a huge upgrade at defending against exploits compared to the stock Pixel OS. We benefit from all their security improvements but they tend to only rarely provide the same things we've shipped.\", https://x.com/GrapheneOS/status/1842476034333016221\nGrapheneOS (@GrapheneOS), 2024, October 4. \"Pixel 6 Pro launched with 5 years of support and is supported until October 2026 so it's not a very good choice for a new purchase but it's perfectly fine to keep using an existing one. 8th/9th generation Pixels have 7 years of support from launch instead of 5 for 6th/7th gen. 8th/9th generation Pixels are also significantly more secure when using GrapheneOS due to having hardware memory tagging (MTE), pointer authentication (PAC) and branch target identification (BTI). We use MTE to provide substantial protection from the majority of exploits.\", https://x.com/GrapheneOS/status/1842381685154873511\nGrapheneOS (@GrapheneOS), 2024, October 4. \"They're easily the most secure Android phones and are very competitive with iPhone security but neither is anything close to virtually impenetrable. Pixel 9 is a small step up for security over Pixel 8 mainly due to minor kernel mitigation improvements which will come to 8 later. Pixel 8 and Pixel 9 security will be nearly the same once the Pixel 8 moves to the same 6.1 LTS branch, which already officially supports the Pixel 8 but isn't being used in production for it yet. They might move 6th/7th gen to the newer kernel branch too, remains to be seen.\", https://x.com/GrapheneOS/status/1842471550114508948\nGrapheneOS (@GrapheneOS), 2024, October 4. \"While not in airplane mode, you're connected to the cellular network so the carrier knows your location. It doesn't particularly matter whether it's data-only in that regard. Not having texts and calls does reduce attack surface for exploits but we could just provide toggles.\", https://x.com/GrapheneOS/status/1842340014342238679\nGrapheneOS (@GrapheneOS), 2024, October 4. \"They don't need an exploit to get into a device running LineageOS since it lacks verified boot and several other important features. Bigger concern is being hit with remote attacks due to the frequent months of delays for basic security patches and rolled back security model. Lack of verified boot means it's possible to directly modify the OS on the flash memory with off-the-shelf tools. It wouldn't be particularly hard to do this while the OS is booted in After First Unlock state. Attacking SoC or RAM is far harder and being hardened over time. Remote attacks are a lot more important and regularly having months of delays for the most important security patches even for Pixels is a big deal. There will likely be months of delays for the updates after the release of Android 15 around October 15th as there were last year.\", https://x.com/GrapheneOS/status/1842277298894246118\nGrapheneOS (@GrapheneOS), 2024, October 3. \"You should really replace it with a supported device. Pixel 3a has been end-of-life for a long time and we stopped supporting it a while ago. We strongly recommend not using end-of-life devices even while we're still providing extended support such as 4th/5th generation Pixels.\", https://x.com/GrapheneOS/status/1841944450081161365\nGrapheneOS (@GrapheneOS), 2024, September 29. \"We're very focused on improving the core OS rather than apps people can replace. We also try to avoid bundling third party apps and services since almost none are aligned with our privacy, security and other values. Replacing or overhauling AOSP apps is a secondary thing for us. App Store, Auditor, Camera and Info are modern GrapheneOS apps. PDF Viewer is a GrapheneOS app but needs a major modernization and basic features added, but it's a uniquely secure Android PDF Viewer so it's very important to have it. Calculator, Clock, Contacts, Files, Gallery, Messaging and Phone are very primitive apps from the Android Open Source Project with minor GrapheneOS changes such as adding a modern call recording implementation to Phone. We need to overhaul or replace these, but cautiously. For Gallery, we want to outright replace it with another app. There are better open source image/video viewers available with acceptable licensing but none has a good enough editor yet. For Clock, it's unclear. For the others, we plan to begin a major overhaul in a few months. We don't have a text to speech app bundled because none of the open source ones is good enough and they have problematic licensing. The same applies to lots of other things. Organic Maps is in the Accrescent store available in our app repo, and we'd prefer not to bundle it. The issue with bundling a high quality app like Organic Maps with an acceptable license is that it might not always be our preferred option, and if we bundle it that takes away our ability to easily change our recommendation. We could add app recommendations instead.\", https://x.com/GrapheneOS/status/1840460774080422323\nGrapheneOS (@GrapheneOS), 2024, September 25. \"Satellite-based location via GNSS + A-GNSS works just as well on GrapheneOS as the stock OS. You're used to using the Google or Apple network location service where you're sending it nearby Wi-Fi networks, cell towers and Bluetooth beacons. It's not the default on GrapheneOS. Rerouting location requests apps make to Play services to the OS simply avoids needing to grant Location access to Play services and has no downside. It doesn't reduce how well it works. If you want to use Google's own network location, disabling it one of the required steps. There's no real advantage to disabling rerouting location requests and granting Location to Play services without also taking further steps to set up using their network location service unless you need some obscure Play services functionality requiring Location access. [...] SUPL currently only functions on either the stock OS or GrapheneOS when you have a carrier even though it could theoretically work without one. SUPL makes getting a satellite-based location lock much faster than without it. That could explain it. There are two forms of A-GNSS used by GrapheneOS and the stock OS: PSDS and SUPL. PSDS downloads static databases of predicted satellite locations, topographical data, etc. SUPL downloads a small database of cell towers for your carrier. Both use a GrapheneOS service by default. The main PSDS databases last around a week but having the latest data works better. As long as you have internet access, that should work well. SUPL is currently only activated when you have a carrier since it uses that to figure out what it should be downloading through it.\", https://x.com/GrapheneOS/status/1839035578807103585\nGrapheneOS (@GrapheneOS), 2024, September 21. \"A radio connected via USB has more attack surface from the OS exposed than the way the internal one is isolated because of the truly massive attack surface of USB. It's a different way of having an isolated radio rather than adding an additional layer of isolation to it.\", https://x.com/GrapheneOS/status/1837689250931757508\nGrapheneOS (@GrapheneOS), 2024, September 21. \"Pattern unlock is essentially a much worse version of a PIN where you can't pick from each value for each step which makes it much weaker and you also can't easily use a randomly generated one as we recommend. It also strongly encourages very weird patterns. It's badly designed. [...] We also mentioned that we're considering extending the duress PIN to working for SIM PIN entry instead of only for using a PIN as an unlock method for a profile. The ability to set both a duress PIN and duress password does not mean you need to set both. Configure what you want. If you have an eSIM or especially a physical SIM, it makes sense to configure a SIM PIN but we recommend not using the same PIN as one used to unlock a profile. Duress PIN doesn't apply to SIM PIN because it wasn't in the scope of the initial feature but we'll likely extend it.\", https://x.com/GrapheneOS/status/1837677713353478290\nGrapheneOS (@GrapheneOS), 2024, September 21. \"Pixels/iPhones do things a bit differently from most phones and have 3 separate chips for the cellular radio, Wi-Fi/Bluetooth and GNSS as opposed to everything being on the main SoC with a unified baseband for those. Qualcomm's approach isn't inherently less secure though. Any device with Wi-Fi or Bluetooth has a Wi-Fi or Bluetooth baseband. It's often a unified chip because they're very similar protocols and need a similar baseband chip / RTOS so it might as well be reused. Decent ones will have internal hardening included sandboxed processes. Bluetooth accessories have one too, not only devices supporting using them. Bluetooth accessories are usually quite bad at protecting against attacks and tend to be way behind on adopting current generation Bluetooth privacy/security or simply getting firmware patches. What's the logic for being very concerned about a cellular radio but not a Wi-Fi radio? They're isolated in the same way, or in the case of laptops/desktops often not really isolated at all right now. In many cases it's even same company making both and the same baseband/RTOS. Since there are 3 separate chips for cellular, Wi-Fi/BT and GNSS on the phones we support, it's technically possible to remove cellular and the phone will still function without it since they're built to be robust to partial hardware failures. Samsung builds both the main SoC and the cellular radio, so what would you gain by removing the cellular radio instead of using airplane mode? It's not very logical to believe Samsung is putting backdoors into an isolated radio rather than the more privileged main SoC.\", https://x.com/GrapheneOS/status/1837596458561741073\nGrapheneOS (@GrapheneOS), 2024, September 20. \"It's far easier for an attacker to do a supply chain attack on a tiny company outsourcing their manufacturing than compromising every iPhone or Pixel without discovery. There's serious value in using a mainstream device with a lot of scrutiny and research targeting it. It would be very difficult for any company to make a computer in any form factor with competitive security to an iPhone or Pixel. A device specifically built for running GrapheneOS couldn't realistically be more secure unless it was made by a very capable company like Samsung.\", https://x.com/GrapheneOS/status/1837160541745188938\nGrapheneOS (@GrapheneOS), 2024, September 19. \"Yes, it does. In terms of security features, Samsung is only missing a few things. In terms of quality of implementation of components like the secure element, it's much worse. Samsung also adds a massive amount of additional complexity and attack surface, ruining things a lot. [...] Samsung tends to lag behind by 2-3 years on shipping the new security features introduced by Pixels, sometimes much more such as with the secure element providing Weaver which shipped on Pixel 2 but Samsung has only had it for a couple years now.\", https://x.com/GrapheneOS/status/1836872006572384706\nGrapheneOS (@GrapheneOS), 2024, September 18. \"Samsung mostly just needs to allow us to support their hardware by not crippling it with other operating systems and making some of the security APIs work with them which wouldn't be very hard. Other OEMs would need to change far more about how they do things. [...] Samsung shipped a lot of it first but it ended up being replaced by standard features, although they do some things their own way. Samsung used to be the leader in security features in the Android world but Pixels overtook them and have been far ahead since the Pixel 3. Samsung was always good at implementing a bunch of security features, but never good at restraining themselves from adding tons of complexity and attack surface. Adding so much stuff to Android also ends up with them having lots of legacy cruft they are stuck keeping around. Their quality of implementation for the security features isn't as high as Pixels but they do have most of the same security features. They don't allow us to use them as Pixels do though. On Pixels, we get to use all of the hardware security features the stock OS does and more. We use hardware security features that are not actually used by the stock Pixel OS in production such as hardware memory tagging (MTE), disabling the USB-C port and pogo pins at a hardware level and pinning-based hardware attestation for our Auditor app among other things.\", https://x.com/GrapheneOS/status/1836565600237424937\nGrapheneOS (@GrapheneOS), 2024, September 18. \"GrapheneOS isn't being built for people who believe Google is a malevolent entity that's trying to harm them. It exists to provide devices with a high level of privacy and security for people who need it. It's not based on those kinds of theories about Google and countering that. [...] GrapheneOS is not about avoiding 1 particular company, and the best way to specifically avoid Google rather than obtaining better privacy, then you'd be avoiding them much more using an iPhone and Macbook. Google plays a huge part in development of the Linux kernel, LLVM, etc.\", https://x.com/GrapheneOS/status/1836544444629139468\nGrapheneOS (@GrapheneOS), 2024, September 18. \"Samsung goes out of the way to prevent an alternate OS from having full support for their hardware including blocking it from being reasonably secure... Non-Pixel devices do not meet our security requirements and the requirements which are missing are very important things.\", https://x.com/GrapheneOS/status/1836540301378249092\nGrapheneOS (@GrapheneOS), 2024, September 18. \"CopperheadOS is taking old snapshots of our code and pretending they made it. They make misguided changes and don't preserve the privacy and security provided by GrapheneOS. CopperheadOS is literally a scam and they're trying to harm the people who build what they're ripping off. CopperheadOS is used by people who got conned into paying for an insecure, poorly done closed source fork of an open source project that's available for free. Copperhead falsely claims to be partnered with orgs which have nothing to do with them since the split from us in 2018...\", https://x.com/GrapheneOS/status/1836341584268325296\nGrapheneOS (@GrapheneOS), 2024, September 18. \"We recommend using a different open source Gallery app. We haven't replaced AOSP Gallery with a fork of a better app yet. Here are 2 options: https://github.com/IacobIonut01/Gallery/blob/main/README.md https://github.com/deckerst/aves/blob/develop/README.md We need to decide on an app as the basis for an actual GrapheneOS Gallery app.[...] The issue we have with this is that we don't want the new app to be a regression in any significant way. We want to keep all the same basic editing functionality for flipping, rotating, color correction, etc. An extremely basic editor isn't enough. AOSP Gallery has aged extremely badly but back in the Android 4.x era before it stopped being actively developed, it was a very high quality app with a lot of functionality. Some of the functionality got stripped down over time and it gained bugs from API level changes, etc. AOSP Gallery got replaced by Google Photos which wasn't even a proper replacement for it and they abandoned it. In the Android 5.x era, AOSP Keyboard was an amazing keyboard which was the basis for Google Keyboard which later became Gboard and they stopped releasing sources. AOSP Keyboard had all the functionality of Google Keyboard at the time other than missing the swipe language library. It had the swipe UI but it was missing the library to make it work for any languages so it was inactive. Anyway, it's essentially the same code 10 years later. Gallery is even more than 10 years since it was last actively developed. The abandoned AOSP apps were actually all great apps back when the Android UI was a lot more primitive and the expectations were far lower. Swipe typing was Android exclusive and in Google Keyboard / Swype.\", https://x.com/GrapheneOS/status/1836289505797558457\nGrapheneOS (@GrapheneOS), 2024, September 18. \"GrapheneOS is not \"exclusive to Google\" but rather exclusive to reasonably secure devices with proper alternate OS support, currently only Pixels.\", https://x.com/GrapheneOS/status/1836286439383760979\nGrapheneOS (@GrapheneOS), 2024, September 19. \"The purpose of GrapheneOS is providing a high level of privacy and security, not specifically avoiding Google, and we are not hostile towards Google. We're against a some of what Google does like the Play Integrity API and support other things like the OpenTitan project, etc. Purpose of GrapheneOS is not specifically \"de-Googling\" and we do not use the term or promote the OS as existing for that purpose. We have to clarify it doesn't include Google apps/services on our home page because of many people asking but it's not in the main description.\", https://x.com/GrapheneOS/status/1836219245979447345\nGrapheneOS (@GrapheneOS), 2024, September 17. \"GrapheneOS has exactly the same camera features and functionality as the stock Pixel OS within any given app. Our own camera app doesn't have the full set of features offered by Pixel Camera but you can use Pixel Camera on GrapheneOS. Our app is quite good on GrapheneOS though. On GrapheneOS or the stock Pixel OS, our Camera app supports automatically using all of the cameras via zoom, Night mode, HDR+ and HDRnet for all image/video modes on Pixels including the preview, video stabilization (EIS), etc. It's fast and works fine. Definitely doesn't suck. If you want Portrait mode, H.265, RAW, Ultra HDR, etc. then you can use Pixel Camera on GrapheneOS. It has the same feature set on GrapheneOS as the stock Pixel OS. It works fine without additional apps but has extra features when Play services, AR services, etc. are present. [...] GrapheneOS has ALWAYS provided the same camera functionality and quality as the stock Pixel OS. Years ago, Pixel Camera wasn't allowed to use Pixel-specific hardware acceleration APIs on GrapheneOS since it's a regular sandboxed app but we added a toggle for that a long time ago. GrapheneOS has a default enabled toggle for allowing standard list of Google apps with TPU/GXP support to use the TPU/GXP acceleration. It doesn't give them access to any extra data so it's on by default and is only included for purity reasons to avoid people being upset.\", https://x.com/GrapheneOS/status/1836150331945967721\nGrapheneOS (@GrapheneOS), 2024, September 12. \"You still have the highest alert tier enabled. Android and iOS don't allow disabling presidential alerts, which are rebranded to other names outside the US. Canada uses the presidential alert tier for all alerts to purposely prevent people from opting out of any kind of alerts... There are regulations requiring not permitting disabling presidential alerts in a lot of countries. GrapheneOS isn't subject to those regulations and we choose to provide a toggle for presidential alerts. Canada abusing presidential alerts for every alert type was a major reason. Our device testing farm has almost 20 devices and receiving every single weather and amber alert across each one would be ridiculous. Canada abusing the system results in people ignoring it or finding a way to disable it. It is possible to disable it on any Android OS via ADB. Possible to disable wireless broadcast alert packages on Android with ADB to break receiving any alerts at all. In general, countries should only use presidential alerts for very extreme situations and shouldn't ever abuse it or run frequent tests. Test opt-out is important too.\", https://x.com/GrapheneOS/status/1834291015068557426\nGrapheneOS (@GrapheneOS), 2024, September 9. \"We don't recommend F-Droid or Aurora Store. We recommend getting apps from the Accrescent app store for the ones already available. Using Obtainium to get apps directly from developers is far from perfect but is less bad than trusting F-Droid as a third party building/signing. Aurora Store isn't a very secure way to get apps from the Play Store. For that purpose, we recommend the sandboxed Play Store feature supported by GrapheneOS for most use cases. APKs from the Play Store are mostly generated/signed by the Play Store and many contain Google Play. Since many of those apps contain Google Play SDK / libraries anyway, you're running Google Play code in the app sandbox by using apps from there anyway. Some of the Google Play libraries don't work without Play services but many including Ads and Analytics work without it. F-Droid can be used as a stopgap if you prefer it over getting apps from developers directly, but we strongly recommend considering migrating away from F-Droid in the long term. Aurora Store is an option, but has problems and isn't usually what we recommend for most people. [...] No, every APK has a whole file signature using a much more modern approach than PGP. Android's package manager verifies the signature of every installed APK. It enforces future updates being signed with the same key or a key authorized by it and disallows a lower versionCode. It's possible to verify key fingerprint to confirm it matches the official developer signing keys. AppVerifier supports doing this automatically for the database of key fingerprints it has included in the app. It also supports manually checking with a provided key fingerprint. F-Droid obtains the source code to build and sign the apps from where it's published on GitHub or elsewhere. If that's compromised, they'll build and sign compromised code. They don't do even cursory review and even if they did would not be a viable way to find hidden attacks. F-Droid signs nearly all the apps themselves, meaning they're another trusted party. You trust the developer, the platform where they downloaded the sources and F-Droid themselves compared to trusting the developer's own signed releases. Accrescent is better than that though. Accrescent publishes the developer signed releases but it has signed metadata itself so it secures the initial download/install without users having to use AppVerifier to check the key fingerprint themselves. You can still do that after installing if you want though. F-Droid greatly increases the risk of a supply chain attack. They do not review/audit the code they're packaging. They run trivial automated scans which attackers are aware of and can run themselves. It is highly unlikely to detect anything regardless. It's more attack surface. F-Droid adds the F-Droid developers along with their poorly maintained infrastructure and build environment as attack surface for all the apps you install from it. Getting one app from it is definitely not as risky as getting as much as you can from it since apps are sandboxed. F-Droid doesn't uphold basic security features and the security model for app sources. It doesn't keep up with current era security protections. They allow infrastructure and the build environment to get incredibly out-of-date. Their core developers have anti-security attitudes.\", https://x.com/GrapheneOS/status/1831053343520952611\nGrapheneOS (@GrapheneOS), 2024, September 9. \"For privacy and security, a late model Google Pixel phone running GrapheneOS is the best option available. The next best option is an iPhone in Lockdown Mode with Advanced Data Protection enabled.\", https://x.com/georgemaschke/status/1833149866161078661\nGrapheneOS (@GrapheneOS), 2024, September 4. \" [...]Regular apps in the Owner user don't have more access. It's not a traditional OS. [...]\", https://x.com/GrapheneOS/status/1831264235151626322\nGrapheneOS (@GrapheneOS), 2024, September 1. \"User profiles mimic having separate phones but they're still processes running within the same OS with certain shared system services. Each one can have their own VPN configuration but the underlying network is shared. It's currently possible for apps with the Network permission to communicate across profiles via localhost until we change that by adding a way to use a separate network namespace for each.\", https://x.com/GrapheneOS/status/1830629297541058656\nGrapheneOS (@GrapheneOS), 2024, September 1. \"Lockdown mode disables a fair bit of Apple service and browser attack surface. It doesn't appear to do anything to stop Cellebrite from getting into the device. Exploits will increasingly be focused on attack surface available with lockdown enabled, which is a limited feature. Apple doesn't support using an alternate OS on iPhones and iOS isn't open source so you're stuck with the level of security they provide you, which doesn't hold up well to sophisticated attackers. There's not really much you can do about it while using it as a regular smartphone. Cellebrite can exploit it to gain access to the OS but can't decrypt most of the data without brute forcing the PIN, and a Pixel 6 or later / iPhone 12 or later currently stops them brute forcing the PIN via Cellebrite Premium. Doesn't mean it can't be done at all. A Pixel with the stock OS has comparable security to an iPhone, but an iPhone has better privacy from apps. GrapheneOS is much better at privacy and security than either since we start from the same baseline as the stock Pixel OS (latest AOSP) and make substantial improvements.\", https://x.com/GrapheneOS/status/1830306938199949341\nGrapheneOS (@GrapheneOS), 2024, September 1. \"Might as well turn it off if it has no support for calls. We allow users to change all the standard settings for all carriers instead of having these toggles available only for specific carriers. We plan to add more toggles for further attack surface reduction and other changes.\", https://x.com/GrapheneOS/status/1830292727587885208\nGrapheneOS (@GrapheneOS), 2024, August 29. \"Data saver is global but you can toggle it from a secondary user. You can also use the per-app background mobile data restriction toggle. Data saver doesn't avoid switching to cellular but rather stops apps using it in the background so it won't matter for most of your apps. Apps with a foreground service are excluded, but many have configuration for this and as an example Google Play heavily avoids it by default.\", https://x.com/GrapheneOS/status/1830039710653108324\nGrapheneOS (@GrapheneOS), 2024, October 23. \"Emergency calls and alerts work fine without a SIM. It can connect to the network without authenticating and sharing radio IDs with no SIM. It would radio IDs if you actually make an emergency call. Use airplane mode if you want cellular off. No SIM is not cellular disabled. [...] Simply use airplane mode when you don't want to be connected to cellular instead of simply not having a SIM. Remove the quick tile so you can't enable cellular by accident. Having a SIM will not connect to connect when in airplane mode, but it can enable Wi-Fi calling/texting. Disabling a SIM via the UI disables authenticating with that carrier and using their network services. It is not what controls cellular being enabled or not. Cellular is not the only way carrier services can be used due to Wi-Fi calling/texting.\", https://x.com/GrapheneOS/status/1849201676151886026\nGrapheneOS (@GrapheneOS), 2024, August 27. \"It's an unsubstantiated claim which is contradicted by leaked information. People can claim anything has a backdoor without providing any evidence for it and it doesn't mean that it does. Huawei devices don't meet our technical security requirements: https://grapheneos.org/faq#future-devices. [...] Huawei devices don't meet our security requirements and can be much more easily compromised. Even if your threat model is heavily based around avoiding the US government, the recommendation makes little sense. Those devices are easier for the US and their allies to exploit. [...] Claiming Qualcomm, Apple, Google, etc. put in US government backdoors is completely unsubstantiated and contradicted by the available evidence including leaks. [...]\", https://x.com/GrapheneOS/status/1828281226433540347\nGrapheneOS (@GrapheneOS), 2024, August 27. \"GrapheneOS is of course not impenetrable and can be exploited by attackers. However, it's significantly harder to exploit from most attack vectors and a lot of attack vectors are outright closed. Can see from Cellebrite's leaked data on capabilities that they struggle with it. It's a lot harder to defend against an attacker with physical access than remote attacks so it makes a lot of sense than Cellebrite has so much success reliably exploiting devices. These tools are widely available to law enforcement internationally, not only western countries.\", https://x.com/GrapheneOS/status/1827805997148315994\nGrapheneOS (@GrapheneOS), 2024, August 23. \"It's developed by a small group of specific people with donations from a very wide range of people supporting our work. We're paying all of the people regularly working on it with those donations and people ones who aren't students are working on it as their full time job. GrapheneOS is an inherently anti-authoritarian project which protects people from both corporations and governments. It's not only doing one or the other. The exploits we're protecting people from are mainly used by governments even if corporations like NSO develop a lot of them.\", https://x.com/GrapheneOS/status/1827105301801386342\nGrapheneOS (@GrapheneOS), 2024, August 23. \"If you care at all about security, we strongly recommend using stock Pixel OS over LineageOS. Going months without half of the important severity patches after quarterly/yearly releases, having a bunch of security features disabled/bypassed, etc. is a downgrade from stock OS.\", https://x.com/GrapheneOS/status/1827038504146051286\nGrapheneOS (@GrapheneOS), 2024, August 23. \"That's not a virtual machine but rather a compatibility layer involving including an outdated version of the Android userspace for very incomplete app compatibility. SailfishOS lacks very basic privacy and security features and proper patches. It's not a private or secure OS.\", https://x.com/GrapheneOS/status/1826925727259701466\nGrapheneOS (@GrapheneOS), 2024, August 22. \"You can disable updates but it's not clear how you'll have a secure devices without updates. It's covered in the usage guide for OS updates and it's a simple toggle in the App Store for package updates. [...] If you disable updates, you won't be getting privacy/security patches fixing known vulnerabilities or the ongoing privacy/security improvements to the OS. Not much point being concerned about backdoors if you're leaving the front door open by missing all the security patches.\", https://x.com/GrapheneOS/status/1826769871994323229\nGrapheneOS (@GrapheneOS), 2024, August 21. \"It's definitely not going to be installed by default but we could decide to provide it in our app repository even though it's available in the Play Store. We currently don't provide apps which can simply be installed via sandboxed Google Play Store. [...] Only the official app is able to properly access the image processor and TPU. Those unofficial variants require the OS special casing them or they're very slow.\", https://x.com/GrapheneOS/status/1826333310790107264\nGrapheneOS (@GrapheneOS), 2024, August 20. \"Seedvault has a device-to-device mode which backs up the same data as the Google Play device-to-device transfer. Seedvault does have a poor UX and poor error handling but there aren't any significant issues with the underlying infrastructure, which is also used by Google Play. Restoring can be triggered after the initial setup with the existing Seedvault integration into GrapheneOS. It has been that way for a while. There's also a feature to restore data for apps after installing them again. We know the Seedvault implementation isn't very good. Replacing it is planned but we've got tons of higher priorities than this including adding important privacy and security features, replacing the AOSP sample apps and making features like our planned virtual machine management app and a unique kind of device management app. Our experience is that only local backup and restore using the home directory is reliable, and that the other options probably shouldn't even be included until they're going to be implemented much better. Most of the complexity of Seedvault is completely unnecessary. Since Seedvault has repeatedly had massive regressions including breaking backups in multiple ways and adding significant security vulnerabilities, we're very careful about updating to the newer upstream code. Several upstream developers try to blame it on us not updating enough. People have the same experience with it elsewhere as they do with the version included in GrapheneOS. We just don't like frequently updating it because they keep breaking things and we don't trust them not to add more obvious vulnerabilities and then not fix them for ages.\", https://x.com/GrapheneOS/status/1825982280415981827\nGrapheneOS (@GrapheneOS), 2024, August 20. \"They're heavily promoting the Play Integrity API to developers. It gets recommended by the Play Console and Android Studio. They present it as a security feature which it clearly isn't. It's at best capable of making ad fraud slightly harder which is the \"legitimate\" purpose. They clearly benefit from harming app compatibility on devices and operating systems not partnering with them and integrating Google apps/services with a deep level of privileged access. It's highly anti-competitive for them to be actively trying to break app compatibility. Google Messages partially uses the Play Integrity API to block users from their RCS service with the excuse being that it's to reduce spam. They claim RCS is open to others to use but yet Google Messages is the only serious option available and they're being anti-competitive. Google uses it to block using tap-to-pay on GrapheneOS despite it being far more secure. Meanwhile, a device from one of their partners with no security patches for years is permitted. They also use host card emulation instead of secure hardware for broader compatibility.\", https://x.com/GrapheneOS/status/1825906264804639194\nGrapheneOS (@GrapheneOS), 2024, August 19. \"Nothing significant has changed. Hardware acceleration for neural networks has been present for around 6 years and is a good feature which improves privacy by encouraging doing things locally instead of on a server. [...] GrapheneOS doesn't include any AI assistant or generative AI. If you want to use those kinds of features, you can install apps providing them and use them. The hardware acceleration works fine on GrapheneOS. It's not part of GrapheneOS so there's nothing for us to handle. There's nothing for us to remove or put toggles to control. If you don't want to use Gemini, then don't go out of the way to install and use AICore. It's possible it works fine when installed on GrapheneOS and we've had no feedback about any issues with it.\", https://x.com/GrapheneOS/status/1825664165568016846\nGrapheneOS (@GrapheneOS), 2024, August 20. \"There are certain things included in Play services which aren't tied to Google services, but it's not a lot. FIDO2, passkeys and the Play services TensorFlow Lite runtime are some major examples. A whole lot also moved into apps themselves via Jetpack (AndroidX).\", https://x.com/GrapheneOS/status/1824943169269621232\nGrapheneOS (@GrapheneOS), 2024, August 17. \"It disables 2G, 3G and 5G for minimal attack surface. 5G is still relatively new and being very actively developed so it has a lot more vulnerabilities than the simpler and far more battle tested 4G code. 2G and 3G are legacy and add unnecessary attack surface. We'll be adding a 5G-only mode as an option too, we just haven't gotten around to it yet. Eventually, 4G won't be universally available anymore. However, 4G and the baseline 5G standard share a lot in common so it might not get phased out in the same way as past standards.\", https://x.com/GrapheneOS/status/1824843276324614231\nGrapheneOS (@GrapheneOS), 2024, August 14. \"It should all work if you install the required apps. The only parts that are not going to work are features requiring special privileged access beyond access to using the TPU for hardware acceleration. For example, waking from sleep with a hotword requires privileged access.\", https://x.com/GrapheneOS/status/1823737888280080840\nGrapheneOS (@GrapheneOS), 2024, August 13. \"There's nothing for us to disable. GrapheneOS is based on the Android Open Source Project, not the stock Pixel OS. It doesn't include Google apps or services. Hardware acceleration for AI is available on essentially every mobile device and isn't something unique to Pixel devices. Pixels have best in class support for it, but it's hardly used by GrapheneOS. Hardware accelerated processing for images is used is the same way as the stock OS for merging multiple frames together when capturing photos. Night mode uses the same approach but brightens the photo. The image processing mostly isn't what people would refer to as AI since it mostly uses algorithms written by humans with parameters tuned by machine learning. There are parts which use neural nets including preview and video capture combining frames in a way that mimics HDR+. Hardware acceleration for generic AI software is available on GrapheneOS. Non-Google apps can use it via the standard AOSP APIs which weren't very capable but are being overhauled to provide LLM model acceleration instead of a lower level, hard to use neural net acceleration API. Certain Google apps know how to directly use the TPU on Pixels acceleration for local AI hardware acceleration, so Pixels have more of these features than elsewhere. Pixel Camera and Google Photos are the major examples, but other Google apps use it. It's a toggle on GrapheneOS. Since it doesn't have any direct privacy impact due to not giving any additional access to data, our toggle for Google apps using the TPU is enabled by default. It exists in case someone wants to reduce attack surface. In theory, there could be a GPU acceleration toggle too. [...] Settings > Apps > Special App Access > Special access to hardware accelerators for Google apps. It's a single toggle rather than a per-app toggle because only specific Google apps know how to use the TPU. Our toggle allows that list of specific Google apps to use the Pixel TPU.\", https://x.com/GrapheneOS/status/1823468140111548667\nGrapheneOS (@GrapheneOS), 2024, August 6. \"iPhones used to be much better but Pixels have had competitive security for years. Samsung is increasingly behind both but much better than Windows or desktop Linux. Samsung was a leader in Android security features but never had a very holistic approach and fell way behind. Pixels have heavily benefited from being very open to security research. They've ended up receiving by far the most external security research attention. Apple puts up lots of barriers to research but it doesn't slow down sophisticated attackers and is a negative, not positive. Samsung is still doing a lot better than non-Pixel Android devices but that's about it. They shipped a lot of security features first but they didn't maintain that lead. They also ship a lot of extra bugs with the huge amount of added code, and it delays full security updates.\", https://x.com/GrapheneOS/status/1820998705303941550\nGrapheneOS (@GrapheneOS), 2024, August 6. \"You can simply buy a Pixel (ideally 8th gen) in a regular store and install GrapheneOS with the easy to use web installer. It's easy to install yourself and we recommend that over buying a device with GrapheneOS. Don't buy a carrier phone since carriers can lock the device.\", https://x.com/GrapheneOS/status/1820895474653786343\nGrapheneOS (@GrapheneOS), 2024, August 6. \"Android isn't a specific operating system and you're not comparing one OS to another. You're comparing one company's devices to the devices from a whole bunch of companies using their own operating systems. Your comparison is inaccurate because you omit comparing to Pixels. You're operating under the typical misconception that Android is one operating system, which is wrong. Pixels don't run the same operating system as Galaxy devices. Even Galaxy devices don't really run the same operating systems as other Galaxy devices. Pixels have their own OS. Pixel OS is based on the latest Android Open Source Project release. The releases for both are synced together. Other OEMs could keep up with the latest AOSP releases too if they wanted for their own operating systems. They're significantly different operating systems though.\", https://x.com/GrapheneOS/status/1820825078784733511\nGrapheneOS (@GrapheneOS), 2024, August 6. \"Librem 5 is nearly entirely closed source hardware and firmware, but that is unrelated to why it's such an insecure device. Librem 5 lacks basics of firmware security updates and basic industry standard mobile security.\", https://x.com/GrapheneOS/status/1819178674798711213\nGrapheneOS, 2024, November 26. \"The sandboxed Google Play compatibility layer prompts for unrestricted background access with a dialog and it has a warning about it in Settings > Apps > Sandboxed Google Play if it's not allowed. It could more aggressively prompt about it than it already does but we didn't want to be too pushy to people who genuinely don't want to enable it.\", https://discuss.grapheneos.org/d/17669-sudden-loss-of-push-notifications-on-gos-on-pixel-7a/16\nGrapheneOS Website, 2024, November 26. \"Changes since the 2024110700 release: [...] raise maximum running users from the standard 3 to 4 for 6GB memory, 6 for 8GB memory, 10 for 12GB memory and 14 for 16GB memory\", https://grapheneos.org/releases#2024111700\nsplattergames, 2024, November 13. \"that doesn't help. Apps need to be reinstalled to initiate communication with Play Services.\", https://discuss.grapheneos.org/d/17339-rappi-app-trying-to-install-module-not-working/4\nNicolas2509, 2024, November 23. \"[...] The way to smooth your usage is to leave the system permission mic and even camera enabled and to deny these permissions at an app level. You can then leave mic enabled for phone and use \"ask every time\" for other apps. You then end up with something that works around the friction you are experiencing and still blocks apps getting mic access without your consent. [...]\", https://discuss.grapheneos.org/d/17616-question-related-to-incoming-calls\nGrapheneOS, 2023, June 15. \"Only downloading the update uses your internet connection. Verifying, installing, validating and finalizing does not. Finalizing takes significantly longer than the stock Pixel OS if you have a lot of apps since we use full ahead-of-time compilation instead JIT compilation with only very sparse use of ahead-of-time compilation based on JIT-generated profiles. This is also why installing/updating apps takes longer on GrapheneOS. It saves battery life and improves performance of the Java/Kotlin code at runtime compared to the stock OS approach, but that's not why we do it. It's an important security feature. Also, not having profiles of which parts of apps you use could be useful in some situations, but isn't why we do this.\", https://discuss.grapheneos.org/d/5616-grapheneos-version-2023061402-released/5\nWonderfall, 2022, July 4. \"What you're describing is very likely a minor inconvenience due to ART AOT compilation (ahead-of-time compilation for the Android RunTime). In other words, when apps are installed on GrapheneOS, their code is fully compiled. On traditional Android distributions, their code can be compiled on the fly (this is called JIT compilation for \"just in time\"). (By code I'm referring to Dalvik bytecode, but there's no need to get too technical.) AOSP makes the distinction between cold/hot code (\"hot\" = often used compared to \"cold\") and uses AOT/JIT depending on the situation, whereas GrapheneOS chooses to use ART AOT all the way. This results in better security and performance, even battery life when you're running apps. The inconvenience is that app installs can take longer, especially on a lower-end device. For most apps this shouldn't take more than a minute, but some can naturally take longer than others.\", https://discuss.grapheneos.org/d/175-app-installation-with-google-play-store-often-takes-a-really-long-time/2\nraccoondad, 2024, November 22. \"https://grapheneos.org/articles/attestation-compatibility-guide The signing key is different per device type\", https://discuss.grapheneos.org/d/17579-compare-boot-id/2\ngrapheneuser1234, 2024, November 21. \"I changed the setting to \"LTE\" from \"LTE only\" and the issue disappeared. Got a relevant question though, I earlier selected \"LTE only\" as it was claimed to be the most secure network option according to some online articles/opinions, so how does \"LTE\" compare in terms of security or attack surface? [...]\", https://discuss.grapheneos.org/d/17405-help-solution-for-complete-breakdown-of-the-phone-app/14\neddy44, 2024, November 17. \"[...] How do I stop a user session from automatically being stopped/ended after a long period (12h+)of non use. When I don't use my phone for extended periods of time, the user session is always closed and I'm back\" into the session of the owner\" (indicated by the user icon on the lock screen) and therefore missing notifications from the user's apps. [...]\", https://discuss.grapheneos.org/d/17441-how-to-stop-user-session-being-ended-after-a-long-period-of-non-use\nGrapheneOS (@GrapheneOS), 2024, November 18. \"[...] but did you enable Phone calls and SMS for the new profile?\" in \"WhatsApp fails to send SMS verification\", https://discuss.grapheneos.org/d/1891-whatsapp-fails-to-send-sms-verification/2\nGrapheneOS (@GrapheneOS), 2024, November 27. \"Our 2024111700 release overhauled the way sandboxed Google Play runs background services. Our GmsCompat app runs a foreground service and used to keep a connection open to Play services and Play Store. It now creates the connection on-demand when they need a foreground service. Toggling off background data usage for Play services and Play Store will now stop their background services being able to use data. That now blocks it from using mobile data when it's not running a foreground service since another foreground service isn't always using it anymore. Some users previously blocked background data usage for them and now they don't have FCM and other features working anymore. Similarly, some users never granted it Unrestricted battery usage which was required for FCM to work reliably but is now even more required than before. Apps can be set to one of 3 different battery modes: Restricted, Optimized (Default) and Unrestricted. Optimized heavily restricts the app running in the background based on how much it's used. Restricted nearly fully prevents apps running in the background on their own. Android 14 QPR2 changed this from a menu with the 3 options to a toggle for Restricted being on or off. The text next to the toggle is a button to open an inner menu with Optimized vs. Unrestricted. Users find this confusing and we're considering changing it back to how it was. Our sandboxed Google Play compatibility layer makes a notification requesting Unrestricted battery usage for Play services with a permission request dialog. The issue is that if users miss this or dismiss it, it's trickier than before to find the setting to enable it manually. We also show this as a detected issue in Settings > Apps > Sandboxed Google Play in the suggestions in shows. The issue is that some users may want the background usage heavily restricted for sandboxed Play services. We don't want to nag people who actually want it very limited. Our compatibility layer already shows a notification about this until users either grant Unrestricted battery mode for sandboxed Play services or press \"Don't show again\" to dismiss the notification permanently. It will show again if it's only dismissed without pressing that. It seems this isn't quite enough because we see a steady stream of people not granting Unrestricted battery mode and not realizing it's why their push notifications are delayed. It's valid to not grant it to it. It runs fine without it granted but it means push, etc. won't work. [...] Allow background usage toggle is not the same thing as the Unrestricted but rather Unrestricted is in the inner menu. Our compatibility layer requests it repeatedly after the initial install of Google Play unless you use \"Don't show again\". Only need to do it manually if you did. Google Services Framework (GSF) is treated as part of Play services (like other shared uid apps) and doesn't have fully separate permissions. Fresh installs of sandboxed Google Play no longer install GSF anymore but it shouldn't be removed for an existing install of it.\", https://x.com/GrapheneOS/status/1861659810036404316\nGrapheneOS Discussion Forum, 2023, April 12. \"I'm wondering if there is a way to prevent the system from automatically downloading and installing system updates? I mean updates to GrapheneOS itself, not to apps. I'd rather get a notification that there is an update and then run it myself manually. I tried disabling \"automatic system updates\" in the developer options (which worked for me in stock Android), but the system went ahead and updated itself anyway. [...] https://grapheneos.org/usage#updates-disabling. ou can also check for system updates from settings or you can find out from Announcements here but in all honesty it's in your interest to keep your OS as up to date as possible. [...] You can add the releases page to an RSS feed to be notified about new releases, and use https://grapheneos.org/usage#updates-disabling to disable the updater if you want. I wouldn't recommend it. You should be up-to-date, and keep in mind that you can control which type of network the updater uses to update. [...] The update process uses quite a bit of my battery and can also make my Pixel6Pro quite warm. So I'd prefer an option to delay it by 24-48 hours so that I can manually start the update later when I might have more battery or have a charger accessible. [...] Another good reason is that while updates do bring along security improvements, they also bring along the possibility of introducing unexpected bugs or even complete system failure. Consequently, as a matter of having a perfectly functional device, it is important to delay updates until a point when you have time to devote to dealing with unexpected problems. [...] For example, I recently deferred an update for 24 hours due to travel. [...] I have it disabled on my Pixel 4A5G as well. Just drains a huge amount of battery in the worst moments possible. Would all be no problem if there would be a \"only update when charging\" option in additon to \"unmetered only\"...\" in \"Is it possible to disable automatic system updates?\", https://discuss.grapheneos.org/d/4449-is-it-possible-to-disable-automatic-system-updates\nGrapheneOS Discussion Forum, 2024, March 15. \"Was curious if there had been any discussion about the coming powered off Find My Device changes with Android 15. Tried a few search terms but couldn't seem to find anything recent. I know we don't know everything about it yet but will there be a reliable way to turn it off? Seems like it runs outside of the OS. https://www.androidpolice.com/android-15-powered-off-finding-pixel-8/ [...] It won't be supported by GrapheneOS. We'd have to go out of the way to add support for it. Bluetooth still has to be configured and enabled by the OS to make it work that way. [...] Finding a lost or stolen device and being able to delete all data remotely is also part of security and privacy. It would be interesting if GOS could support this. [...] I think there are real situations when this can not be achieved, e.g. completely drained battery, Faraday cage, etc. Also, you need a trusted service/infrastructure. Not everyone can trust Google for example. GrapheneOS devs are putting efforts into hardening the possibility of the data being recovered, e.g. time-based auto-reboot feature to bring the encrypted data at rest. And this works every time. [...] Question in terms of privacy, is it ok to activate the FMD currently available on GOS, being able to delete and reset the phone in case of loss, theft or seizure of one's device is really interesting? [...] If I remember correctly the gps location will be encrypted end-to-end by Google Password Manager and can only read by the client or who has access to the password. However you have to provide more privileges to googles framework to ensure that FMD can continuously track the device. I never tried to remote wipe it, but ringing and location tracking worked. be aware that when you use only GPS tracking without WiFi/Bluetooth Search the accuracy is limited in buildings or bridges.\" in \"Android Find My Device when powered off\", https://discuss.grapheneos.org/d/11520-android-find-my-device-when-powered-off\nGrapheneOS Discussion Forum, 2024, November 3. \"I've created a secondary user profile, separated from the owner profile, for my google sandboxing. I've installed Google Maps though Google Play using a google account, but for some reason when I open Google Maps I can't see my position on the map. The app doesn't seems to be able to locate me, even though, Location is enabled + Precise Location is also enabled for both Maps and Google Services. I tried with 5G and Wifi with the same results. [...] have you disabled \"reroute location requests to the OS\" in the \"Sandboxed Google Play (work profile)\" settings under Settings -> Apps? Also, you can turn on Wi-Fi and Bluetooth scanning for a faster location lock under Settings -> Location -> Location services. I guess right now it's just awfully slow at getting a lock since it only works via GPS and not the Play Services.\" in \"GPS doesn't seems to work with Google Maps\", https://discuss.grapheneos.org/d/17058-gps-doesnt-seems-to-work-with-google-maps\nother8026, 2024, October 24. \"Just a reminder not to disable, uninstall, or change permissions for any \"system\" apps. Sometimes nothing bad happens, other times something like this does.\", https://discuss.grapheneos.org/d/16744-no-keyboard-on-lockscreen-on-pixel-9-pro-with-fossify-keyboard/9\nAeon, 2024, October 24. \"Settings -> System -> System update -> Set Permitted networks to Unmetered. By definition mobile data is an metered network.\" in \"How to turn off downloading updates on mobile data?\", https://discuss.grapheneos.org/d/16770-how-to-turn-off-downloading-updates-on-mobile-data/2\nSide Of Burritos (@sideofburritos), 2024, September 16. \"GrapheneOS: After 3 Years, This Is How I Install Apps on My ‚ÄúDe-Googled‚Äù Phone\", https://www.youtube.com/watch?v=IAoCfrqxIEg\nGrapheneOS Discussion Forum, 2022, June 24. \"Work profiles are convenient but that's about it. They should not be considered as a proper separate workspace, which user profiles are. Each user profile has its own encryption keys, making them more powerful if you want to strictly isolate data that apps should never access to. And as you said, you don't need a third-party app to enable and manage the feature. [...] By their very nature, user profiles provide totally different workspaces and GrapheneOS provides the option to put the entire profile at rest with an \"End session\" button. Encryption keys are protected by their lock method like you would expect. You don't get that flexibility with work profiles. In fact, each user profile has its own Weaver slot on the secure element. [...] GrapheneOS aims to make user profiles more convenient and the de facto choice for compartmentalization. By design, work profiles are also made to allow communication between apps on the user profile and the nested work profile. That can't happen with apps on different user profiles (even the with the mutually consented IPC that takes place in a user profile) unless they use network (which can be revoked by the network permission, including localhost). (Also to be clear, the app sandbox always applies whether you use work/user profiles or not. These profiles are not substitutes for the app sandbox.) [...] the device manager app has ownership over the data of the work profile it manages, and not yourself, fundamentally. You're trusting a third-party app with considerable permissions over that data. Work profiles were designed with BYOD deployments (bring your own device) in mind. [...] External documentation to learn more about user/work profiles: https://source.android.com/devices/tech/admin/multi-user https://source.android.com/devices/tech/admin/managed-profiles [...] it lets me have a second copy of apps (signed into different accounts), and be more sure I'm not accidentally giving permissions I didn't mean to. E.g. if I accidentally give permissions to see contacts, in the work profile I don't have any contacts. [...] If you want to use different accounts in the same application, different profiles are the way to go. However, you should be aware that using different identities in different profiles doesn't provide any privacy benefits without tweaking other things. It shouldn't be too hard to figure out that you are the same person using two accounts. [...] opted to use Shelter and install Google Play (and services) in the work profile only. Then I install any apps I need to get from Play in that profile. That way whatsapp, google services etc can't see my contacts, files, sms etc, except for what is in the work profile, which is nothing.\" in \"Work Profile vs. User Profile\", https://discuss.grapheneos.org/d/115-work-profile-vs-user-profile\nGrapheneOS (@GrapheneOS), 2024, September 19. \"Carrier-based calls/texts don't go through the user's VPN whether it's using the cellular connection or Wi-Fi. VoLTE, VoNR and VoWi-Fi are based on using SIP via IMS over an IPSec tunnel. The IPSec tunnel to the carrier is the VPN and there's always one. VoWi-Fi could theoretically go through another VPN but it would need to support sending IPSec traffic through it which is not usually the case. You can't trivially send IPSec through another VPN. Linux kernel doesn't support nesting IPSec itself and it causes MTU issues when doing it multiple machines / virtual machines. Connectivity checks are optional and inherently need to check the underlying networks. NTP is UDP and correct time is required for certificate verification including for a VPN so Android has it bypass the VPN, but we don't use NTP and we don't have our HTTPS network time bypass the VPN. There are also the DNS requests to the DNS resolver provided by the network in order to do these things and to connect to the VPN server that's being used. None of this is a leak but rather just how things are designed to work. It's not possible to send VoLTE through a VPN and VoWi-Fi would normally break, although it is technically possible for the VPN and the IPSec tunne lto be set up in a way that cooperates and manages to work with some types of VPNs but not most. It definitely can't go via Tor which can't even handle UDP.\", https://discuss.grapheneos.org/d/15856-are-vpn-leaks-a-ghost-of-the-past-from-now-on-grapheneos-version-2024091700/8\nDanielMicay, 2019, October 8. \"The verified boot hash is unique for every release of the OS. It's the verified boot key fingerprint which never changes, and it's enforced that the signing key never changes. It's fully expected that the hash changes and there's no reason to have an alert for it. There could be a way to get a notification, but I don't think it would be useful, since updating is frequent and fully expected. The verified boot hash is included as additional information and is useful for identifying the precise release currently being used, unlike the patch levels, which only change monthly. A notification for this would simply be a notification that a device has been updated, since the signing key is enforced by both verified boot and attestation verification, along with preventing downgrades. [...] You receive alerts when valid attestations weren't received during the chosen time period. It has nothing to do with settings. It would be possible to provide different kinds of alerts, but the current baseline alerts are only about whether the device has sent valid attestations within the time period.\", https://www.reddit.com/r/GrapheneOS/comments/df7htu/comment/f31nmos/\nIamZorro, 2024, September 27. \"[...] I had forgotten that that I turned on the mock location feature on for one use case and forgot to turn it off\" in \"GPS Bug : same position stuck since 1 week\", https://discuss.grapheneos.org/d/16061-gps-bug-same-position-stuck-since-1-week/3\nGrapheneOS (@GrapheneOS), 2024, October 22. \"Each profile including secondary users, work profiles and a Private Space have their own VPN configuration by design. It's a major part of why the feature exists. It means you aren't stuck with the same VPN exit IP for each profile tying them together.\", https://discuss.grapheneos.org/d/16647-vpn-not-work-in-private-space/5\nGrapheneOS (@GrapheneOS), 2023, October 8. \"For apps using FCM, you need Unrestricted battery mode for Play services. For apps using their own background push service, you need Unrestricted battery mode for the app. Many apps like Signal use FCM when available and their own push when it's not available. Apps doing their own push need to run a foreground service which must create a non-hidden persistent notification which the user can disable without breaking it. It will show up in active apps when the service is running, so you can see when it's running the push service. Google Play services does not create the notifications for apps. FCM is not a push notification service but rather a push messaging service. It delivers messages to the apps and they handle the messages. [...] All the apps you listed likely using FCM via Play services for push when it's available, and some of them will do their own push when it's not available. Their decision about using their own push may be done statically on first launch after install or app data reset rather than dynamically at each app launch. It depends on how the app chose to do it. [...] Also, the Play Store is enabled, right? Play services depends on the Play Store.\",\nGrapheneOS Discussion Forum, 2024, October 10. \"I was in the middle of an important discussion with my partner and all of a sudden the call drops and then i check the phone and i see the phone is restarting and starts updating. in the middle of the day it was 5pm. [...] In your updater app settings, you can make it so it doesn't automatically reboot after downloading the new update. Instead, when it's ready, you'll receive a notification prompting you to reboot, and the new update will only be applied when you next reboot.\", https://discuss.grapheneos.org/d/16374-reboot-in-the-middle-of-a-whatsapp-call-on-a-secondary-profile\nGrapheneOS Discussion Forum, 2024, October 4. \"i can access the backup settings from each profile. enter each time a different 12 word code. but i only have one usb disk [...] Backups of several profiles on one USB drive are possible. There is the folder ‚Äú.SeedVaultAndroidBackup‚Äù on the USB drive. And seedvault creates a separate subfolder for each profile. So that they can coexist without any problems. If you back up a profile for which a backup already exists, this existing backup is deleted and replaced by the new one, but the backups from the other profiles are not touched. Please note: if you switch between profiles, you must always disconnect and reconnect the USB drive, otherwise the new profile cannot access the USB drive. When restoring, simply enter the 12 words of the profile from which you want to restore the backup. There is no direct selection option. [...] Seedvauly backups tend to work better if you make them to the devices internal storage and then transfer them to external usb drive. They still often miss things so best not to fully rely on them and use alterative methods for important data.\", https://discuss.grapheneos.org/d/16201-seedvault-multiple-profiles-backups-on-same-disk\nGrapheneOS Discussion Forum, 2024, October 5. \"The solution was to reinstall aurora store. [...] This also solved the issue that molly did not regognise the GSF.\" in \"Pixel camera crash on pixel 9 pro\", https://discuss.grapheneos.org/d/16213-pixel-camera-crash-on-pixel-9-pro\nGrapheneOS (@GrapheneOS), 2024, September 24. \"[...] It would be great to have the option to deny a secondary profile all internet access. [...] There is a workaround for you, this also works for any work profile as well. Install a VPN [...] In the mentioned workflow VPN has to be installed in the user profile that is meant to not have internet access. In addition, settings> Network&internet> VPN> cogwheel next to Name of VPN Always-on VPN and Block connections without VPN has to be activated in the corresponding user profile.\" in \"Deny all internet access to secondary profile\", https://discuss.grapheneos.org/d/15987-deny-all-internet-access-to-secondary-profile\nGrapheneOS (@GrapheneOS), 2024, October 8. \"Regular apps can't access hardware identifiers. On GrapheneOS, Google Play is a regular sandboxed app. There's no way to grant hardware identifier access to regular apps. You'll just need to tell them your IMEI. Since you need to return the device anyway, you could just install the stock OS for this if you need to.\", https://discuss.grapheneos.org/d/7891-sharing-imei-with-google-account-for-warranty-claim/6\nGrapheneOS Website, Homepage, 2024, November 28. \"GrapheneOS is a privacy and security focused mobile OS with Android app compatibility developed as a non-profit open source project. It's focused on the research and development of privacy and security technology including substantial improvements to sandboxing, exploit mitigations and the permission model. It was founded in 2014 and was formerly known as CopperheadOS. GrapheneOS improves the privacy and security of the OS from the bottom up. It deploys technologies to mitigate whole classes of vulnerabilities and make exploiting the most common sources of vulnerabilities substantially more difficult. It improves the security of both the OS and the apps running on it. The app sandbox and other security boundaries are fortified. GrapheneOS tries to avoid impacting the user experience with the privacy and security features. Ideally, the features can be designed so that they're always enabled with no impact on the user experience and no additional complexity like configuration options. It's not always feasible, and GrapheneOS does add various toggles for features like the Network permission, Sensors permission, restrictions when the device is locked (USB-C / pogo pins, camera, quick tiles), etc. along with more complex user-facing privacy and security features with their own UX.\", https://grapheneos.org/\nGrapheneOS Website, Features overview, 2024, November 28. \"GrapheneOS is focused on substance rather than branding and marketing. It doesn't take the typical approach of piling on a bunch of insecure features depending on the adversaries not knowing about them and regressing actual privacy/security. It's a very technical project building privacy and security into the OS rather than including assorted unhelpful frills or bundling subjective third party apps choices.\", https://grapheneos.org/features\nGrapheneOS (@GrapheneOS), 2024, November 5. \"Using a VPN is helpful if you want to hide which servers you're connecting to from your ISP. You can hide that you're using GrapheneOS if you set the connectivity checks to Standard while using a VPN in at least the Owner user, no need to change or disable other connections. Mullvad is a good option tested on GrapheneOS by the developers including testing compatibility with our usage of hardware memory tagging on 8th/9th generation Pixels. Official WireGuard app is mostly fine but has unresolved invalid memory accesses caught by memory tagging.\", https://x.com/GrapheneOS/status/1853801135884132837\nGrapheneOS (@GrapheneOS), 2024, September 21. \"It's far better to use a VPN on each [device] to get a separate exit IP shared with a bunch of unrelated people for each in most cases. Tying all your devices together with the same IP hurts far more than it could possibly help. It gives more data rather than giving less data.\", https://x.com/GrapheneOS/status/1837705313677418733\nGrapheneOS Website, Features overview, 2024, November 28. \"The first line of defense is attack surface reduction. Removing unnecessary code or exposed attack surface eliminates many vulnerabilities completely. [...] The next line of defense is preventing an attacker from exploiting a vulnerability, either by making it impossible, unreliable or at least meaningfully harder to develop. [...] there's often a high performance, memory or compatibility cost to deploying them. [...] thus we offer toggles for users to choose the compromises they prefer instead of forcing it on them. [...] The final line of defense is containment through sandboxing at various levels: fine-grained sandboxes around a specific context like per site browser renderers, sandboxes around a specific component like Android's media codec sandbox and app/workspace sandboxes like the Android app sandbox used to sandbox each app which is also the basis for user/work profiles. [...] Remote code execution vulnerabilities are the most serious and allow an attacker to gain a foothold on the device or even substantial control over it remotely. Local code execution vulnerabilities allow breaking out of a sandbox including the app sandbox or browser renderer sandbox after either compromising an app/browser renderer remotely, compromising an app's supply chain or getting the user to install a malicious app. [...] Attack surface reduction: Greatly reduced remote, local and proximity-based attack surface by stripping out unnecessary code, making more features optional and disabling optional features by default (NFC, Bluetooth, UWB, etc.), when the screen is locked (USB, USB-C, pogo pins, camera access) and optionally after a timeout (Bluetooth, Wi-Fi).\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"USB-C port and pogo pins control: Our USB-C port and pogo pins setting protects against attacks through USB-C or pogo pins while the OS is booted. [...] The default is Charging-only when locked, which significantly reduces attack surface when the device is locked. After locking, it blocks any new USB connections immediately through either USB-C and pogo pins at both the hardware level via configuring the USB controller and also at the OS level in the kernel to provide a second layer of defense. It disables the data lines at a hardware level as soon as the existing connections end which happens right away if there were no USB connections. It also disables USB-C alternate modes including DisplayPort at both the OS and hardware level.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Sandboxed Google Play: GrapheneOS has a compatibility layer providing the option to install and use the official releases of Google Play in the standard app sandbox. Google Play receives absolutely no special access or privileges on GrapheneOS as opposed to bypassing the app sandbox and receiving a massive amount of highly privileged access. [...] Only apps within the same profile can use it, and they need to explicitly choose to use it. [...] Sandboxed Google Play is close to being fully functional and provides near complete compatibility with the app ecosystem depending on Google Play. Only a small subset of privileged functionality which we haven't yet ported to different approaches with our compatibility layer is unavailable. Some functionality is inherently privileged and can't be provided as part of the compatibility layer. By default, location requests are rerouted to a reimplementation of the Play geolocation service provided by GrapheneOS. You can disable rerouting and use the standard Play services geolocation service instead if you want the Google network location service and related features. Our compatibility layer includes full support for the Play Store. Play Store services are fully available including in-app purchases, Play Asset Delivery, Play Feature Delivery and app/content license checks. It can install, update and uninstall apps with the standard approach requiring that the user authorizes it as an app source and consents to each action.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Android Auto requires privileged access in order to work. GrapheneOS uses an extension of the sandboxed Google Play compatibility layer to make Android Auto work with a reduced level of privileges.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Network permission toggle: GrapheneOS adds a Network permission toggle for disallowing both direct and indirect access to any of the available networks. The device-local network (localhost) is also guarded by this permission, which is important for preventing apps from using it to communicate between profiles. [...] To avoid breaking compatibility with Android apps, the added permission toggle is enabled by default. However, the OS app installation UI has been extended to show the toggle as part of the installation confirmation page so users can disable it when installing an app. [...] When the Network permission is disabled, GrapheneOS pretends the network is down. [...] This results in apps handling it as if the network is down rather than crashing or showing errors from trying to use the network and being unable to do it.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Sensors permission toggle: disallow access to all other sensors not covered by existing Android permissions (Camera, Microphone, Body Sensors, Activity Recognition) including an accelerometer, gyroscope, compass, barometer, thermometer and any other sensors present on a given device. When access is disabled, apps receive zeroed data when they check for sensor values and don't receive events. GrapheneOS creates an easy-to-disable notification when apps try to access sensors blocked by the permission being denied. This makes the feature more usable since users can tell if the app is trying to access this functionality. To avoid breaking compatibility with Android apps, the added permission is enabled by default.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Storage Scopes: GrapheneOS provides Storage Scopes as a fully compatible alternative to the standard Android storage permissions. Instead of granting storage permissions, users can enable Storage Scopes to make the app assume that it has all storage permissions that it asked for. On Android, an app that doesn't have any storage permissions is still allowed to create files and directories, and is allowed to access the files that it created. Users can optionally add files and directories as storage scopes to permit the app to access files created by other apps.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Contact Scopes: GrapheneOS provides Contact Scopes as an alternative to granting the Contacts permission. By default, it acts as if the contacts list is empty and users can grant different kinds of access to specific contacts or groups of contacts.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Closed device identifier leaks: [...] Our secure application spawning system primarily exists to significantly improve protection against exploitation. However, it also improves privacy. On a device without our secure application spawning system, the secrets used for probabilistic exploit mitigations such as ASLR are usable as device identifiers persisting until reboot. This is an easy way to identify the device from apps in different profiles. [...]\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"PIN scrambling: GrapheneOS adds a toggle for enabling PIN scrambling to raise the difficulty of figuring out the PIN being entered by a user either due to physical proximity or a side channel. PIN scrambling is applied to both the lock screen and SIM PIN/PUK.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Supports longer passwords: GrapheneOS supports setting longer passwords by default: 128 characters instead of 16 characters. This avoids the need to use a device manager to enable this functionality. This feature allows users to make use of diceware passwords if they don't want to depend on the security of the secure element which provides very aggressive throttling and offers a high level of security even for a random 6 digit PIN.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Auto reboot: [...] A countdown timer is started each time the device is locked, and the device will reboot if a successful unlock doesn't occur before the timer reaches zero. Unlocking any profile cancels the timer, not just the Owner profile. [...] This feature doesn't apply when the device is in \"Before First Unlock\" state, meaning that it will not lead to the device continuously rebooting, as data is already at rest.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Duress PIN/Password: GrapheneOS provides users with the ability to set a duress PIN/Password that will irreversibly wipe the device (along with any installed eSIMs) once entered anywhere where the device credentials are requested (on the lockscreen, along with any such prompt in the OS). The wipe does not require a reboot and cannot be interrupted. [...] Note that if the duress PIN/Password is the same as the actual unlock method, the actual unlock method always takes precedence, and therefore no wipe will occur.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Improved user profiles: Android's user profiles are isolated workspaces with their own instances of apps, app data and profile data (contacts, media store, home directory, etc.). Apps can't see the apps in other user profiles and can only communicate with apps within the same user profile (with mutual consent with the other app). Each user profile has their own encryption keys based on their lock method. [...] GrapheneOS raises the limit on the number of secondary user profiles to 32 (31 + guest) instead of only 4 (3 + guest) to make this feature much more flexible. GrapheneOS also enables support for logging out of user profiles without needing a device manager controlling the device to use this feature. Logging out makes profiles inactive so none of the apps installed in them can run. It also purges the disk encryption keys from memory and hardware registers, putting the user profile back at rest. GrapheneOS adds a toggle to the user management settings for disabling secondary user app installation. [...] This allows installing an app in a secondary user that's already installed in the Owner user without needing to download it again. This helps a lot with using the toggles added for disabling app installation by secondary users. GrapheneOS supports forwarding notifications from users running in the background to the currently active user.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"GrapheneOS app repository: GrapheneOS includes our own security, minimalism and usability-focused app repository client for using our first-party app repository. [...] it will be used to distribute first-party GrapheneOS builds of externally developed open source apps with hardening applied.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"User installed apps can be disabled: GrapheneOS adds support for disabling user installed apps instead of only being able to disable system apps. This allows users to completely prevent one of the apps they've installed from being able to run without being forced to uninstall it and lose their app data.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Improved VPN leak blocking: GrapheneOS greatly improves Android's protection against VPN leaks for both the built-in VPN support and VPN apps with the standard \"Block connections without VPN\" toggle enabled. [...] Android VPN configuration is split up for each profile which means work profiles, Private Spaces and secondary users have their own VPN configuration which is a fantastic privacy feature. [...] Finding and resolving all forms of VPN leaks is one of our top priorities at the moment and we don't currently consider this to be a complete feature due to less severe additional issues we've discovered.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Seamless automatic OS update system that just works and stays out of the way in the background without disrupting device usage, with full support for the standard automatic rollback if the first boot of the updated OS fails.\", https://grapheneos.org/features\nGrapheneOS Website, Features overview, 2024, November 28. \"Enable the \"Always-on VPN\" and \"Block connections without VPN\" toggles for VPNs by default.\", https://grapheneos.org/features\nGrapheneOS Website, Install, 2024, November 28. \"GrapheneOS has two officially supported installation methods. You can either use the WebUSB-based installer recommended for most users or the command-line installation guide aimed at more technical users. We strongly recommend using one of the official installation methods. Third party installation guides tend to be out-of-date and often contain misguided advice and errors. [...] The web-based installation approach avoids needing any software beyond a browser with WebUSB support and you can still avoid trusting our server infrastructure by checking the verified boot key hash.\", https://grapheneos.org/install/\nGrapheneOS Website, Usage guide, 2024, November 28. \"Storage Scopes: GrapheneOS provides the Storage Scopes feature as a fully compatible alternative to the standard Android storage permissions. Storage Scopes can be enabled only if the app doesn't have any storage permission. Enabling Storage Scopes makes the app assume that it has all of storage permissions that were requested by it, despite not actually having any of them. This means that the app can't see any of the files that were created by other apps. The app is still allowed to create files and directories, same as any other modern app that doesn't have any storage access permission. [...] Optionally, users can specify which of the files created by other apps the app can access. Access can be granted to a specific file or to all files in a directory.\", https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"Contact Scopes: On Android, contact access is controlled with an all-or-nothing Contacts permission, which grants both read and write access to all contacts stored on the device. A lot of apps (e.g. popular messaging apps) refuse to work unless the Contacts permission is granted. GrapheneOS provides the Contact Scopes feature as an alternative to granting the Contacts permission. Enabling Contact Scopes makes the app assume that it has the Contacts permission, despite not actually having it. By default, an app that has Contact Scopes enabled is not allowed any kind of contact access. [...] Single contact. Access is granted to all contact data, except contact photo. Contact group (\"label\"). Equivalent to granting access to all contacts in the group.\", https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"Update Security: The update server isn't a trusted party since updates are signed and verified along with downgrade attacks being prevented. The update protocol doesn't send identifiable information to the update server and works well over a VPN / Tor. GrapheneOS isn't able to comply with a government order to build, sign and ship a malicious update to a specific user's device based on information like the IMEI, serial number, etc. The update server only ends up knowing the IP address used to connect to it and the version being upgraded from based on the requested incremental.\", https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"Updates: Disabling: It's highly recommended to leave automatic updates enabled and to configure the permitted networks if the bandwidth usage is a problem on your mobile data connection. However, it's possible to turn off the update client by going to Settings > Apps, enabling Show system via the menu, selecting System Updater and disabling the app. If you do this, you'll need to remember to enable it again to start receiving updates.\", https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"Web Browsing: GrapheneOS includes our Vanadium subproject providing privacy and security enhanced releases of Chromium. Vanadium is both the user-facing browser included in the OS and the provider of the WebView used by other apps to render web content. [...] Chromium-based browsers like Vanadium provide the strongest sandbox implementation, leagues ahead of the alternatives. It is much harder to escape from the sandbox and it provides much more than acting as a barrier to compromising the rest of the OS. Site isolation enforces security boundaries around each site using the sandbox by placing each site into an isolated sandbox. [...] We recommend against trying to achieve browser privacy and security through piling on browser extensions and modifications. Most privacy features for browsers are privacy theater without a clear threat model and these features often reduce privacy by aiding fingerprinting and adding more state shared between sites. Every change you make results in you standing out from the crowd and generally provides more ways to track you. [...] For sensors, the Sensors app permission added by GrapheneOS can be toggled off for the browser app as a whole instead.\", https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"Camera: GrapheneOS has the same camera capabilities and quality as the stock OS. It will match the stock OS when comparing the same app on each OS. GrapheneOS uses our own modern Camera app rather than the standard AOSP Camera app. GrapheneOS Camera is far better than any of the portable open source camera alternatives and even most proprietary camera apps including paid apps. On Pixels, Pixel Camera can be used as an alternative with more features. [...] GrapheneOS includes our own modern camera app focused on privacy and security. It includes modes for capturing images, videos and QR / barcode scanning [...] Pixel Camera (previously known as Google Camera) can take full advantage of the available cameras and image processing hardware as it can on the stock OS and does not require GSF or sandboxed Google Play on GrapheneOS. Direct TPU and GXP access by Google apps including Pixel Camera is controlled by a toggle added by GrapheneOS and doesn't provide them with any additional access to data. The toggle exists for attack surface reduction.\", https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"Wi-Fi and Bluetooth scanning for improving location detection are disabled by default, unlike the stock OS. These can be toggled in Settings > Location > Location services > Wi-Fi and Bluetooth scanning. These features enable scanning even when Wi-Fi or Bluetooth is disabled, so these need to be kept disabled to fully disable the radios when Wi-Fi and Bluetooth are disabled. GrapheneOS itself doesn't currently include a supplementary location service based on Wi-Fi and Bluetooth scanning. These options impact whether apps such as sandboxed Google Play are able to use the functionality if you grant them the Location permission. GrapheneOS plans to eventually include an OS service based on local databases rather than a network-based service giving the user's location to a server whenever location is being used.\", https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"LTE-Only mode: If you have a reliable LTE connection from your carrier, you can reduce attack surface by disabling 2G, 3G and 5G connectivity in Settings > Network & internet > SIMs > SIM > Preferred network type. Traditional voice calls will only work in the LTE-only mode if you have either an LTE connection and VoLTE (Voice over LTE) support or a Wi-Fi connection and VoWi-Fi (Voice over Wi-Fi) support. VoLTE / VoWi-Fi works on GrapheneOS for most carriers unless they restrict it to carrier phones. Some carriers may be missing VoWi-Fi due to us not including their proprietary apps. Please note that AT&T users may see \"5Ge\" being used when LTE Only mode is enabled as AT&T intentionally mislabel LTE services as \"5Ge\" to mislead users. This feature is not intended to improve the confidentiality of traditional calls and texts, but it might somewhat raise the bar for some forms of interception. It's not a substitute for end-to-end encrypted calls / texts or even transport layer encryption. LTE does provide basic network authentication / encryption, but it's for the network itself. The intention of the LTE-only feature is only hardening against remote exploitation by disabling an enormous amount of both legacy code (2G, 3G) and bleeding edge code (5G).\", https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"Google Play: The simplest approach is to only use the Owner user profile. Apps installed in the Owner profile are sandboxed the same way as everywhere else and don't receive any special access. If you want to choose which apps use Google Play rather than making it available to all of them, install it in a separate user or work profile for apps depending on Google Play. You could also do it the other way around, but it makes more sense to try to use as much as possible without Google Play rather than treating not using it as the exceptional case. [...] Signing in into a Google account is optional, unless you want to use features depending on being signed into an account. For example, some apps use Google account authentication instead of their accounts having a username and password. The Play Store requires being signed into an account in order to install apps or use in-app purchases. [...] The Play Store provides many services used by apps including Play Asset Delivery, Play Feature Delivery, in-app purchases and license checks for paid apps. The Play Store app is also the most secure way to install and update apps from the Play Store. Our compatibility layer has support for Play Games Services which you can obtain by installing Google Play Games from the Play Store. Many games on the Play Store depend on having Google Play Games installed. [...] The compatibility layer has a configuration menu available at Settings > Apps > Sandboxed Google Play. By default, apps using Google Play geolocation are redirected to our own implementation on top of the standard OS geolocation service. You don't need to grant any permissions to Google Play or change any settings for working location in apps using Google Play geolocation due to our rerouting feature. If you want to use Google's network location service to provide location estimates without satellite reception, you can disable the \"Reroute location requests to OS APIs\" toggle and grant what it requires to provide network location. You will need to grant \"Allow all the time\" Location access to Google Play services along with the Nearby Devices permission for it to have all the access it needs. You need to use the \"Google Location Accuracy\" link from the sandboxed Google Play configuration menu to access the Google Play services menu for opting into their network location service, otherwise this is all pointless. It will send the nearby Wi-Fi and cellular networks provided via the Location and Nearby Devices permissions to their service to retrieve a location estimate. In order to fully take advantage of Wi-Fi and Bluetooth scanning, you also need to enable the scanning toggles in Settings > Location  > Location services which are disabled by default and control whether apps with the required permissions can scan when Wi-Fi and Bluetooth are otherwise disabled. Re-routing location to the OS geolocation service will use more power than using the Google Play geolocation service since we do not provide a network-based location service and implement it via GNSS / A-GPS only. In the future, we plan on providing a pseudo-network geolocation service for the OS by using a local database of cell towers, and the location redirection feature can also make use of this future OS implementation for network location requests once it's available.\", https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"eSIM: [...] in order to manage and add eSIMs, proprietary Google functionality is needed. This is fully disabled by default. eSIM support on GrapheneOS doesn't require any dependency on Google Play, and never shares data to Google Play even when installed. It won't connect to a Google service unless the carrier uses one themselves. eSIM support can be enabled in Settings > Network & internet > eSIM support. The toggle is persistent across every boot. Note that if the eSIM installation process does not progress past the \"Checking network info...\" stage despite having a stable Internet connection, you may need to call the USSD code ##4636##* and then enable DSDS in the menu that is presented.\"*, https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"Android Auto: GrapheneOS provides an option to install and use the official releases of Android Auto. Android Auto requires privileged access in order to work. GrapheneOS uses an extension of the sandboxed Google Play compatibility layer to make Android Auto work with a reduced level of privileges. To install Android Auto, use the GrapheneOS App Store. Android Auto can't be installed through the Play Store or other app sources. Android Auto depends on sandboxed Google Play, you'll be prompted to install it if it's not already installed. After installation, Android Auto has to be set up from the Settings > Apps > Sandboxed Google Play > Android Auto configuration screen, which contains permission toggles, links to related configuration screens, configuration tips, and links to optional Android Auto dependencies. The permission toggles ask for a confirmation before turning on. The confirmation popup explains what access each permission toggle provides. By default, Android Auto is not granted any kind of privileged access. It's treated the same way other apps are treated. In order to work, Android Auto has to be granted baseline permissions for wired or wireless Android Auto. Wired Android Auto requires far less access than wireless Android Auto does. Baseline permissions are controlled by the \"Allow permissions for wired / wireless Android Auto\" toggles. For some cars, baseline permissions for wireless Android Auto are needed even when using wired Android Auto. Therefore, if wired Android Auto is unable to connect to the car with only wired permissions granted, try granting wireless permissions instead. To forward notifications from the device to the car, Android Auto has to be allowed notification access. The notification access settings are linked below the permission toggles. In order to show up in the Android Auto car interface, apps have to be installed from the Play Store and include Android Auto support.\", https://grapheneos.org/usage\nGrapheneOS Website, Usage guide, 2024, November 28. \"Please note that in some regions, LTE is referred to as 4G. Generally 5G, SMS, MMS, Calls and VoLTE will work fine on GrapheneOS with officially supported carriers by Google. Wi-Fi calling may vary due to a reliance on proprietary Google apps which GrapheneOS does not ship. [...] If you are having problems sending or receiving SMS/MMS messages, we suggest that you perform the following steps: Deregister your phone number from Apple iMessage. Deregister your phone number from Google Chat Features. Deregister your phone number from your carrier's RCS service (Not all carriers have this) [...] Some carriers require you to explicitly opt in to use services such as Wi-Fi calling. Consult your carrier's documentation on the process for this or contact them. Reset Mobile Network Settings in Settings > System > Reset options and then reboot the device. USA users only: You may need to request your carrier to enable CDMA-less mode if you have issues. Follow your carrier's instructions for setting up APNs, this can be found in Settings > Network & internet > SIMs > SIM > Access Point Names. If calls do not work and you have LTE-only mode enabled, try toggling it off. If \"Allow 2G\" is disabled, try toggling it back on. Your carrier may not properly support VoLTE. As a last resort you may need to ask your carrier for a replacement SIM card.\", https://grapheneos.org/usage\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"Device Support: GrapheneOS has official production support for the following devices [...] The following devices are end-of-life, no longer receive firmware or driver security updates [...] only the Android Open Source Project security backports, certain other security patches, and other minimal changes to keep them working [...] We strongly recommend only purchasing one of the following devices for GrapheneOS due to better security and a long minimum support guarantee from launch for full security updates and other improvements [...] 8th/9th generation Pixels provide a minimum guarantee of 7 years of support from launch instead of the previous 5 year minimum guarantee. 8th/9th generation Pixels also bring support for the incredibly powerful hardware memory tagging security feature [...] Broad device support would imply mainly supporting very badly secured devices unable to support our features. It would also take a substantial amount of resources away from our work on privacy and security, especially since a lot of it is closely tied to the hardware such as the USB-C port control and fixing or working around memory corruption bugs uncovered by our features. [...] GrapheneOS can only fully provide security updates to a device provided that the OEM is releasing them. When an OEM is no longer providing security updates, GrapheneOS aims to provide harm reduction releases for devices which only have a minimum of 3 years support. [...] Harm reduction releases do not have complete security patches because it's not possible to provide full security updates for the device without OEM support and they are intended to buy users some limited time to migrate to a supported device.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"Disk Encryption: Sensitive data is stored in user profiles. User profiles each have their own unique, randomly generated disk encryption key and their own unique key encryption key is used to encrypt it. The owner profile is special and is used to store sensitive system-wide operating system data. This is why the owner profile needs to be logged in after a reboot before other user profiles can be used. The owner profile does not have access to the data in other profiles. [...] GrapheneOS enables support for ending secondary user profile sessions after logging into them. It adds an end session button to the lockscreen and in the global action menu accessed by holding the power button. This fully purges the encryption keys and puts the profiles back at rest. This can't be done for the owner profile without rebooting due to it encrypting the sensitive system-wide operating system data. Using a secondary profile for regular usage allows you to make use of the device without decrypting the data in your regular usage profile. It also allows putting it at rest without rebooting the device. Even if you use the same passphrase for multiple profiles, each of those profiles still ends up with a unique key encryption key and a compromise of the OS while one of them is active won't leak the passphrase. The advantage to using separate passphrases is in case an attacker records you entering it. [...] In order to retrieve the Weaver token, the secure element requires the correct Weaver key. A secure internal timer is used to implement hardware-based delays for each attempt at key derivation. It quickly ramps up to 1 day delays before the next attempt. [...] The password token, Weaver token and other values like the OS verified boot key are used by the TEE as inputs to a hardware-bound key derivation algorithm provided by the SoC.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"Apps can identify their own app installation via their app data and can directly (until that's removed) or indirectly identify a profile. Profiles should be used when separate identities are desired. Profiles can be used as temporary ephemeral identities by creating them for a specific need and then deleting them.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"Cellular tracking, interception and silent SMS: GrapheneOS always considers networks to be hostile and avoids placing trust in them. [...] Cellular networks use inherently insecure protocols and have many trusted parties. Even if interception of the connection or some other man-in-the-middle attack along the network is not currently occurring, the network is still untrustworthy and information should not be sent unencrypted. Authenticated transport encryption such as HTTPS for web sites avoids trusting the cellular network. End-to-end encrypted protocols such as the Signal messaging protocol also avoid trusting the servers. [...] Connecting to your carrier's network inherently depends on you identifying yourself to it and anyone able to obtain administrative access. Activating airplane mode will fully disable the cellular radio transmit and receive capabilities, which will prevent your phone from being reached from the cellular network and stop your carrier (and anyone impersonating them to you) from tracking the device via the cellular radio. The baseband implements other functionality such as Wi-Fi and GPS functionality, but each of these components is separately sandboxed on the baseband and independent of each other. Enabling airplane mode disables the cellular radio, but Wi-Fi can be re-enabled and used without activating the cellular radio again. This allows using the device as a Wi-Fi only device. Receiving a silent SMS is not a good indicator of being targeted by your cell carrier, police or government because anyone on the cell network can send them including yourself. Cellular triangulation will happen regardless of whether or not SMS texts are being sent or received by the phone. Even if an SMS did serve a useful purpose for tracking, a silent SMS would be little different than receiving unsolicited spam. In fact, sending spam would be stealthier since it wouldn't trigger alerts for silent SMS but rather would be ignored with the rest of the spam. Regardless, sending texts or other data is not required or particularly useful to track devices connected to a network for an adversary with the appropriate access. Airplane mode is the only way to avoid the cellular network tracking your device and works correctly on the devices we support.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"When you have both a cellular connection and Location enabled, control plane and/or user plane (SUPL) A-GNSS is used in addition to PSDS to greatly reduce the time needed for GNSS to obtain an initial location lock. These obtain coarse location info from a server based on nearby cell towers. Control plane A-GNSS is provided by the cellular connection itself and therefore doesn't have any real privacy implications while SUPL connects to a server often not provided by the carrier. Most A-GNSS services only accelerate obtaining a satellite-based location and won't provide an estimate on their own. The carrier can choose a SUPL server as part of their carrier configuration but most leave it at the default of supl.google.com. By default, GrapheneOS overrides the carrier/fallback SUPL server and uses the supl.grapheneos.org proxy. [...] GrapheneOS also disables sending IMSI and phone number as part of SUPL.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"The OS uses the network-provided DNS servers by default. [...] A VPN provides a network layered on top of the underlying networks and the OS uses the VPN-provided DNS servers for everything beyond resolving the IP address of the VPN and performing network connectivity checks on each of the underlying networks in addition to the VPN itself. Using the network-provided DNS servers is the best way to blend in with other users. Network and web sites can fingerprint and track users based on a non-default DNS configuration.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"VPNs can be configured under Settings > Network & internet > VPN. Support for the following protocols is included: IKEv2/IPSec MSCHAPv2, IKEv2/IPSec PSK and IKEv2/IPSec RSA. Apps can also provide userspace VPN implementations. The only apps we can recommend is the official WireGuard app and the official Mullvad app. Mullvad's app is tested on GrapheneOS with hardware memory tagging, unlike the official WireGuard app which has invalid memory accesses detected by memory tagging if it's enabled.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"Why am I seeing a message about my bootloader not being locked when setting up the device? If you are seeing this warning when setting up your device after GrapheneOS has been installed, it means that not all of the installation steps have been completed. This can be remediated by finishing the installation process and locking the device's bootloader. [...] It is important to note that this is something that should be done before placing any data onto the device, as until this step is completed, the device is considered to be insecure. Please note that locking the bootloader wipes all user data on the device.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"Notifications: [...] they rely on receiving events through Google servers via Firebase Cloud Messaging (FCM) in the background and sometimes even in the foreground, although it doesn't have good reliability/latency. [...] In order to properly implement either push messaging or frequent polling themselves, an app needs to run a foreground service. This is displayed as a persistent notification. It will normally be marked as a silent notification with the lowest possible default importance for a foreground service (IMPORTANCE_LOW). It can be reduced to the lowest importance (IMPORTANCE_MIN) by the user if they set the notification channel to be collapsed which will collapse it in the notification tray and it won't show up as an icon in the status bar or on the lockscreen anymore. Users can also disable the notification and the foreground service will continue working. A battery optimization exception is also needed for the app to bypass device idle states and run while the device is idle. If you can tolerate delays while the device is idle, then the battery optimization exception isn't mandatory.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"Factory Reset Protection: No, since this is strictly a theft deterrence feature, not a security feature, and the standard implementation depends on having the device tied to an account on an online service. The only advantage would be encouraging thieves to return a stolen device for a potential reward after realizing that it has no value beyond scrapping it for parts. [...] since this has no security value and the ability to deter theft is questionable, implementing this is an extremely low priority.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"Bundling additional apps with the OS can increase attack surface, unless users go out of the way to disable apps they aren't using. [...] GrapheneOS is focused on making meaningful improvements to privacy and security, and bundling assorted apps into the OS is not only usually outside of that focus but often counter to it.\", https://grapheneos.org/faq\nGrapheneOS Website, Frequently Asked Questions, 2024, November 28. \"There are various companies selling devices with GrapheneOS [...] We are currently not affiliated with or endorse any company selling devices with GrapheneOS. Our official recommendation is to buy a supported device and install GrapheneOS yourself.\", https://grapheneos.org/faq\nGrapheneOS Website, Device integrity monitoring About / Tutorial, 2024, November 28. \"The Auditor app uses hardware-based security features to validate the identity of a device, along with the authenticity and integrity of the operating system. It ensures the device is running a verified operating system with a locked bootloader and that no tampering has occurred. [...] Verified boot validates the integrity and authenticity of firmware and the entire operating system (both the kernel and userspace) from an immutable hardware root of trust [...] Local verification: press Auditor on the device that will be verifying the Auditee. Press Auditee on the device that's going to be verified. Point the camera of the Auditee at the QR code on the Auditor to read the challenge. Tap the QR code on the Auditor to advance ahead (if you do this too early, you can press back). Point the camera of the Auditor at the QR code on the Auditee to read the attestation. View verification of the attestation results. [...] Scheduled remote verification: create an account on https://attestation.app/ from a separate device. Press the 'Enable remote verification' button in the app. Scan the account QR code displayed on https://attestation.app. Configure an alert email address to receive alerts if the device fails to provide valid attestations in time. Refresh https://attestation.app/ to view the initial attestation result.\", https://attestation.app/about, https://attestation.app/tutorial\nAndroid Open Source Project Documentation, Private Space, 2024, November 28. \"The sandboxed space is a separate Android profile [...] When the space is locked, the private profile user is stopped, and when the space is unlocked, the user is started. [...] The private profile can exist on the same device as a work profile and a clone profile. It is a sandboxed space separate from the main Android user. When the private space is unlocked the apps become visible in settings, Sharesheet, photo picker, and DocsUI. The apps inside private space aren't shown in any of these surfaces when private space is locked. Private space can have only one instance on a single device, and can exist only for the main user (not for secondary users or within other profiles). [...] Telephony intents are routed to the main user and display a notification. All other intents are limited to private profile, and are not redirected\", https://source.android.com/docs/security/features/private-space\nEylenburg Website, Comparison of Android-based Operating Systems, 2024, November 28. https://eylenburg.github.io/android_comparison.htm\nPrivSec Website, F-Droid Security Issues, 2024, November 26. https://eylenburg.github.io/android_comparison.htm\nGrapheneOS (@GrapheneOS), 2024, February 21. \"CalyxOS rolls back security rather than improving it. It doesn't provide comparable privacy features either, and most of the features they do provide in these areas are serious flawed. For example, they provide a panic wipe feature trivially bypassed by holding volume down...\", https://discuss.grapheneos.org/d/7891-sharing-imei-with-google-account-for-warranty-claim/6\nSide Of Burritos (@sideofburritos), 2024, November 25. \"I Tried to Brick My GrapheneOS Installation (So You Don‚Äôt Have To)\", https://www.youtube.com/watch?v=ik0AiO0WtuU\nGrapheneOS Website, Web Installer, 2024, November 30. \"Installation of the stock OS via the stock factory images is similar to the process described above but with Google's web flashing tool. However, before flashing and locking, there's an additional step to fully revert the device to a clean factory state. The GrapheneOS factory images flash a non-stock Android Verified Boot key which needs to be erased to fully revert back to a stock device state. Before flashing the stock factory images, you should boot the device into fastboot mode and make sure the bootloader is unlocked. Then erase the custom Android Verified Boot key to untrust it:\", https://grapheneos.org/install/web\nGrapheneOS (@GrapheneOS), 2024, December 1. \"We wrote the code in a way that Auditor could easily support other instances of the service but it currently only supports using https://attestation.app rather than using the domain in the QR code. It also has key pinning for the CA root keys for https://attestation.app.\", https://x.com/GrapheneOS/status/1863351293009412393\nVeritasium (@veritasum), 2024, September 21, \"Exposing The Flaw In Our Phone System\", https://www.youtube.com/watch?v=wVyu7NB7W6Y\nAndroid Open Source Project Documentation, App Security, 2024, December 3. \"All apps on Android run in an Application Sandbox. By default, an Android app can only access a limited range of system resources. The system manages Android app access to resources that, if used incorrectly or maliciously, could adversely impact the user experience, the network, or data on the device. These restrictions are implemented in a variety of different forms. Some capabilities are restricted by an intentional lack of APIs to the sensitive functionality (for example, there is no Android API for directly manipulating the SIM card). In some instances, separation of roles provides a security measure, as with the per-app isolation of storage. In other instances, the sensitive APIs are intended for use by trusted apps and protected through a security mechanism known as Permissions.\", https://source.android.com/docs/security/overview/app-security\nPrivacy Guides Forum Discussion, 2024, December 3. \"Sharing files between GrapheneOS profiles\", https://discuss.privacyguides.net/t/sharing-files-between-grapheneos-profiles/13293/1\n@GrapheneOS@grapheneos.social, 2024, July 20. \"GrapheneOS App Store now includes a mirror of Accrescent, which is a privacy and security focused alternative to the Play Store distributing developer builds of apps: https://accrescent.app/ Accrescent comes from within the GrapheneOS community and we're collaborating together. Accrescent is in alpha and isn't yet open to any developers uploading their apps. It will have a lot more apps available in the future. It will become a full alternative to Play Store permitting closed source apps too, but you'll be able to filter to show only open source apps. Lead dev of Accrescent is a GrapheneOS user and contributor. It'll be a good place to publish apps for GrapheneOS users. AppVerifier, BeauTyXT and Transcribro are from the same person who wrote our Info app. Molly is a security-focused fork of Signal from another GrapheneOS user. AppVerifier was based on a planned GrapheneOS feature for users to verify APK files based on their key fingerprint. The feature is currently stalled since relying on the clipboard isn't ideal. For now, users can use AppVerifier from Accrescent until we ship a built-in approach. We'll be delegating distributing developer builds of apps signed by the developers to Accrescent rather than doing it in ourselves. Our App Store will be focused on our own apps and eventually hardened, rebranded builds of important third party apps widely used by our community. [...] We can't recommend using F-Droid due to extremely poor security practices throughout their client, repository design and infrastructure. The developers have an anti-security attitude and have demonstrated thoroughly untrustworthy behavior. We're going to be supporting Accrescent and helping them get what they need to reach a stable release and provide a huge number of apps with the support of the app developers. People are free to use F-Droid on GrapheneOS but we won't promote it. [...] F-Droid attempts to do repository metadata signing but both the approach they take and implementation is quite flawed. Transport security via HTTPS using WebPKI doesn't only trust their server. It also trusts every CA and sub-CA without a way to monitor abuse since unlike a browser there isn't enforced Certificate Transparency. We pin root keys and backup keys for our services in our apps but of course it's not what's relied on for verifying App Store repository metadata. [...] It does protect against an attacker who has compromised both the server for the app store and has obtained the developer's signing key. They would need to put a backdoor into a release, which they could do whether or not it's open source software and whether or not it's built by the app store. F-Droid pretends building apps offers protection against this but the reality is that they blindly build the code and there is no way to auto-detect backdoors being added. [...] The app's signing key is how the OS package manager verifies the app, but both our App Store and Accrescent also verify each downloaded APK based on the repository metadata. We were explaining that having control of the app repository and the developer's signing key isn't enough because the repository metadata is signed offline. That's why we said they'd need to actually put a backdoor into either the app's source code or the latest official build of the app. Reproducible build verification can't actually prevent this. It only prevents them putting it into the build without also putting it into the source code. It's clearly possible to put a backdoor into the source code without it being detected as shown by the multiple times it has been caught happening after the fact in widely used projects. The most well known recent example was the xz ssh backdoor, but that's not the only one, and there may be important missed cases.\", https://grapheneos.social/@GrapheneOS/112821386750410102"}}}